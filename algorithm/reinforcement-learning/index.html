
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="一个有待面面俱到的机器学习文档">
      
      
        <meta name="author" content="ricolxwz">
      
      
        <link rel="canonical" href="https://ml.ricolxwz.de/algorithm/reinforcement-learning/">
      
      
        <link rel="prev" href="../markov-chain/">
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.43">
    
    
      
        <title>强化学习 - 机器学习</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.0253249f.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      

  
  
  
  
  <style>:root{--md-annotation-icon:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.17 2.76A10.1 10.1 0 0 1 12 2c1.31 0 2.61.26 3.83.76 1.21.5 2.31 1.24 3.24 2.17s1.67 2.03 2.17 3.24c.5 1.22.76 2.52.76 3.83 0 2.65-1.05 5.2-2.93 7.07A9.97 9.97 0 0 1 12 22a10.1 10.1 0 0 1-3.83-.76 10 10 0 0 1-3.24-2.17A9.97 9.97 0 0 1 2 12c0-2.65 1.05-5.2 2.93-7.07.93-.93 2.03-1.67 3.24-2.17M12 17l1.56-3.42L17 12l-3.44-1.56L12 7l-1.57 3.44L7 12l3.43 1.58z"/></svg>');}</style>


    
    
      
    
    
      
    
    
      <link rel="stylesheet" href="https://unpkg.com/katex@0/dist/katex.min.css">
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
<link href="example.com" rel="icon" /> 
<link href="https://cdn.jsdelivr.net/npm/@fontsource/mononoki@5.1.0/index.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@ayahub/webfont-harmony-sans-sc@1.0.0/css/index.min.css">

   <link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="../../assets/javascripts/glightbox.min.js"></script></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#_1" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../.." title="机器学习" class="md-header__button md-logo" aria-label="机器学习" data-md-component="logo">
      <!--
  Copyright (c) 2016-2024 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->

<!-- Logo -->
<img id="logo_light_mode" src="https://cdn.jsdelivr.net/gh/sigmax0124/logo@master/favion-big-mc-212121-000000-1.svg" alt="logo">
<img id="logo_dark_mode" src="https://cdn.jsdelivr.net/gh/sigmax0124/logo@master/favion-big-mc-000000-212121-1.svg" alt="logo">
    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            机器学习
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              强化学习
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
      <div class="md-header__option">
  <div class="md-select">
    
    <button class="md-header__button md-icon" aria-label="选择当前语言">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"/></svg>
    </button>
    <div class="md-select__inner">
      <ul class="md-select__list">
        
          <li class="md-select__item">
            <a href="./" hreflang="zh" class="md-select__link">
              中文
            </a>
          </li>
        
          <li class="md-select__item">
            <a href="../../en/algorithm/reinforcement-learning/" hreflang="en" class="md-select__link">
              English
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</div>
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="分享" aria-label="分享" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
    
  
  开始

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../probability/" class="md-tabs__link">
          
  
    
  
  概率

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../" class="md-tabs__link">
          
  
    
  
  算法

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="机器学习" class="md-nav__button md-logo" aria-label="机器学习" data-md-component="logo">
      <!--
  Copyright (c) 2016-2024 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->

<!-- Logo -->
<img id="logo_light_mode" src="https://cdn.jsdelivr.net/gh/sigmax0124/logo@master/favion-big-mc-212121-000000-1.svg" alt="logo">
<img id="logo_dark_mode" src="https://cdn.jsdelivr.net/gh/sigmax0124/logo@master/favion-big-mc-000000-212121-1.svg" alt="logo">
    </a>
    机器学习
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
      
        
          
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../.." class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    开始
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            开始
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../probability/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    概率
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2" id="__nav_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            概率
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/random-event-and-probability/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    随机事件和概率
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/one-dimensional-random-variable-distribution/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    一维随机变量及其分布
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/multi-dimensional-random-variable-distribution/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    多维随机变量及其分布
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/numerical-characteristics-of-random-variable/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    随机变量的数字特征
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/large-number-central-limit-theorem/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    大数定律与中心极限定理
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    算法
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            算法
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../exam/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    考点
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../linear-regression/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    线性回归
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../preprocessing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    预处理
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../knn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    最邻近
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../naive-bayes/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    朴素贝叶斯
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../evaluation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    评估
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../decision-tree/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    决策树
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ensemble-learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    集成学习
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../svm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    支持向量机
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../dimensional-reduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    降维
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_12" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../neural-network/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    神经网络
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_12" id="__nav_3_12_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_12_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_12">
            <span class="md-nav__icon md-icon"></span>
            神经网络
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../neural-network/fnn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    前馈神经网络
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_12_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../neural-network/cnn/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    卷积神经网络
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_12_3" id="__nav_3_12_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_12_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_12_3">
            <span class="md-nav__icon md-icon"></span>
            卷积神经网络
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../neural-network/cnn/resnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ResNet
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../neural-network/rnn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    递归神经网络
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../neural-network/transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Transformer
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../clustering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    聚类
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../markov-chain/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    马尔可夫链
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    强化学习
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    强化学习
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      背景
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      定义
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      马尔可夫决策过程
    </span>
  </a>
  
    <nav class="md-nav" aria-label="马尔可夫决策过程">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#policy" class="md-nav__link">
    <span class="md-ellipsis">
      策略和最佳策略
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#value-function" class="md-nav__link">
    <span class="md-ellipsis">
      价值函数
    </span>
  </a>
  
    <nav class="md-nav" aria-label="价值函数">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bellman-function" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman方程
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#q-algo" class="md-nav__link">
    <span class="md-ellipsis">
      Q学习算法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Q学习算法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dql" class="md-nav__link">
    <span class="md-ellipsis">
      深度Q学习算法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="深度Q学习算法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      损失函数
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      经验回放
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      目标网络
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      优先级经验回放
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      应用
    </span>
  </a>
  
    <nav class="md-nav" aria-label="应用">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#alphago" class="md-nav__link">
    <span class="md-ellipsis">
      AlphaGo
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AlphaGo">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      神经网络
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#search-tree" class="md-nav__link">
    <span class="md-ellipsis">
      搜索过程
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      训练流程
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      战果
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    <span class="md-ellipsis">
      特点
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    <span class="md-ellipsis">
      深度强化学习
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      背景
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      定义
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      马尔可夫决策过程
    </span>
  </a>
  
    <nav class="md-nav" aria-label="马尔可夫决策过程">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#policy" class="md-nav__link">
    <span class="md-ellipsis">
      策略和最佳策略
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#value-function" class="md-nav__link">
    <span class="md-ellipsis">
      价值函数
    </span>
  </a>
  
    <nav class="md-nav" aria-label="价值函数">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bellman-function" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman方程
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#q-algo" class="md-nav__link">
    <span class="md-ellipsis">
      Q学习算法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Q学习算法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dql" class="md-nav__link">
    <span class="md-ellipsis">
      深度Q学习算法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="深度Q学习算法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      损失函数
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      经验回放
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      目标网络
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      优先级经验回放
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      应用
    </span>
  </a>
  
    <nav class="md-nav" aria-label="应用">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#alphago" class="md-nav__link">
    <span class="md-ellipsis">
      AlphaGo
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AlphaGo">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      神经网络
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#search-tree" class="md-nav__link">
    <span class="md-ellipsis">
      搜索过程
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      训练流程
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      战果
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    <span class="md-ellipsis">
      特点
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    <span class="md-ellipsis">
      深度强化学习
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                



                  

  
    <a href="https://github.com/ricolxwz/ml/edit/master/docs/algorithm/reinforcement-learning.md" title="编辑此页" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/ricolxwz/ml/raw/master/docs/algorithm/reinforcement-learning.md" title="查看本页的源代码" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


  <h1>强化学习</h1>

<h2 id="_1">背景</h2>
<p>监督学习旨在通过训练集中的已知标签<span class="arithmatex">\(y\)</span>来训练模型, 使模型的输出尽可能模仿这些标签, 通常用于分类或回归任务. 例如, 将不同类型的宝可梦进行分类, 在这种情况下, 输入有一个正确答案或者标签, 模型试图预测每个输入的标签.</p>
<figure>
<p><a class="glightbox" href="https://img.ricolxwz.io/4ac5abb0cf015b7b4090727b14619b9f.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="" loading="lazy" src="https://img.ricolxwz.io/4ac5abb0cf015b7b4090727b14619b9f.png" width="350" /></a></p>
</figure>
<p>但是, 在一些复杂任务中, 如让机器人学会骑自行车中, 定义"正确答案"非常困难, 很难提供明确的监督信号去指导算法模仿, 因此监督学习不适合这类问题. </p>
<figure>
<p><a class="glightbox" href="https://img.ricolxwz.io/59327b41a383dd54769205f503770e69.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="" loading="lazy" src="https://img.ricolxwz.io/59327b41a383dd54769205f503770e69.png" width="250" /></a></p>
</figure>
<h2 id="_2">定义</h2>
<p>强化学习可以用来解决上述问题, 可以通过婴儿学习的类比来解释: 婴儿在成长过程中没有明确的老是指导, 而是通过与环境的互动积累经验. 例如, 婴儿通过玩耍, 挥动手臂或四处观察等活动, 接受到来自环境的正面(如称赞)和负面(如批评)反馈, 这些反馈塑造了他们的行为和性格, 这种试错和反馈的过程类似于强化学习中的"反馈机制".</p>
<figure>
<p><a class="glightbox" href="https://img.ricolxwz.io/3c49aa991e47c617c6c802da039debb3.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="" loading="lazy" src="https://img.ricolxwz.io/3c49aa991e47c617c6c802da039debb3.png" width="500" /></a></p>
</figure>
<p>强化学习的目标是学习如何做出一系列良好的决策. 智能体, Agent根据当前的状态<span class="arithmatex">\(s_t\)</span>采取行动<span class="arithmatex">\(a_t\)</span>, 并根据从环境中获得的奖励来调整其策略, 通过试错来最大化累计奖励. 这种奖励可以用奖励函数<span class="arithmatex">\(r_t\)</span>来描述, 它是一个标量, 反馈智能体在<span class="arithmatex">\(t\)</span>时刻的表现好坏. </p>
<figure>
<p><a class="glightbox" href="https://img.ricolxwz.io/572319de8ee175bda36bc5024ccf4a7a.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="" loading="lazy" src="https://img.ricolxwz.io/572319de8ee175bda36bc5024ccf4a7a.png" width="250" /></a></p>
</figure>
<details class="tip" open="open">
<summary>Tip</summary>
<p>"奖励假设"告诉我们"所有目标都可以通过最大化预期的累计奖励来描述", 这意味着, 只要能够设计一个合适的奖励函数, 我们就可以通过强化学习实现各种复杂的目标, 涵盖从游戏到机器人控制的不用应用场景.</p>
</details>
<p>在每一个时间步, 智能体接受状态<span class="arithmatex">\(s_t\)</span>和奖励<span class="arithmatex">\(r_t\)</span>, 执行行动<span class="arithmatex">\(a_t\)</span>. 环境接受行动<span class="arithmatex">\(a_t\)</span>, 更新状态为<span class="arithmatex">\(s_{t+1}\)</span>, 并发出新奖励<span class="arithmatex">\(r_{t+1}\)</span>. 历史<span class="arithmatex">\(H_t=s_1, r_1, a_1, ..., a_{t-1}, s_t, r_t\)</span>记录了智能体从开始到当前时间步<span class="arithmatex">\(t\)</span>为止所有的状态, 行动和奖励, 接下来的行动<span class="arithmatex">\(a_t\)</span>取决于这个历史.</p>
<h2 id="_3">马尔可夫决策过程</h2>
<p>马尔可夫决策过程, Markov Decision Process, MDP, 是用于定义强化学习问题的数学框架, MDP通常通过五元组<span class="arithmatex">\((\mathcal{S}, \mathcal{A}, \mathcal{R}, \mathbb{P}, \gamma)\)</span>来表示:</p>
<ul>
<li><span class="arithmatex">\(\mathcal{S}\)</span>: 表示所有可能的状态集合, 即智能体在环境中可能遇到的不同状态</li>
<li><span class="arithmatex">\(\mathcal{A}\)</span>: 表示所有可能的行动集合, 即智能体可以采取的各种行动</li>
<li><span class="arithmatex">\(\mathcal{R}\)</span>: 表示在特定的状态和行动下, 奖励值的分布</li>
<li><span class="arithmatex">\(\mathbb{P}\)</span>: 表示给定当前的状态和所选行动后, 转移到下一个状态的概率分布</li>
<li><span class="arithmatex">\(\gamma\)</span>: 折扣因子, 用于确定未来奖励的价值, 较低的<span class="arithmatex">\(\gamma\)</span>会让智能体更关注当前的奖励, 较高的<span class="arithmatex">\(\gamma\)</span>会让智能体考虑长期的回报</li>
</ul>
<p>在马尔可夫决策过程中, 假设只需要关注当前状态<span class="arithmatex">\(s_t\)</span>就可以决定下一步, 而不依赖于更早的历史.</p>
<p>智能体由三个部分组成:</p>
<ol>
<li><a href="#policy">策略</a>: Policy, 定义了每个状态下智能体采取的行动. 通常用<span class="arithmatex">\(\pi(a|s)\)</span>表示. 策略可以是确定性的(每个状态对应唯一的行动), 或者是随机性的(每个状态对应行动的概率分布)</li>
<li><a href="#value-function">价值函数</a>: Value Function, 衡量智能体在某个策略下给定状态或在给定状态下采取行动后的预期回报. 它能帮助智能体判断哪些状态或状态和行动的组合更有利, 常见的价值函数有状态值函数<span class="arithmatex">\(V^{\pi}(s)\)</span>和动作值函数<span class="arithmatex">\(Q^{\pi}(s, a)\)</span>, <span class="arithmatex">\(\pi\)</span>是某个特定的策略, 侧重于评估</li>
<li>
<p>模型: Model, 用于模拟环境的动态行为. 即预测在某个状态下采取特定行动后, 环境如何变化. 通常提供以下信息: a. 从当前状态<span class="arithmatex">\(s\)</span>采取行动<span class="arithmatex">\(a\)</span>后, 可能会转移到哪个下一个状态<span class="arithmatex">\(s'\)</span>; b. 在状态<span class="arithmatex">\(s\)</span>执行动作<span class="arithmatex">\(a\)</span>后, 可能获得的即时奖励<span class="arithmatex">\(r\)</span>. 侧重于模拟</p>
<details class="tip" open="open">
<summary>Tip</summary>
<p>有些RL算法会使用模型来模拟和预见环境的变化, 称为基于模型的算法, 而某些RL算法不需要模型, 称为无模型算法. 如果是基于模型的算法, 直接计算价值函数需要智能体在真实环境中进行多次试错, 以获取足够的数据来估计每个状态或给定状态采取行动后的预期汇报, 这对于一些复杂或者代价高昂的环境来说可能不切实际. 模型可以用来模拟环境, 这样, 智能体可以通过在模型中进行"虚拟"实验来估计价值函数, 而不是每次都与真实环境互动, 使得智能体能够"离线学习", 这种方式既节省了资源, 也加快了学习过程.</p>
</details>
</li>
</ol>
<details class="example" open="open">
<summary>例子</summary>
<p>假设有一个扫地机器人, 它完成扫地后要回到它的基地, 起始位置可以是任何房间.</p>
<p><figure markdown='1'>
<a class="glightbox" href="https://img.ricolxwz.io/d18b1e9303deaf5464c7261cd95d395a.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="" loading="lazy" src="https://img.ricolxwz.io/d18b1e9303deaf5464c7261cd95d395a.png" width="200" /></a>
</figure></p>
<p>如上图, 总共有<span class="arithmatex">\(12\)</span>个房间, 机器人完成扫地后可以在任意房间, 意味着总共有<span class="arithmatex">\(12\)</span>个状态, 它需要做的是在最少的步骤下回到灰色方块所指示的房间. 它的行为可以是向上, 向下, 向左, 向右. 每次移动都设置为一个负奖励, 表示每走一步就扣一分, 鼓励机器人在最少的步数内到达基地. </p>
</details>
<h3 id="policy">策略和最佳策略</h3>
<details class="example" open="open">
<summary>例子</summary>
<p>策略<span class="arithmatex">\(\pi\)</span>是机器人在每个状态下选择的动作规则, 例如, 在每个房间, 机器人可以选择向上, 向下, 向左, 向右. 最优策略<span class="arithmatex">\(\pi^*\)</span>是使得奖励总和最大化的策略. 由于初始状态和状态转移概率的不确定性, 如机器人初始可能在任意一个房间, 往各个房间(即下一个状态)走的概率也可能不一样.</p>
<p><figure markdown='1'>
<a class="glightbox" href="https://img.ricolxwz.io/7236bec10dca7d6d88ece46d3301adb6.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="" loading="lazy" src="https://img.ricolxwz.io/7236bec10dca7d6d88ece46d3301adb6.png" width="400" /></a>
</figure></p>
<p>在这种情况下, 最优策略需要在所有可能的路径中最大化期望奖励. 使用公式可以表示为<span class="arithmatex">\(\pi^*=argmax_{\pi}\mathbb{E}[\sum_{t\geq 0}\gamma^t r_t|\pi]\)</span>. <span class="arithmatex">\(\sum_{t\geq 0}\)</span>表示的是从时间步<span class="arithmatex">\(t=0\)</span>开始一直到未来的所有时间步的奖励<span class="arithmatex">\(r_t\)</span>之和, 每个奖励都乘以折扣因子<span class="arithmatex">\(\gamma\)</span>, 用于表示对未来奖励的重视程度. 条件期望<span class="arithmatex">\(\mathbb{E[...|\pi]}\)</span>表示在给定策略<span class="arithmatex">\(\pi\)</span>下计算的期望值, 即在该策略引导下, 累积的折扣奖励的期望. 初始状态<span class="arithmatex">\(s_0\sim p(s_0)\)</span>, 表示机器人初始状态符合一定的概率分布. 行为<span class="arithmatex">\(a_t\sim \pi(\cdot |s_t)\)</span>, 表示在每个时间步<span class="arithmatex">\(t\)</span>, 机器人的行为符合概率分布<span class="arithmatex">\(\pi(\cdot|s_t)\)</span>, 策略<span class="arithmatex">\(\pi\)</span>是一个关于状态的概率分布, 描述了在不同状态下选择不同动作的概率. 状态转移<span class="arithmatex">\(s_{t+1}\sim p(\cdot | s_t, a_t)\)</span>, 表示机器人的下一个状态符合概率分布<span class="arithmatex">\(p(\cdot|s_t, a_t)\)</span>, 这个转移概率分布描述了在当前状态和采取的动作下, 环境可能转移到的下一个状态的分布.</p>
</details>
<h3 id="value-function">价值函数</h3>
<p>价值函数用于衡量智能体在<strong>某个策略</strong>下 <ins>给定状态</ins> 或 <ins>在给定状态下采取行动后</ins> 的预期回报. 根据划线部分的不同, 可以分为两种价值函数: 状态值函数和动作值函数.</p>
<ul>
<li>状态值函数: 表示在某个策略<span class="arithmatex">\(\pi\)</span>下, 特定状态<span class="arithmatex">\(s\)</span>的好坏程度. 它定义为从状态<span class="arithmatex">\(s\)</span>出发, 遵循策略<span class="arithmatex">\(\pi\)</span>所能获得的期望累计奖励. 公式为<span class="arithmatex">\(V^{\pi}(s)=\mathbb{E}[\sum_{t\geq 0}\gamma^t r_t|s_0=s, \pi]\)</span></li>
<li>动作值函数: 表示在某个策略<span class="arithmatex">\(\pi\)</span>下, 当在状态<span class="arithmatex">\(s\)</span>时执行动作<span class="arithmatex">\(a\)</span>的好坏程度. 它定义为首先在状态<span class="arithmatex">\(s\)</span>执行动作<span class="arithmatex">\(a\)</span>, 然后遵循策略<span class="arithmatex">\(\pi\)</span>所能获得的期望累计奖励. 公式为<span class="arithmatex">\(Q^{\pi}(s, a)=\mathbb{E}[\sum_{t\geq 0}\gamma^t r_t|s_0=s, a_0=a, \pi]\)</span>. </li>
</ul>
<h4 id="bellman-function">Bellman方程</h4>
<p>定义<span class="arithmatex">\(Q^*(s, a)\)</span>为最优Q值函数, 公式为<span class="arithmatex">\(Q^*(s, a)=max_{\pi}\mathbb{E}[\sum_{t\geq 0}\gamma^t r_t|s_0=s, a_0=a, \pi]\)</span>, 注意和动作值函数做区分, 这里表示的是<strong>在所有可能的策略</strong>中, 而动作值函数表示的是<strong>在特定的策略下</strong>. 最优Q值函数满足以下Bellman方程: <span class="arithmatex">\(Q^*(s, a) = \mathbb{E}_{s' \sim \mathcal{E}} \left[ r + \gamma \max_{a'} Q^*(s', a') \,|\, s, a \right]\)</span>.</p>
<p>这个公式的直观理解就是如果下一个状态-动作<span class="arithmatex">\(s', a'\)</span>对应的最优Q值<span class="arithmatex">\(Q^*(s', a')\)</span>已知, 那么在状态<span class="arithmatex">\(s\)</span>执行动作<span class="arithmatex">\(a\)</span>所能获得的最优回报<span class="arithmatex">\(Q^*(s, a)\)</span>, 等于即时奖励<span class="arithmatex">\(r\)</span>加上下一个状态-动作对<span class="arithmatex">\(s', a'\)</span>的最优Q值函数的期望值. <span class="arithmatex">\(s'\sim \mathcal{E}\)</span>指的是下一个状态<span class="arithmatex">\(s'\)</span>符合状态转移概率分布<span class="arithmatex">\(p(\cdot|s, a)\)</span>. </p>
<p>最优策略<span class="arithmatex">\(\pi^*\)</span>指的就是在每个状态下, 都选择能够使得<span class="arithmatex">\(Q^*(s, a)\)</span>最大化的动作/行为. </p>
<h2 id="q-algo">Q学习算法</h2>
<p>Q学习算法的目的就是获得所有状态-动作对的最优Q值, 步骤为:</p>
<ol>
<li>初始化: 对于所有的状态-动作对<span class="arithmatex">\(s, a\)</span>, 初始化<span class="arithmatex">\(Q(s, a)\)</span>, 通常为<span class="arithmatex">\(0\)</span>, 构成的表称为Q表</li>
<li>持续交互: <ol>
<li>开始一个新的回合: 随机设定一个初始状态<ol>
<li>根据当前状态<span class="arithmatex">\(s\)</span>和策略(如<span class="arithmatex">\(\epsilon\)</span>-贪心策略)选择一个动作<span class="arithmatex">\(a\)</span></li>
<li>执行动作<span class="arithmatex">\(a\)</span>, 获得即时奖励<span class="arithmatex">\(r\)</span>和下一个状态<span class="arithmatex">\(s'\)</span></li>
<li>我们并不知道每个状态-动作对的最优Q值<span class="arithmatex">\(Q^*(s, a)\)</span>, 所以需要使用增量更新的方式逐步逼近最优Q值, <span class="arithmatex">\(Q(s, a)\)</span>: <span class="arithmatex">\(Q(s, a)\leftarrow Q(s, a)+\alpha[r+\gamma max_{a'} Q(s', a')-Q(s, a)]\)</span>. 这个公式的推导: 在执行动作<span class="arithmatex">\(a\)</span>后, 根据Bellman方程, 我们可以算出一个目标Q值, 表示接下来采取最优动作时可以获得地期望回报, 目标Q值为<span class="arithmatex">\(R(s, a, s')+\gamma max_{a'}Q(s', a')\)</span>. 那么它和当前Q值之间的差距就是增量<span class="arithmatex">\(\Delta Q(s, a)=\)</span>目标Q值<span class="arithmatex">\(-Q(s, a)=(R(s, a, s') + \gamma max_{a'}Q(s', a'))-Q(s, a)\)</span>. 然后, 设置一个学习率<span class="arithmatex">\(\alpha\)</span>用来控制差距调整的大小<span class="arithmatex">\(\alpha[r+\gamma max_{a'} Q(s', a')-Q(s, a)]\)</span>, Q.E.D</li>
<li>将<span class="arithmatex">\(s'\)</span>作为新的当前状态, 如果<span class="arithmatex">\(s'\)</span>是终止状态, 回合结束; 如果不是终止状态, 则继续</li>
</ol>
</li>
<li>回合结束? 进行下一个回合</li>
</ol>
</li>
<li>终止: 若达到固定回合数, 或者Q表中所有的Q值变化都很小了, 小于某个阈值近似收敛, 则终止算法, Q表中的值作为学习到的最优Q值.</li>
</ol>
<details class="example" open="open">
<summary>例子</summary>
<p>继续上面的例子. 假设向上走是<span class="arithmatex">\(0\)</span>, 向下走<span class="arithmatex">\(1\)</span>, 向左走<span class="arithmatex">\(2\)</span>, 向右走<span class="arithmatex">\(3\)</span>.</p>
<ol>
<li>初始化: 首先, 初始化一个Q表, 这张表是<span class="arithmatex">\(12\times 4\)</span>, 因为有<span class="arithmatex">\(12\)</span>个状态, <span class="arithmatex">\(4\)</span>种动作. 初始值全部都是<span class="arithmatex">\(0\)</span></li>
<li>持续交互<ol>
<li>开始一个新的回合, 随机设定初始房间为<span class="arithmatex">\(8\)</span><ol>
<li>选择动作: 根据<span class="arithmatex">\(\epsilon\)</span>-贪心策略, 机器人在每个房间选择一个动作. 例如在在房间<span class="arithmatex">\(8\)</span>选择向上走</li>
<li>执行动作和接受奖励: 执行动作, 获取即时奖励, 并转移到下一个状态<span class="arithmatex">\(s'\)</span>. 例如向上走到了房间<span class="arithmatex">\(6\)</span>, 由于不是目标房间, 即时奖励为<span class="arithmatex">\(-1\)</span></li>
<li>更新Q值: 用公式更新Q表中<span class="arithmatex">\(Q(8, 0)\)</span>的值, 我们可以在当前的Q表中找到<span class="arithmatex">\(max_{a'}Q(6, *)\)</span>(初始为<span class="arithmatex">\(0\)</span>, 后续会慢慢更新), 例如<span class="arithmatex">\(a'\)</span>是向下走, <span class="arithmatex">\(max_{a'}Q(s', a')=Q(6, 1)\)</span>, 如果新的Q值变大了, 说明这个房间<span class="arithmatex">\(8\)</span>向上这个方向比较好</li>
<li>将房间<span class="arithmatex">\(6\)</span>设置为新的状态, 重复上述过程</li>
</ol>
</li>
<li>回合结束? 如果它走到了房间<span class="arithmatex">\(1\)</span>或房间<span class="arithmatex">\(12\)</span>, 回合结束, 进行下一个回合</li>
</ol>
</li>
<li>终止: 若达到固定回合数, 或者Q表中所有的Q值变化都很小了, 小于某个阈值近似收敛, 则终止算法, Q表中的值作为学习到的最优Q值</li>
</ol>
</details>
<h3 id="dql">深度Q学习算法</h3>
<p>在传统的Q学习中, Q值是保存在Q表中的, 这个方法在小规模的, 离散的状态空间中效果良好, 但是在复杂环境中主要会面临以下问题:</p>
<ul>
<li>状态空间过大, Q表会变得极其庞大, 导致存储和计算的资源需求变得不现实</li>
<li>无法处理连续状态, 对于很多现实问题, 例如自动驾驶或者机器人控制, 状态(例如位置, 速度, 角度)都是连续的, 无法用有限的Q表表示</li>
<li>难以进行泛化, 传统Q学习在Q表中仅能记录每个具体状态-动作对的Q值, 无法在类似状态之间进行泛化. 这意味着智能体在一个状态下学到的信息无法被类似的状态直接利用</li>
</ul>
<p>为了解决上述问题, 深度Q学习(DQN)引入了神经网络来近似最优Q值函数, 其结果<span class="arithmatex">\(Q_w(s, a)\simeq Q^*(s, a)\)</span>, <span class="arithmatex">\(w\)</span>为权重. 即给定<span class="arithmatex">\(s, a\)</span>, 神经网络输出的Q值接近真实的最优Q值.</p>
<p>这种网络被称为"Q网络", 如图.</p>
<figure>
<p><a class="glightbox" href="https://img.ricolxwz.io/674d26b8e92821c8839d8bbbe9a298ba.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="" loading="lazy" src="https://img.ricolxwz.io/674d26b8e92821c8839d8bbbe9a298ba.png" width="300" /></a></p>
</figure>
<p>左图是❌错误的, 这个神经网络接受一个状态和一个动作, 然后输出该特定状态-动作对的Q值<span class="arithmatex">\(Q_w(s, a)\)</span>. 这个方法的问题在于, 每次只能输出一个特定动作的Q值, 导致效率低下. 特别是在动作空间很大的情况下, 这会导致大量重复计算.</p>
<p>右图是✅正确的, 即只将状态<span class="arithmatex">\(s\)</span>输入网络, 然后让网络输出该状态下所有动作的Q值<span class="arithmatex">\(Q_w(s, a_1), Q_w(s, a_2), ..., Q_w(s, a_m)\)</span>. 这种设计更加高效, 因为对于每个状态<span class="arithmatex">\(s\)</span>, 网络只需要一次前向传播, 就能计算出所有可能动作的Q值. 在DQN算法中, 通常会将状态<span class="arithmatex">\(s\)</span>输入到神经网络, 网络的最后一层输出一个大小为动作数<span class="arithmatex">\(m\)</span>的向量, 其中每个元素对应一个一个状态-动作对的Q值.</p>
<details class="warning" open="open">
<summary>注意</summary>
<p>DQN能够处理连续的状态值, 但是不能处理连续的动作值. 因为DQN的输出层是每个离散动作输出一个Q值. 如果需要处理连续动作空间, 通常会使用其他算法, 如DDPG, SAC, TD3.</p>
</details>
<h4 id="_4">损失函数</h4>
<p>为了训练Q网络, 我们需要定义一个损失函数<span class="arithmatex">\(L\)</span>来最小化Q值的预测误差. 它是预测Q值和目标值之间的均方误差. 根据Bellman方程, 目标Q值为<span class="arithmatex">\(r+\gamma max_{a'}Q_w(s', a')\)</span>, 所以<span class="arithmatex">\(L = \left( r + \gamma \max_{a'} Q_w(s', a') - Q_w(s, a) \right)^2\)</span>, 通过最小化这个损失函数, 神经网络会逐步调整权重, 使得<span class="arithmatex">\(Q_w(s, a)\)</span>尽可能逼近最优Q值<span class="arithmatex">\(Q^*(s, a)\)</span>.</p>
<h4 id="_5">经验回放</h4>
<p>在连续的时间步中, 状态和动作是相关的, 因此直接使用这些样本进行训练会导致模型学习到时间相关性, 从而影响模型的稳定性.</p>
<p>经验回放是指智能体在环境中收集的经验, 一个四元组(当前状态, 采取的动作, 获得的奖励和到达的下一状态)存储在一个记忆池中, 然后在训练神经网络的时候从这个数据集中随机抽取经验进行预测, 然后根据损失函数更新Q网络的参数. 这种随机抽样能够打破时间上的相关性, 使得训练数据更加多样化, 独立, 从而减少模型的偏差.</p>
<hr />
<p>算法可以被表示为:</p>
<ol>
<li>初始化经验回放记忆池<span class="arithmatex">\(\mathcal{D}\)</span>, 容量为<span class="arithmatex">\(N\)</span></li>
<li>随机初始化Q网络的参数<span class="arithmatex">\(\theta\)</span></li>
<li>持续交互:<ol>
<li>开始一个新的回合: 随机设定初始状态序列<span class="arithmatex">\(s1=\{x_1\}\)</span>, 对其进行预处理<span class="arithmatex">\(\phi_1 = \phi(s_1)\)</span>, 其中<span class="arithmatex">\(x_1\)</span>是初始状态<ol>
<li>根据当前状态<span class="arithmatex">\(s\)</span>和<span class="arithmatex">\(\epsilon\)</span>-贪心策略选择一个动作<span class="arithmatex">\(a_t\)</span><ol>
<li>以概率<span class="arithmatex">\(\epsilon\)</span>选择随机动作: 这是为了探索(exploration), 让智能体有一定的概率选择随机动作<span class="arithmatex">\(a_t\)</span>, 从而探索更多的状态空间</li>
<li>以概率<span class="arithmatex">\(1-\epsilon\)</span>选择估计最优动作: 这是为了利用(exploitation)当前网络的知识, 选择当前Q网络估计的最优动作<span class="arithmatex">\(a_t = argmax_a Q(\phi(s_t), a; \theta)\)</span>. 其中, <span class="arithmatex">\(\phi(s_t)\)</span>是当前时间步经过预处理后的状态序列, <span class="arithmatex">\(a\)</span>动作, <span class="arithmatex">\(\theta\)</span>是Q网络当前的参数</li>
</ol>
</li>
<li>执行选择的动作<span class="arithmatex">\(a_t\)</span>, 并获得即时奖励<span class="arithmatex">\(r_t\)</span>和下一状态<span class="arithmatex">\(x_{t+1}\)</span></li>
<li>更新状态序列, 然后对其进行预处理, 得到<span class="arithmatex">\(\phi_{t+1}\)</span></li>
<li>将该经验<span class="arithmatex">\((\phi_t, a_t, r_t, \phi_{t+1})\)</span>存储在记忆池<span class="arithmatex">\(\mathcal{D}\)</span>中 </li>
<li>从记忆池<span class="arithmatex">\(\mathcal{D}\)</span>中随机采样一个小批量的经验<span class="arithmatex">\((\phi_j, a_j, r_j, \phi_{j+1})\)</span>, 进行预测, 打破样本相关性<ol>
<li>如果<span class="arithmatex">\(\phi_{j+1}\)</span>是终止状态, 则直接利用<span class="arithmatex">\(r_j\)</span>作为目标Q值, 即<span class="arithmatex">\(y_j=r_j\)</span></li>
<li>如果<span class="arithmatex">\(\phi_{j+1}\)</span>不是终止状态, 则根据Bellman公式有<span class="arithmatex">\(y_j=r_j+\gamma max_{a'}Q(\phi_{j+1}, a'; \theta)\)</span></li>
</ol>
</li>
<li>计算误差函数<span class="arithmatex">\(L = (y_j - Q(\phi_j, a_j;\theta ))^2\)</span>进行梯度下降最小化更新网络参数<span class="arithmatex">\(\theta\)</span></li>
<li>当记忆池容量已满, 则结束当前回合; 否则, 继续</li>
</ol>
</li>
<li>回合结束? 进行下一回合</li>
</ol>
</li>
<li>终止: 若达到固定回合数, 或所有的Q值变化都很小了, 小于某个阈值近似收敛, 则终止算法</li>
</ol>
<h4 id="_6">目标网络</h4>
<p>第一个场景:</p>
<p>在Q网络中, 网络的参数<span class="arithmatex">\(\mathcal{w}\)</span>是在不断更新的, 而目标Q值的计算过程是基于网络的当前参数的, 这会导致目标Q值不太稳定, 导致Q值很难收敛. 为了解决这个问题, DQN引入了一个目标网络, 目标网络的参数由主网络的参数复制而来, 但是在一段时间内保持固定不变. 相应的, 损失函数应该更新为<span class="arithmatex">\(L=(r+\gamma max_{a'}Q_{\hat{w}}(s', a') - Q_w(s, a))^2\)</span>. <span class="arithmatex">\(\hat{w}\)</span>为目标网络的参数.</p>
<p>第二个场景:</p>
<p>在Q网络中, 计算目标Q值时的<span class="arithmatex">\(max\)</span>函数很可能会受到噪音的影响, 导致选择的动作不对.</p>
<details class="example" open="open">
<summary>例子</summary>
<p>假设智能体在一个简单的游戏环境中, 当前状态<span class="arithmatex">\(s\)</span>下有两个可选动作: <span class="arithmatex">\(a_1\)</span>和<span class="arithmatex">\(a_2\)</span>. 假设下一状态<span class="arithmatex">\(s'\)</span>有三个可能的动作<span class="arithmatex">\(a'_1, a'_2, a'_3\)</span>, 并且每个动作的Q值估计如下：</p>
<p>真实的Q值:</p>
<ul>
<li><span class="arithmatex">\(Q(s', a'_1) = 3.0\)</span></li>
<li><span class="arithmatex">\(Q(s', a'_2) = 2.9\)</span></li>
<li><span class="arithmatex">\(Q(s', a'_3) = 2.0\)</span></li>
</ul>
<p>噪音影响下的Q值:</p>
<ul>
<li><span class="arithmatex">\(Q(s', a'_1) = 2.9\)</span></li>
<li><span class="arithmatex">\(Q(s', a'_2) = 3.1\)</span></li>
<li><span class="arithmatex">\(Q(s', a'_3) = 2.2\)</span></li>
</ul>
<p>这会导致我们选择<span class="arithmatex">\(a_2'\)</span>而不是<span class="arithmatex">\(a_1'\)</span></p>
</details>
<p>解决这种问题的方法还是使用目标网络. 使用当前网络<span class="arithmatex">\(Q_w\)</span>来选择动作<span class="arithmatex">\(a'=argmax_{a'}Q_w(s', a')\)</span>, 而使用目标网络<span class="arithmatex">\(Q_{\hat{w}}\)</span>来评估选择的动作<span class="arithmatex">\(Q_{\hat{w}}(s', a')\)</span>. 所以损失函数公式进一步改写为<span class="arithmatex">\(L=(r+\gamma Q_{\hat{w}}(s', argmax_{a'}Q_w(s', a')) - Q_w(s, a))^2\)</span></p>
<h4 id="_7">优先级经验回放</h4>
<p>这是对普通的经验回放的一种改进方法. 在普通的经验回放中, 都是均匀采样的, 不考虑每条经验的重要性. 但是在实际训练中, 有些经验对网络的改进有很大的影响, 特别是TD误差较大的经验, 因为它们表示了智能体对未来奖励较大的更新需求, 普通的均匀采样可能会浪费资源在较低TD误差的样本上.</p>
<p>TD误差的计算公式为<span class="arithmatex">\(|\delta_i|=|r+\gamma max_{a'}Q_{\hat{w}}(s', a') - Q_w(s, a)|\)</span>. 将经验的优先级设置为TD误差加上一个小常数来防止优先级为零<span class="arithmatex">\(p_i=|\delta_i|+\epsilon\)</span>. 然后计算每条经验的被采样概率<span class="arithmatex">\(p(i) = \frac{p_i^\alpha}{\sum_k p_k^{\alpha}}\)</span>. <span class="arithmatex">\(\alpha\)</span>是一个超参数, 控制TD误差的影响程度, 当<span class="arithmatex">\(\alpha=0\)</span>的时候退化成均匀采样. 同时, 在实际采样的时候, 可以引入一定的随机性, 以增加样本的多样性, 避免集中在高TD误差的经验上, 同时保留了优先级的优势.</p>
<h2 id="_8">应用</h2>
<h3 id="alphago">AlphaGo</h3>
<p><a href="https://deepmind.google/research/breakthroughs/alphago/">AlphaGo</a>是从2014年开始由Google DeepMind开发的人工智能围棋软件. 它背后使用的是强化学习, 相关论文: <a href="https://www.science.org/doi/10.1126/science.aar6404"><em>A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play</em></a>.</p>
<h4 id="_9">神经网络</h4>
<p>AlphaGo中由两个神经网络: 策略网络(Policy Network)和价值网络(Value Network):</p>
<ul>
<li>策略网络: 用于生成围棋的落子策略, 对于当前棋盘的状态, 它输出每个可能落子位置的概率. 策略网络最初是通过人类专家对局的数据进行训练. 通过观察专家的走法, 它学习如何根据当前棋盘状态预测下一步的最佳落子位置, 这一阶段帮助AlphaGo建立基础的围棋理解, 模仿人类的策略. 在监督学习完成后, 策略网络进一步通过自我对弈的方式进行强化学习, 它不断和自己下棋, 通过优化获胜几率, 最终开发出新的策略</li>
<li>价值网络: 用于估计当前棋盘的胜负值, 它输出一个值, 该值表示在给定棋盘状态下获胜的可能性. 它的训练方式主要依靠强化学习, 通过大量的自我对弈, 它学会了在任意棋盘下预测当前局面的胜负可能性 </li>
</ul>
<h4 id="search-tree">搜索过程</h4>
<p>AlphaGo大大优化了搜索过程, 如果没有AlphaGo, 我们使用的是完整搜索树, 又叫"穷尽搜索", 其中的每个节点代表棋盘的一个状态, 每条路径代表可能的下一步行动, 这种方式会产生一个庞大的搜索树, 尤其在围棋这类游戏中, 完全搜索完所有的路径是不可能的, 因为节点数量会指数级增长. 策略网络和价值网络大大简化了这棵树:</p>
<ul>
<li>策略网络减少宽度: 策略网络能够减少在某个局面下需要考虑的落子位置数量. 它通过对可能的落子位置进行概率排序. 重点关注较高概率的几步, 从而缩小搜索空间</li>
<li>价值网络减少深度: 价值网络通过对局面进行评估, 提前得出某些路径的大概率结果, 减少搜索树的深度. 这样可以在关键位置提前终止搜索, 不必深入每一个分支</li>
</ul>
<h4 id="_10">训练流程</h4>
<p>AlphaGo的训练过程可以总结为:</p>
<ol>
<li>AlphaGo的自我对弈: AlphaGo在每个局面下使用策略网络和价值网络进行选择, 生成一系列局面, 这种自我对弈会产生大量数据</li>
<li>新的策略网络训练: AlphaGo通过自我对弈的结果, 改善策略网络</li>
<li>新的价值网络训练: AlphaGo通过自我对弈的结果, 改善价值网络</li>
<li>在下一轮迭代中使用新的策略网络和价值网络</li>
</ol>
<h4 id="_11">战果</h4>
<p>AlphaZero在不同的棋类游戏中进展神速.</p>
<ul>
<li>国际象棋: AlphaZero在四小时就超过了顶尖国际象棋引擎StockFish的水平</li>
<li>将棋: AlphaZero在两小时内超过了顶尖象棋引擎Elmo的水平</li>
<li>围棋: AlphaZero在八小时内就超过了AlphaGo的水平</li>
</ul>
<h4 id="_12">特点</h4>
<p>深度学习能够处理复杂棋类游戏中的庞大搜索空间, 通过神经网络预测落子位置和局面胜负概率, 大大提高了搜索效率, 见<a href="#search-tree">搜索过程</a>. 自我对弈生成了大量数据用于训练神经网络, 从简单对手逐步提升, 自动构建了一个由弱到强的训练体系, 它不仅可以帮助AlphaZero掌握既定策略, 还可以发现新的策略和方法, 提高棋艺水平.</p>
<h2 id="_13">深度强化学习</h2>
<p>RL是一种用于决策制定的通用框架, 旨在训练一个能够自主行动的智能体. 而DL是一种通用的表示学习框架, 用于学习达到目标所需的表示. 而深度强化学习DRL是RL和DL的集合, 通过利用深度神经网络来处理和强化学习过程中的庞大状态和动作空间. 在传统的强化学习中, 智能体的状态和动作空间是非常大或是连续的, 难以有效表示和处理, 而深度学习的优势在于它可以通过神经网络从复杂的高维数据中提取有效特征, 使得它称为强化学习的有效工具. 之前讲的深度Q学习算法就是DRL的一种, 除此之外, 还有策略梯度方法, Actor-Critic方法等等.</p>







  
    
  
  
    
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="最后更新">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">2024年10月31日</span>
  </span>

    
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="创建日期">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">2024年10月7日</span>
  </span>

    
    
    
      
  <span class="md-source-file__fact">
    
      
  <span class="md-icon" title="贡献者">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 2A10 10 0 0 0 2 12c0 4.42 2.87 8.17 6.84 9.5.5.08.66-.23.66-.5v-1.69c-2.77.6-3.36-1.34-3.36-1.34-.46-1.16-1.11-1.47-1.11-1.47-.91-.62.07-.6.07-.6 1 .07 1.53 1.03 1.53 1.03.87 1.52 2.34 1.07 2.91.83.09-.65.35-1.09.63-1.34-2.22-.25-4.55-1.11-4.55-4.92 0-1.11.38-2 1.03-2.71-.1-.25-.45-1.29.1-2.64 0 0 .84-.27 2.75 1.02.79-.22 1.65-.33 2.5-.33s1.71.11 2.5.33c1.91-1.29 2.75-1.02 2.75-1.02.55 1.35.2 2.39.1 2.64.65.71 1.03 1.6 1.03 2.71 0 3.82-2.34 4.66-4.57 4.91.36.31.69.92.69 1.85V21c0 .27.16.59.67.5C19.14 20.16 22 16.42 22 12A10 10 0 0 0 12 2"/></svg>
  </span>
  <span>GitHub</span>

    
    <nav>
      
        <a href="https://github.com/ricolxwz" class="md-author" title="@ricolxwz">
          
          <img src="https://avatars.githubusercontent.com/u/92409532?v=4&size=72" alt="ricolxwz">
        </a>
      
      
      
    </nav>
  </span>

    
  </aside>





  <h2 id="__comments">评论</h2>
  <script src="https://giscus.app/client.js"
        data-repo="sigmax01/ml"
        data-repo-id="R_kgDOMNX2Cw"
        data-category="Announcements"
        data-category-id="DIC_kwDOMNX2C84CgVid"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="preferred_color_scheme"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>

  <!-- Synchronize Giscus theme with palette -->
  <script>
    var giscus = document.querySelector("script[src*=giscus]")

    // Set palette on initial load
    var palette = __md_get("__palette")
    if (palette && typeof palette.color === "object") {
      var theme = palette.color.scheme === "slate"
        ? "transparent_dark"
        : "light"

      // Instruct Giscus to set theme
      giscus.setAttribute("data-theme", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener("DOMContentLoaded", function() {
      var ref = document.querySelector("[data-md-component=palette]")
      ref.addEventListener("change", function() {
        var palette = __md_get("__palette")
        if (palette && typeof palette.color === "object") {
          var theme = palette.color.scheme === "slate"
            ? "transparent_dark"
            : "light"

          // Instruct Giscus to change theme
          var frame = document.querySelector(".giscus-frame")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            "https://giscus.app"
          )
        }
      })
    })
  </script>

                

              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  回到页面顶部
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      <p xmlns:cc="http://creativecommons.org/ns#" >版权所有 &copy 2024-至今 由 <span property="cc:attributionName">许文泽</span> 采用 <a href="https://creativecommons.org/licenses/by-nc/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC 4.0&nbsp</a>许可证发布</p>
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://buymeacoffee.com/ricolxwz" target="_blank" rel="noopener" title="buymeacoffee.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d="M88 0C74.7 0 64 10.7 64 24c0 38.9 23.4 59.4 39.1 73.1l1.1 1C120.5 112.3 128 119.9 128 136c0 13.3 10.7 24 24 24s24-10.7 24-24c0-38.9-23.4-59.4-39.1-73.1l-1.1-1C119.5 47.7 112 40.1 112 24c0-13.3-10.7-24-24-24zM32 192c-17.7 0-32 14.3-32 32L0 416c0 53 43 96 96 96l192 0c53 0 96-43 96-96l16 0c61.9 0 112-50.1 112-112s-50.1-112-112-112l-48 0L32 192zm352 64l16 0c26.5 0 48 21.5 48 48s-21.5 48-48 48l-16 0 0-96zM224 24c0-13.3-10.7-24-24-24s-24 10.7-24 24c0 38.9 23.4 59.4 39.1 73.1l1.1 1C232.5 112.3 240 119.9 240 136c0 13.3 10.7 24 24 24s24-10.7 24-24c0-38.9-23.4-59.4-39.1-73.1l-1.1-1C231.5 47.7 224 40.1 224 24z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://ricolxwz.de" target="_blank" rel="noopener" title="ricolxwz.de" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d="M256 512A256 256 0 1 0 256 0a256 256 0 1 0 0 512zm50.7-186.9L162.4 380.6c-19.4 7.5-38.5-11.6-31-31l55.5-144.3c3.3-8.5 9.9-15.1 18.4-18.4l144.3-55.5c19.4-7.5 38.5 11.6 31 31L325.1 306.7c-3.2 8.5-9.9 15.1-18.4 18.4zM288 256a32 32 0 1 0 -64 0 32 32 0 1 0 64 0z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/ricolxwz" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://gitlab.com/ricolxwz" target="_blank" rel="noopener" title="gitlab.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="m503.5 204.6-.7-1.8-69.7-181.78c-1.4-3.57-3.9-6.59-7.2-8.64-2.4-1.55-5.1-2.515-8-2.81s-5.7.083-8.4 1.11c-2.7 1.02-5.1 2.66-7.1 4.78-1.9 2.12-3.3 4.67-4.1 7.44l-47 144H160.8l-47.1-144c-.8-2.77-2.2-5.31-4.1-7.43-2-2.12-4.4-3.75-7.1-4.77a18.1 18.1 0 0 0-8.38-1.113 18.4 18.4 0 0 0-8.04 2.793 18.1 18.1 0 0 0-7.16 8.64L9.267 202.8l-.724 1.8a129.57 129.57 0 0 0-3.52 82c7.747 26.9 24.047 50.7 46.447 67.6l.27.2.59.4 105.97 79.5 52.6 39.7 32 24.2c3.7 1.9 8.3 4.3 13 4.3s9.3-2.4 13-4.3l32-24.2 52.6-39.7 106.7-79.9.3-.3c22.4-16.9 38.7-40.6 45.6-67.5 8.6-27 7.4-55.8-2.6-82"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://hub.docker.com/u/ricolxwz" target="_blank" rel="noopener" title="hub.docker.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://t.me/ricolxwz" target="_blank" rel="noopener" title="t.me" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d="M248 8C111 8 0 119 0 256S111 504 248 504 496 393 496 256 385 8 248 8zM363 176.7c-3.7 39.2-19.9 134.4-28.1 178.3-3.5 18.6-10.3 24.8-16.9 25.4-14.4 1.3-25.3-9.5-39.3-18.7-21.8-14.3-34.2-23.2-55.3-37.2-24.5-16.1-8.6-25 5.3-39.5 3.7-3.8 67.1-61.5 68.3-66.7 .2-.7 .3-3.1-1.2-4.4s-3.6-.8-5.1-.5q-3.3 .7-104.6 69.1-14.8 10.2-26.9 9.9c-8.9-.2-25.9-5-38.6-9.1-15.5-5-27.9-7.7-26.8-16.3q.8-6.7 18.5-13.7 108.4-47.2 144.6-62.3c68.9-28.6 83.2-33.6 92.5-33.8 2.1 0 6.6 .5 9.6 2.9a10.5 10.5 0 0 1 3.5 6.7A43.8 43.8 0 0 1 363 176.7z"/></svg>
    </a>
  
    
    
    
    
    <a href="mailto:ricol.xwz@outlook.com" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d="M256 64C150 64 64 150 64 256s86 192 192 192c17.7 0 32 14.3 32 32s-14.3 32-32 32C114.6 512 0 397.4 0 256S114.6 0 256 0S512 114.6 512 256l0 32c0 53-43 96-96 96c-29.3 0-55.6-13.2-73.2-33.9C320 371.1 289.5 384 256 384c-70.7 0-128-57.3-128-128s57.3-128 128-128c27.9 0 53.7 8.9 74.7 24.1c5.7-5 13.1-8.1 21.3-8.1c17.7 0 32 14.3 32 32l0 80 0 32c0 17.7 14.3 32 32 32s32-14.3 32-32l0-32c0-106-86-192-192-192zm64 192a64 64 0 1 0 -128 0 64 64 0 1 0 128 0z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["content.code.annotate", "content.code.copy", "content.code.select", "content.tabs.link", "content.footnote.tooltips", "content.tooltips", "content.action.edit", "content.action.view", "navigation.tabs", "navitation.sections", "navigation.expand", "navigation.indexes", "navigation.top", "navigation.tracking", "search.suggest", "search.highlight", "search.share"], "search": "../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
        <script src="../../javascripts/favicon.js"></script>
      
        <script src="../../javascripts/katex.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
      
        <script src="../../javascripts/analysis.js"></script>
      
        <script src="../../js/open_in_new_tab.js"></script>
      
    
  <script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body>
</html>