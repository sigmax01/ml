{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 朴素贝叶斯(练习1)\n",
    "\n",
    "<style>\n",
    ".CodeMirror pre {\n",
    "    white-space: pre-wrap;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "朴素贝叶斯是一种高度基于训练集的算法. 这种模型/分类器的训练速度非常快, 以高斯朴素贝叶斯为例, 它只需要计算需要的统计参数, 如均值和方差, 还有标签中每个类别的概率, 从面上来看, 好像没有训练过程. 因为如同线性回归这样的算法, 会经过不断的优化产生回归系数, 这个回归系数就构成了模型. 但是朴素贝叶斯没有这样的过程, 它只需要计算一些统计参数, 然后就可以进行预测了. 预测的过程会利用这些统计参数, 结合特征在标签下的条件概率, 然后得到一个后验概率, 这个后验概率就用于预测了."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T04:54:12.397294Z",
     "start_time": "2024-08-24T04:54:11.798087Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy import signal\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#for accuracy_score, classification_report and confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will firstly learn how to create a Naive Bayes (NB) classifier in sklearn. Then we will learn how to obtain different performance metrics (precision, recall, F1-score and confusion matrix), how to apply different procedures for evaluating the performance of classifiers (cross-validation and leave-one-out) and finally, how to use grid search with cross-validation for model selection. \n",
    "\n",
    "首先, 我们会学习如何创造一个朴素贝叶斯分类器(用Sklearn). 然后我们会学习如何获取衡量性能的不同目标(精度, 召回率, F1分数和混淆矩阵), 如何应用不同的程序来评估分类器的性能(交叉验证和留一法), 最后, 如何使用带有交叉验证的网格搜索来选择模型."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating an NB classifier\n",
    "\n",
    "There are four main types of NB classifiers in sklearn:  <b>GaussianNB</b>, <b>CategoricalNB</b>,  <b>MultinomialNB</b> and <b>BernoulliNB</b>. \n",
    "\n",
    "- The first two are the ones we discussed at the lecture -  <b>GaussianNB</b> is applicable to numeric data, while <b>CategoricalNB</b> is applicable to categorical data. \n",
    "\n",
    "- <b>BernoulliNB</b> and <b>MultinomialNB</b> are mostly used for text clasification; they assume binary data and count data respectively, e.g. how many times a word appears in a document.\n",
    "\n",
    "In this tutorial we will create a NB for the iris data which is a numeric dataset, so we will use the <b>GaussianNB</b> class to create the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the iris data and create the training and test splits:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T05:53:00.008606Z",
     "start_time": "2024-08-24T05:52:59.989917Z"
    }
   },
   "source": [
    "# load the iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "# create the training and test splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, stratify=iris.target, random_state=42) # stratify的作用是分层, 因为当将数据集随机分为训练集和测试集的时候, 有时候会出现某类别的数据在训练集或者测试集中缺失或者失衡的情况. 比如, 如果某一类别的所有样本都分配到测试集中, 那么模型在训练的时候将无法学习如何预测这个类别, 因为训练集中没有该类别的样本. 为了避免这种情况, 可以使用分层的方法, 即stratification. 例如, 如果整个数据集中某一个类别的比例是60%, 另一个类别的比例是40%, 那么分层抽样的时候, 训练集和测试集中该类别的比例也会是60%和40%. random_state是随机种子, 为了保证每次运行代码时, 随机分配的结果都是一样的."
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T06:12:21.816724Z",
     "start_time": "2024-08-24T06:12:21.813173Z"
    }
   },
   "cell_type": "code",
   "source": "X_train",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.4, 2.8, 6.1, 1.9],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [5.9, 3. , 5.1, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.1, 3.5, 1.4, 0.2],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [4.6, 3.2, 1.4, 0.2]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T05:53:15.338476Z",
     "start_time": "2024-08-24T05:53:15.336167Z"
    }
   },
   "cell_type": "code",
   "source": "iris.target # 这个应该是这个数据集的标签",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task:\n",
    "Use GaussianNB from sklearn.naive_bayes to create an NB classifier on the training data and evaluate its acuracy on the test data.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T06:08:32.789701Z",
     "start_time": "2024-08-24T06:08:32.787786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 导包\n",
    "from sklearn.naive_bayes import GaussianNB"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T06:10:34.180862Z",
     "start_time": "2024-08-24T06:10:34.176780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "y_pred"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0, 1, 1, 2, 2, 2, 1, 2, 1, 1, 0, 0, 0, 1, 0, 1, 2, 1,\n",
       "       2, 1, 2, 1, 0, 2, 0, 2, 2, 2, 0, 0, 0, 0, 2, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T06:14:23.918991Z",
     "start_time": "2024-08-24T06:14:23.914765Z"
    }
   },
   "cell_type": "code",
   "source": "accuracy_score(y_test, y_pred) # 使用sklearn.metrics.accuracy_score来计算准确率",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9210526315789473"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. More performance measures: precision, recall and F1 score. Confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to accuracy, we can calculate other performance measures - e.g. precision, recall and their combination - the F1-score. In sklearn this can be convenintly done using the <b>classification_report</b> method, which also shows the accuracy. The confusion matrix can be calculated using the <b>confusion_matrix</b> method.\n",
    "\n",
    "### Task: \n",
    "1) Continuing on the previous exercise (NB classifier on the iris data), write the Python code to calculate precision, recall, F1 measure and confusion matrix on the test set by using the methods <b>classification_report</b> and <b>confusion_matrix</b>.\n",
    "\n",
    "See:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "2) Examine the results:\n",
    "- How are the precision, recall and F1-score calculated - per class or overall for the test set?\n",
    "- Making sense of the confusion matrix: Where are the correctly classified examples? How many examples from class 1 are incorrectly classified as class 2?\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T06:16:36.414737Z",
     "start_time": "2024-08-24T06:16:36.411071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 得到混淆矩阵\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12,  0,  0],\n",
       "       [ 0, 12,  1],\n",
       "       [ 0,  2, 11]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "混淆矩阵的$C_{ij}$的意思是说: 实际上是第$i$类, 但是预测为了第$j$类的样本数. 例如, $C_{12}$的意思是说: 实际上是第$1$类, 但是预测为了第$2$类的样本数."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T06:20:52.018360Z",
     "start_time": "2024-08-24T06:20:52.013200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 得到分类报告\n",
    "metrics.classification_report(y_test, y_pred, output_dict=True) # 可以看到里面的accuracy和上面我们算出来的是一样的"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 12.0},\n",
       " '1': {'precision': 0.8571428571428571,\n",
       "  'recall': 0.9230769230769231,\n",
       "  'f1-score': 0.8888888888888888,\n",
       "  'support': 13.0},\n",
       " '2': {'precision': 0.9166666666666666,\n",
       "  'recall': 0.8461538461538461,\n",
       "  'f1-score': 0.88,\n",
       "  'support': 13.0},\n",
       " 'accuracy': 0.9210526315789473,\n",
       " 'macro avg': {'precision': 0.9246031746031745,\n",
       "  'recall': 0.923076923076923,\n",
       "  'f1-score': 0.9229629629629629,\n",
       "  'support': 38.0},\n",
       " 'weighted avg': {'precision': 0.9226190476190477,\n",
       "  'recall': 0.9210526315789473,\n",
       "  'f1-score': 0.9209356725146198,\n",
       "  'support': 38.0}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-validation for evaluating performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation, in particular <b>10-fold stratified cross-validation</b>, is the standard method in machine learning for evaluating the performance of classification and prediction models. Recall that we are interested in the generalization performance, i.e. how well a classifier will perform on new, previously unseen data.\n",
    "\n",
    "To perform cross-validation in sklearn, we can use the <b>cross_val_score</b> function. It takes as parameters the classifier we would like to evaluate and the data - the feature vectors and the target classes (also caled ground-truth labels). The parameter <b>cv</b> specifies the number of folds; the default value is 3, so we need to set it to 10 for 10-fold cross-validation. Note that this function performs <b>stratified</b> cross validation for classification tasks. \n",
    "\n",
    "Let's evaluate our NB classifier using 10-fold cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.93333333 0.93333333 1.         0.93333333 0.93333333 0.93333333\n",
      " 0.86666667 1.         1.         1.        ]\n",
      "Average cross-validation score: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(nb, iris.data, iris.target, cv=10)\n",
    "print(\"Cross-validation scores: {}\".format(scores)) #accuracy for each fold\n",
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean())) #average accuracy over all folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important result is the average cross-validation score (the average accuracy over the the 10 folds) but it is also useful to look at the accuracy for each fold. In our case, we can see that there is a relatively high variation between the 10 folds  - from 86% to 100% accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: \n",
    "Compare NB's cross-validation accuracy with NB's accuracy when we used a single training/test split. How can you explain the difference? Which is the more reliable measure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-one-out cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave-one-out is a special case of cross-validation where each fold is a single example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evaluations:  150\n",
      "Mean accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "one_out = LeaveOneOut()\n",
    "scores = cross_val_score(nb, iris.data, iris.target, cv=one_out)\n",
    "print(\"Number of evaluations: \", len(scores))\n",
    "print(\"Mean accuracy: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task:\n",
    "What are the advantages and disadvantages of leave-one-out?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Grid search with cross-validation for parameter selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve the generalization performance of machine learning algorithms by tuning their parameters. NB doesn't have many parameters but in the previous weeks we saw that the performance of k-Nearest Neighbor algorithm depends on the number of neighbors and distance measure type, the performance of regression models depends on the values of <b>alpha</b> and <b>C</b>. \n",
    "\n",
    "In sklearn we can use grid search with cross-validation to search through different parameter combinations and select the best one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider k-nearest neighbor (k-NN) as an example and tune two of its parameters by considering the following values:\n",
    "- number of neareast neighbours <b>n_neighbours</b> = 1, 3, 5, 11 and 13\n",
    "- distance measure - Manhattan and Euclidean, which can be controlled by the value of parameter <b>p</b>, 1 or 2 respectively\n",
    "\n",
    "This gives us 5 x 2 combinations of paramneter values. We would like to find the best combination - the one that we expect to generalise well on new examples.\n",
    "\n",
    "We will use the following procedure, called <b>grid-search with cross-validation for parameter tuning</b>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudocode:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the parameter grid (i.e. the parameter combinations)\n",
    "Split the data into training set and test set\n",
    "For each parameter combination\n",
    "    Train a k-NN classifier on the training data using 10-fold cross-validation as an evaluation procedure\n",
    "    Compute the cross-validation accuracy cv_acc\n",
    "    If cv_acc > best_cv_acc \n",
    "       best_cv_acc = cv_acc\n",
    "       best_parameters = current parameters\n",
    "Rebuild the k-NN model using the whole training data and best_parameters\n",
    "Evaluate it on the test data and report the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data is split into training set and test set\n",
    "- The cross-validation loop uses the training data. It is performed for every parameter combination. Its purpose is to select the best parameter combination - the one with the highest cross-validation accuracy. This involves, for every parameter combination, building 10 models on 90% of the training data (9 folds) and evaluating them on the remaining 10% (1 fold).\n",
    "- Once this is done, a new model is trained using the selected best parameter combination on the <b>whole training set</b> and evaluated on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter grid:\n",
      "{'n_neighbors': [1, 3, 5, 11, 15], 'p': [1, 2]}\n",
      "Test set score: 0.95\n",
      "Best parameters: {'n_neighbors': 11, 'p': 1}\n",
      "Best cross-validation score: 0.98\n",
      "Best estimator:\n",
      "KNeighborsClassifier(n_neighbors=11, p=1)\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_neighbors': [1, 3, 5, 11, 15],\n",
    "              'p': [1, 2]}\n",
    "print(\"Parameter grid:\\n{}\".format(param_grid))\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=10,\n",
    "                          return_train_score=True)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Test set score: {:.2f}\".format(grid_search.score(X_test, y_test)))\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "print(\"Best estimator:\\n{}\".format(grid_search.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: We create an object of type GridSearchCV, which is then fitted to the <b>training</b> data. This fitting includes 2 things: \n",
    "1. Searching for and determining the best parameter combination - the one with the best cross-validation accuracy <b>and</b> \n",
    "2. Building a new model on the whole training set with the best parameter combination from 1.\n",
    "\n",
    "It is important to understand the difference between <b>best cross-validation score</b> and <b>test set score </b>:\n",
    "- <b>best cross-validation score</b> is the mean cross-validation accuracy, with cross-validation performed on the <b>training set</b>. This step involves building 10 models on the training data, each time using 9 folds together (90% of the training data) to create the model and testing this model the remaining 10th fold (10% of the training data). The purpose of this step is to select the best parameter combination, which is the one with the highest cross-validation accuracy.\n",
    "- <b>test set score</b> - this is the the accuracy on the test of a model that was created using the <b>whole training set</b> (100%) with the selected parameters. This is the result that we report as a measure of generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 0.95\n"
     ]
    }
   ],
   "source": [
    "nb = GaussianNB().fit(X_train, y_train)\n",
    "scores = cross_val_score(nb, iris.data, iris.target, cv=10)\n",
    "scores = cross_val_score(nb, iris.data, iris.target, cv=one_out)\n",
    "\n",
    "param_grid = {'n_neighbors': [1, 3, 5, 11, 15], 'p': [1, 2]}\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=10, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Test set score: {:.2f}\".format(grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "This tutorial is based on:\n",
    "\n",
    "Aurelien Geron (2022). Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow, O'Reilly.\n",
    "\n",
    "Andreas C. Mueller and Sarah Guido (2016). Introduction to Machine Learning with Python: A Guide for Data Scientists, O'Reilly.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
