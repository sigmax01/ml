---
title: 递归神经网络
comments: true
---

递归神经网络, Recurrent Neural Networks, RNN, 用于处理序列化的数据. 文字或者自然语言就是一种序列化的数据, 段落就是单词的序列, 每个段落可能有不同数量的单词, 每个段落中可能包含有长距离依赖关系的单词, 如`The dog in that house is aggressive"和"The dogs in that house are aggressive"中的🐕和谓语之间的长距离依赖关系. 处理这种类型的数据需要模型能够"记住"之前序列中的某些信息.

## 架构

RNN一次处理序列中的一个元素, 例如一个单词. 举例来说, 句子"The dog in the house is aggreesive"会按照时间顺序分解为: $t$时刻处理"the", $t+1$时刻处理"dog"依次类推. RNN之所以被称为"递归神经网络"是因为它包含反馈连接, 即输出会反馈给输入, 这与前馈神经网络不同, 前馈神经网络是有向无环图, acyclic graph, 而递归神经网络是有向有环图, cyclic graph, 更具体的说, 一个神经元在某个时间步的输出会反馈到下一个时间步的相同神经元中, 因此, RNN具备一定的记忆能力, 可以记住之前的激活状态, 由于RNN能够记住过去的激活状态, 因此它能够捕捉长距离的依赖关系, 这种特性使得RNN特别适合用于处理序列数据, 例如语言模型和时间序列预测.

RNN有不同的架构, 其中包括简单的RNN, 又称为Elman网络和长短期记忆网络, LSTM. 

### 简单RNN

简单RNN含有由$1$个隐藏层构成的前馈神经网络, 这个隐藏层特别的, 含有一个记忆缓存, 会存储隐藏层之前一个时间步的状态. 在每一个时间步, 记忆缓存中的数据会和下一组输入结合作为隐藏层神经元的下一次输入.

<figure markdown='1'>
![](https://img.ricolxwz.io/9af1c54b02f52241514fdb945d00fa2b.png){ loading=lazy width='600' }
</figure>

我们定义如下符号:

- $x_t$: $t$时刻的输入向量
- $h_t$: $t$时刻的隐藏层激活值, 表示经过隐藏层计算后的中间状态. 在图中$h_t$会结合前一个时间步的隐藏状态$h_{t-1}$以及当前时间步的输入向量$x_t$进行计算
- $y_t$: $t$时刻的输出向量
- $\bm{w}_{xh}$: 输入层和隐藏层之间的权重
- $\bm{w}_{hy}$: 隐藏层和输出层之间的权重
- $\bm{w}_{hh}$: 记忆缓存和隐藏层之间的权重

我们就来考虑上述的情况, $2$个输出层神经元, $3$个隐藏层神经元, $2$个输出层神经元. 在每一个时间步都会有一个新的输入向量. 每个隐藏层神经元都会接收到所有输入层神经元的输出和记忆缓存中的上一个状态信息. 隐藏层处理后, 将结果传递给输出层神经元, 同时将状态写到记忆缓存中.

$h_t$的计算公式为$h_t = f_{\bm{w}}(h_{t-1}, x_t)$. 其中$h_{t-1}$是旧状态, $x_t$是本次的输入向量. 函数$f_{\bm{w}}$通常为反正切函数, 即$h_t=\tanh (\bm{w}_{hh}h_{t-1}+\bm{w}_{xh}x_t+b_h)$. 而输出$y_t = \bm{w}_{hy}h_t+b_y$.

<figure markdown='1'>
![](https://img.ricolxwz.io/11265e875c7e0b8f052246f6603345f0.png){ loading=lazy width='450' }
</figure>

#### 展开

在上面RNN的架构图中, 这种循环结构使得理解在多个时间步之间的信息流动变得困难. 为了更好地理解RNN的信息流动的过程, 我们可以将RNN沿着时间展开, unroll, 然后每一个时间步的计算看作是一个独立的前馈神经网络. 展开后的RNN就像一系列相互连接的前馈神经网络层, 这样可以帮助我们更清晰地看到每个时间步的输入, 隐藏状态和输出之间的关系, 如下图所示.

<figure markdown='1'>
![](https://img.ricolxwz.io/b84aa790eb9534370b8dd379974e2801.png){ loading=lazy width='350' }
</figure>

注意, 上图中可以明显看到, 权重$\bm{w}_{xh}$, $\bm{w}_{hy}$, $\bm{w}_{hh}$在每个时间步都是"共享"的, RNN在处理数据的时候, 会不断通过多个时间步来传播信息, 如果每个时间步的权重都不同, 那么模型会变得非常复杂, 不容易训练, 因此, 这种RNN采用的是权重共享机制.

#### 通过时间反向传播

通过时间反向传播, Backpropagation Through Time, BPTT是训练RNN的关键算法, 它是标准的反向传播算法的扩展, 用来处理时间序列数据中的依赖关系. 

RNN的损失函数是每个时间步的输出$y_t$和真实标签$y_t^{true}$之间的误差之和, 假设我们使用均方误差作为损失函数, 那么损失$L$为$L=\sum_{t=1}^T(y_t-y_t^{true})^2$. 目标是通过最小化这个损失函数来更新RNN的权重. 在$t$这个时间步的误差被反向传播到所有对其有贡献的参数, 这意味着它向网络先前的状态传播, 所以叫做"时间倒流". 如图所示.

<figure markdown='1'>
![](https://img.ricolxwz.io/e6fcd9d89c50433e4a2291b36ca56da2.png){ loadin=lazy width='600' }
</figure>

举一个例子:

假设我们有$4$个字符: $\{h, e, l, o\}$, 然后我们用"hello"这个单词进行训练.

1. 问题描述: 我们希望训练一个RNN, 根据前面的字符串来预测下一个字符, 对于单词"hello", 输入序列: "h", "e", "l", "l"; 目标序列: "e", "l", "l", "o"
2. 网络结构: 
    - 输入层: 每个字符采用one-hot编码, 维度为$4$, 例如, 对于"h"来说, 编码为$1000$, 对于"e"来说, 编码为$0100$
    - 隐藏层: 隐藏状态的维度设置为$n$(可自定义)
    - 输出层: 输出对下一个字符的预测, 维度为$4$
3. 正向传播:
    1. 初始化隐藏状态$h_0$, 通常设置为零向量或者随机初始化
    2. 对于每一个时间步$t$:
        1. 输入向量$x_t$: 当前字符的one-hot编码
        2. 计算隐藏状态$h_t$: $h_t=\tanh(\bm{w}_{xh}x_t+\bm{w}_{hh}h_{t-1}+b_h)$
        3. 计算输出$y_t$: $y_t=\bm{w}_{hy}h_t+b_y$
        4. 预测下一个字符的概率分布: $\hat{y_t}=softmax(y_t)$
        5. 计算损失: $L_t=-\sum_{i=1}^4 y_{t, i}\log(\hat{y}_{t, i})$
4. 损失累积
    - 何时发生: 完成所有时间步的正向传播后
    - 总损失计算: $L=\sum{t=1}^T L_t$
5. 通过时间反向传播
    - 何时发生: 完成整个序列的正向传播和总损失计算后
    - 具体过程:
        从时间步$t=T$反向到$t=1$: