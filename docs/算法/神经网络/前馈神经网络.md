---
title: 前馈神经网络
comments: true
---

## 神经元

神经网络由神经元(单元, 节点)组成, 这些神经元通过有向链接互相连接, 每个连接都有一个相关的数值权重. 神经元会组成层状结构, 包括输入层, 输出层或多个隐藏层. 在训练的过程中, 权重会被调整, 以学习执行一个特定的任务. 

在生物神经网络中, 每个神经元与其他神经元相连, 当它兴奋的时候, 就会向相连的神经元发送化学物质, 从而改变这些神经元内的电位, 如果某神经元的电位超过一个阈值, 那么它就会被激活, 即兴奋起来, 向其他神经元发送化学物质. 机器学习中的神经元也类似, 神经元接受到其他$n$个神经元传递过来的输入信号, 这些输入信号通过带权重的连接进行传递, 神经元接收到总输入将与神经元的阈值进行比较, 然后通过"激活函数"处理以产生神经元的输出.

1943年, McMulloch和Pittes将上述情景抽象, 这就是一直沿用至今的"M-P"神经元模型:

<figure markdown='1'>
![](https://img.ricolxwz.io/9e2e01f533f1d459d48b80f753580c0c.png){ loading=lazy width='500' }
</figure> 

每一个神经元包含:

- 输入向量$\bm{x}$, 表示输入数据, 其中每个元素$x_1, x_2, ..., x_n$, 对应$n$个特征
- 权重向量$\bm{w}$: 每个输入都有一个对应的权重$w_1, w_2, ..., w_n$, 对应$n$个参数
- 偏置$b$: 是一个常数, 在将输入加权求和后添加, 用于平移激活曲线
- 求和$\sum$: 输入$x_1, x_2, ..., x_n$和权重$w_1, w_2, ..., w_n$相乘相加, 加上偏置值, 结果表达式为$\bm{w}\bm{x}+b$
- 传递函数$f$: 又叫激活函数, 是将求和结果进行映射并生成输出, 其中$a=f(\bm{w}\bm{x}+b)$ 
- 输出$a$: 对加权和及偏置应用激活函数后产生的最终结果

<figure markdown='1'>
![](https://img.ricolxwz.io/f2b529bb036ac241e7d09acd263e4c90.png){ loading=lazy width='500' }
</figure>

理想的激活函数应该是阶跃函数(如上左图所示), 它将输入值映射为输出值为$0$或$1$, $1$对应于神经元兴奋, $0$对应于神经元抑制. 但是阶跃函数具有不连续, 不光滑等不太好的特性, 因此实际常用sigmoid函数作为激活函数(如上右图所示).

## 感知机

感知机, Perceptron, 它是最简单的神经网络, 由两层神经元组成, 输入层接受外界输入的信号后传递给输出层, 输出层是M-P神经元, 也被称为"阈值逻辑单元", 激活函数为阶跃函数.

<figure markdown='1'>
![](https://img.ricolxwz.io/c4090b80e90cb85ba787e93a035cb384.png){ loading=lazy width='300' }
</figure>

它由Frank Rosenblatt在1957年提出. 同时它的局限性由Marvin Minsky和Seymour Papert在书<Perceptrons>中提出. Rosenblatt和他的同事意识到这个局限可以通过使用更复杂的NNs, 即multi-layer perceptrons解决, 但是他们没有将感知机使用于神经网络的训练.

### 学习算法

那么, 感知机是如何进行学习的呢?

之前我们说过, 神经元会在训练过程中调整权重$w_1, w_2$, 这个其实就是感知机学习的过程. 权重的更新公式为$\bm{w}^{new}=\bm{w}^{old}+e\bm{x}^T$, $e=t-a$, $t$为目标输出($0$或$1$), $a$为实际输出($0$或$1$), $\bm{x}$为输入向量; 同时还要调整截距, $b^{new}=b^{old}+e$

- 当$e=1$的时候, $\bm{w}^{new}=\bm{w}^{old}+\bm{x}^T$, $b^{new}=b^{old}+1$
- 当$e=-1$的时候, $\bm{w}^{new}=\bm{w}^{old}-\bm{x}^T$, $b^{new}=b^{old}-1$
- 当$e=0$的时候, $\bm{w}^{new}=\bm{w}^{old}$, $b^{new}=b^{old}$

具体算法如下:

1. 初始化权重和截距$\bm{w}$, $b$为小的随机数, 设置当前迭代次数为$1$
2. 对于每一个训练样本$\{\bm{x}, t\}$
    1. 计算$a$, 这一步又叫作网络激活
    2. 计算误差$e=t-a$
    3. 更新权重和截距: $\bm{w}^{new}=\bm{w}^{old}+e\bm{x}^T$, $b^{new}=b^{old}+e$
3. 在每次迭代结束时(即循环一遍所有的训练样本)检查是否满足停止条件, 如果所有的样本都被正确分类, 或者训练次数达到最大迭代次数, 则停止训练, 否则继续迭代, 执行第2步

???+ example "例子"

    给出下列的训练样本:

    |序号|特征(输入)|标签(输出)|
    |-|-|-|
    |1|1 0 0|0|
    |2|1 0 1|1|
    |3|1 1 0|0|

    初始权重为$\bm{w}=[0.3\ 0.2\ 0.4]$, 初始截距为$b=0.1$. 最大迭代次数为$5$.

    === "第一个样本 ---->"

        - $a=sign([0.3\ 0.2\ 0.4][1\ 0\ 0]+0.1)=sign(0.4)=1$, 错误, $e=0-1=-1$
        - $\bm{w}^{new}=[0.3\ 0.2\ 0.4] + (-1)[1\ 0\ 0]=[-0.7\ 0.2\ 0.4]$
        - $b^{new}=0.1+(-1)=-0.9$

    === "第二个样本 ---->"

        - $a=sign([-0.7\ 0.2\ 0.4][1\ 0\ 1]-0.9)=sign(-1.2)=0$, 错误, $e=1-0=1$
        - $\bm{w}^{new}=[-0.7\ 0.2\ 0.4] + (1)[1\ 0\ 1]=[0.3\ 0.2\ 1.4]$
        - $b^{new}=-0.9+1=0.1$

    === "第三个样本"

        - $a=sign([0.3\ 0.2\ 1.4][1\ 1\ 1]+0.1)=sign(0.6)=1$, 错误, $e=0-1=-1$
        - $\bm{w}^{new}=[0.3\ 0.2\ 1.4] + (-1)[1\ 1\ 1]=[-0.7\ -0.8\ 1.4]$
        - $b^{new}=0.1-1=-0.9$

    第一轮迭代结束后的权重$\bm{w}=[-0.7\ -0.8\ -1.4]$, 截距$b=-0.9$, 检查:

    1. 所有的样本都被正确分类? 
        1. 第一个样本: $a=sign([-0.7\ -0.8\ 1.4][1\ 0\ 0]-0.9)=sign(-1.6)=0$, 正确✅
        2. 第二个样本: $a=sign([-0.7\ -0.8\ 1.4][1\ 0\ 1]-0.9)=sign(-0.2)=0$, 错误❎
        3. 第三个样本: 无需检查第三个样本, 因为第二个样本已经错误
    2. 达到最大迭代次数? 🈚️

    进入第二轮迭代...

### 限制条件

如果训练样本是线性可分的, 即样本可以用一条直线在超平面(1)中分开, 感知机学习算法保证能够在有限步骤内找到一组权重和截距, 这组权重和截距能正确地将所有的训练样本分类. 这时候, 感知机将会找到一个线性的决策边界, 但不一定是"最优"的边界, 而是找到一个可行的边界后就会停止. 在现实世界中, 大多数的问题都是线性不可分的, 这意味着训练样本无法用一个超平面正确分开, 这也是感知机的最大局限之一.
{.annotate}

1. 什么是超平面, 可以在[这里](/算法/支持向量机#边际最大超平面)找到.

### 逻辑门

=== "与门"

    感知机能够实现与门, 这是一个线性可分的问题, 如图所示.

    <figure markdown='1'>
    ![](https://img.ricolxwz.io/46ab58645bfbd9676237b0a659b777b2.png){ loading=lazy width='300' }
    </figure>

    感知机能够找到一个线性的决策边界, 如$w_1=1=w_2=1, b=2$, 即$y=sign(x_1+x_2-2)$.

=== "或门"

    感知机能够实现或门, 这是一个线性可分的问题, 如图所示.

    <figure markdown='1'>
    ![](https://img.ricolxwz.io/b08609a89b6033ac9fae3be1f27257a3.png){ loading=lazy width='300' }
    </figure>

    感知机能够找到一个线性的决策边界, 如$w_1=w_2=1, b=0.5$, 即$y=sign(x_1+x_2-0.5).$

=== "与非门"

    感知机能够实现与非门, 这是一个线性可分的问题, 如图所示.

    <figure markdown='1'>
    ![](https://img.ricolxwz.io/4abb87c91ddaecb717517d3b804c5dc1.png){ loading=lazy width='300' }
    </figure>

=== "异或门"

    感知机无法实现异或门, 这不是一个线性可分的问题, 如图所示.
    <figure markdown='1'>
    ![](https://img.ricolxwz.io/0a17bba3cfe79eb90d6be35c888c0e0d.png){ loading=lazy width='300' }
    </figure>

    然而, 异或门可以通过与门, 非门和与非门的组合实现, 如图所示.

    <figure markdown='1'>
    ![](https://img.ricolxwz.io/363ac96f910fd04b3f83b13284af5a29.png){ loading=lazy width='300' }
    </figure>

    所以, 只要使用一个两层的感知机就能解决异或问题.

    <figure markdown='1'>
    ![](https://img.ricolxwz.io/710cecd9811caa570be36bda6616991c.png){ loading=lazy width='300' }
    </figure>

---

从上面的实验中, 我们得出结论, 如果增加更多的层, 可以得到更加复杂的决策边界, 如图所示.

<figure markdown='1'>
![](https://img.ricolxwz.io/7018b43e284156ad9fbd3b1c3e4d240f.png){ loading=lazy width='800' }
</figure>

## 前馈神经网络

### 架构

前馈神经网络, Feedforward NN, 的架构如图所示.

<figure markdown='1'>
![](https://img.ricolxwz.io/182181d298a387078d650701b708d254.png){ loading=lazy width='500' }
</figure>

具体说明如下:

- 输入层: 位于网络最底层, 输入变量$x_1, x_2, ..., x_n$表示输入特征. 每个输入节点代表一个特征, 输入层只复杂将数据传递到下一层(隐藏层), 不执行任何计算
- 隐藏层: 网络中间的层. 每个隐藏层的神经元接受来自上一层的加权求和输入, 并加上一个偏置项$b_m$, 然后通过激活函数$f$进行线性变换, 公式为$z_m=\sum_{i=1}^n w_{im}x_i+b_m$
- 输出层: 位于网络最顶层, 输出结果$o_k$是隐藏层经过类似的加权和计算后的输出, 公式为$z_k=\sum_{i=1}^m w_{wk}o_i+b_k$

除此之外, FNN还有两个非常重要的特征:

- 每一个神经元只接受前一个层的输出
- 当前层的每一个神经元都与前一个层的所有神经元相连
- 每个神经元的输入在完成加权和计算后, 会通过一个激活函数, 这个激活函数不局限于阶跃函数, 可以是ReLu, Sigmoid, Tanh函数, 最常用的是Sigmoid函数. 特别注意, 这个激活函数要可微

### 反向传播算法

对于每一个训练样本$\{\bm{x}, t\}$, $\bm{x}=\{x_1, x_2, ..., x_n\}$, $t$为其标签. 将其传入网络, 直到输出层, 这个过程称为正向传播, 将其输出$o$与标签$t$进行比较, 计算误差, 根据误差, 从输出层到输入层逐级反向传播, 调整每个神经元的权重, 以减小误差, 这个过程就是反向传播. 权重的更新公式为$w_{pq}^{new}=w_{pq}^{old}+\Delta w_{pq}$.

那么, 我们怎么计算这个权重变化$\Delta w_{pq}$的呢? 可以定义一个误差损失函数然后使用梯度下降算法解决.

[^1]: 神经元模型和感知机 | Machine Learning. (2018, August 12). https://zhjunqin.gitbook.io/machine-learning/ji-qi-xue-xi/shen-jing-wang-luo/shen-jing-yuan-mo-xing