---
title: 前馈神经网络
comments: true
---

## 神经网络

### 神经元

神经网络由神经元(单元, 节点)组成, 这些神经元通过有向链接互相连接, 每个连接都有一个相关的数值权重. 神经元会组成层状结构, 包括输入层, 输出层或多个隐藏层. 在训练的过程中, 权重会被调整, 以学习执行一个特定的任务. 

在生物神经网络中, 每个神经元与其他神经元相连, 当它兴奋的时候, 就会向相连的神经元发送化学物质, 从而改变这些神经元内的电位, 如果某神经元的电位超过一个阈值, 那么它就会被激活, 即兴奋起来, 向其他神经元发送化学物质. 机器学习中的神经元也类似, 神经元接受到其他$n$个神经元传递过来的输入信号, 这些输入信号通过带权重的连接进行传递, 神经元接收到总输入将与神经元的阈值进行比较, 然后通过"激活函数"处理以产生神经元的输出.

1943年, McMulloch和Pittes将上述情景抽象, 这就是一直沿用至今的"M-P"神经元模型:

<figure markdown='1'>
![](https://img.ricolxwz.io/9e2e01f533f1d459d48b80f753580c0c.png){ loading=lazy width='500' }
</figure> 

每一个神经元包含:

- 输入向量$\bm{x}$, 表示输入数据, 其中每个元素$x_1, x_2, ..., x_n$, 对应$n$个特征
- 权重向量$\bm{w}$: 每个输入都有一个对应的权重$w_1, w_2, ..., w_n$, 对应$n$个参数
- 偏置$b$: 是一个常数, 在将输入加权求和后添加, 用于平移激活曲线
- 求和$\sum$: 输入$x_1, x_2, ..., x_n$和权重$w_1, w_2, ..., w_n$相乘相加, 加上偏置值, 结果表达式为$\bm{w}\bm{x}+b$
- 传递函数$f$: 又叫激活函数, 是将求和结果进行映射并生成输出, 其中$a=f(\bm{w}\bm{x}+b)$ 
- 输出$a$: 对加权和及偏置应用激活函数后产生的最终结果

<figure markdown='1'>
![](https://img.ricolxwz.io/f2b529bb036ac241e7d09acd263e4c90.png){ loading=lazy width='500' }
</figure>

理想的激活函数应该是阶跃函数(如上左图所示), 它将输入值映射为输出值为$0$或$1$, $1$对应于神经元兴奋, $0$对应于神经元抑制. 但是阶跃函数具有不连续, 不光滑等不太好的特性, 因此实际常用sigmoid函数作为激活函数(如上右图所示).

### 感知机

感知机, Perceptron, 它是最简单的神经网络, 由两层神经元组成, 输入层接受外界输入的信号后传递给输出层, 输出层是M-P神经元, 也被称为"阈值逻辑单元", 激活函数为阶跃函数.

<figure markdown='1'>
![](https://img.ricolxwz.io/c4090b80e90cb85ba787e93a035cb384.png){ loading=lazy width='300' }
</figure>

#### 学习算法

那么, 感知机是如何进行学习的呢?

之前我们说过, 神经元会在训练过程中调整权重$w_1, w_2$, 这个其实就是感知机学习的过程. 权重的更新公式为$\bm{w}^{new}=\bm{w}^{old}+e\bm{x}^T$, $e=t-a$, $t$为目标输出($0$或$1$), $a$为实际输出($0$或$1$), $\bm{x}$为输入向量; 同时还要调整截距, $b^{new}=b^{old}+e$

- 当$e=1$的时候, $\bm{w}^{new}=\bm{w}^{old}+\bm{x}^T$, $b^{new}=b^{old}+1$
- 当$e=-1$的时候, $\bm{w}^{new}=\bm{w}^{old}-\bm{x}^T$, $b^{new}=b^{old}-1$
- 当$e=0$的时候, $\bm{w}^{new}=\bm{w}^{old}$, $b^{new}=b^{old}$

具体算法如下:

1. 初始化权重和截距$\bm{w}$, $b$为小的随机数, 设置当前迭代次数为$1$
2. 对于每一个训练样本$\{\bm{x}, t\}$
    1. 计算$a$, 这一步又叫作网络激活
    2. 计算误差$e=t-a$
    3. 更新权重和截距: $\bm{w}^{new}=\bm{w}^{old}+e\bm{x}^T$, $b^{new}=b^{old}+e$
3. 在每次迭代结束时(即循环一遍所有的训练样本)检查是否满足停止条件, 如果所有的样本都被正确分类, 或者训练次数达到最大迭代次数, 则停止训练, 否则继续迭代, 执行第2步

???+ example "例子"

    给出下列的训练样本:

    |序号|特征(输入)|标签(输出)|
    |-|-|-|
    |1|1 0 0|0|
    |2|1 0 1|1|
    |3|1 1 0|0|


    初始权重为$\bm{w}=[0.3\ 0.2\ 0.4]$, 初始截距为$b=0.1$. 最大迭代次数为$5$.

    === "第一个样本 ---->"

        - $a=sign([0.3\ 0.2\ 0.4][1\ 0\ 0]+0.1)=sign(0.4)=1$, 错误, $e=0-1=-1$
        - $\bm{w}^{new}=[0.3\ 0.2\ 0.4] + (-1)[1\ 0\ 0]=[-0.7\ 0.2\ 0.4]$
        - $b^{new}=0.1+(-1)=-0.9$

    === "第二个样本 ---->"

        - $a=sign([-0.7\ 0.2\ 0.4][1\ 0\ 1]-0.9)=sign(-1.2)=0$, 错误, $e=1-0=1$
        - $\bm{w}^{new}=[-0.7\ 0.2\ 0.4] + (1)[1\ 0\ 1]=[0.3\ 0.2\ 1.4]$
        - $b^{new}=-0.9+1=0.1$

    === "第三个样本"

        - $a=sign([0.3\ 0.2\ 1.4][1\ 1\ 1]+0.1)=sign(0.6)=1$, 错误, $e=0-1=-1$
        - $\bm{w}^{new}=[0.3\ 0.2\ 1.4] + (-1)[1\ 1\ 1]=[-0.7\ -0.8\ 1.4]$
        - $b^{new}=0.1-1=-0.9$

    第一轮迭代结束后的权重$\bm{w}=[-0.7\ -0.8\ -1.4]$, 截距$b=-0.9$, 检查:

    1. 所有的样本都被正确分类? 
        1. 第一个样本: $a=sign([-0.7\ -0.8\ 1.4][1\ 0\ 0]-0.9)=sign(-1.6)=0$, 正确✅
        2. 第二个样本: $a=sign([-0.7\ -0.8\ 1.4][1\ 0\ 1]-0.9)=sign(-0.2)=0$, 错误❎
        3. 第三个样本: 无需检查第三个样本, 因为第二个样本已经错误
    2. 达到最大迭代次数? 🈚️

    进入第二轮迭代...


[^1]: 神经元模型和感知机 | Machine Learning. (2018, August 12). https://zhjunqin.gitbook.io/machine-learning/ji-qi-xue-xi/shen-jing-wang-luo/shen-jing-yuan-mo-xing

