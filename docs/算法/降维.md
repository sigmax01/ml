---
title: 降维
comments: true
---

???+ info "信息"

    - 已省略例子边框

## 动机

有一些机器学习算法的样本可能包含成百上千个特征. 这就会导致训练时候的维度很高, 进而导致训练速度变慢, 不可靠的分类(样本在高维空间的分布非常稀疏), 过拟合, 不易于解释, 无法可视化(人类只能翻译低维数据, 最高$3$维). 并且不是所有的特征对于结果都是重要的, 需要的是找到一小个必须的, 充分的特征子集以进行分类. 降维就是一个除去多余的, 重复的特征并减少噪音的过程.

## PCA

Principle Component Analysis (PCA), 主成分分析, 是一种最人们的降维方法, 又被称为特征投影方法. 主要思想是找到一个当前维度的子维度, 然后将数据投影到新的维度上. 新的维度对应的训练集(投影)可以被用作机器学习算法的输入用于训练模型.  

现在, 我们来看一个例子, 请解答, 下列数据沿着哪根轴的离散程度最大, $X$还是$Y$?

<figure markdown='1'>
![](https://img.ricolxwz.io/d7068ef901f3546bee95f37508f59d83.png){ loading=lazy width='500' }
</figure>

答案是$X$轴. 

我们再来看另一个例子, 请解答, 下列数据沿着哪根轴的离散程度最大, $X$, $Y$还是其他轴?

<figure markdown='1'>
![](https://img.ricolxwz.io/e9f5fe7ebe5c3a9a81e7754e3ac799ee.png){ loading=lazy width='500' }
</figure>

答案是$Z_1$轴, 如图所示.

<figure markdown='1'>
![](https://img.ricolxwz.io/858f3ee26803470379d34852975ff769.png){ loading=lazy width='500' }
</figure>

注意看上图, 我们有两个多出来的轴, 一个是$Z_1$, 另一个是$Z_2$, $Z_1$方向的离散程度是最大的, $Z_2$方向的离散程度是最小的. 事实上, $Z_1$和$Z_2$是$X$和$Y$的线性组合: $Z_1=1\times X+1\times Y$, $Z_2=-1\times X+1\times Y$. 事实上, 我们可以想象, 将这些数据投影到$Z_1$轴上, 投影后的值为在$Z_1$轴上的坐标, 投影后的值的方差(关于什么是方差, 请见[这里](/算法/线性回归/#偏差和方差))表示的就是数据的离散程度, 而数据分布越离散, 每个数据点的概率取值就比较小, 此时该分布的熵就更大, 包含更多信息. [^1]

现在, 有$N$个样本, 每个样本有$m$个特征(维度), 目标是找到$k$个新的坐标轴$Z_1, Z_2, ..., Z_k(k<m)$, 这些坐标轴之间相互正交(不相关)且数据在轴上离散程度的排序为$Var(Z_1)>Var(Z_2)>...>Var(Z_k)$, 我们称$Z_1, ..., Z_m$是主成分. 主成分是在数据中找到的新的坐标轴, 这些坐标轴按照数据在上面离散程度(方差)的大小进行排序, 每个主成分都是原始特征的线性组合. 第一个主成分是使得数据方差最大的方向, 第二个主成分是与第一个主成分正交的条件下, 方差最大的方向, 依此类推... 由于我们选出的是$k$个轴, 所以原始数据从$m$维降为了$k$维. 

对于上面的例子, 原始维度为$2$维, 坐标轴为$X$和$Y$, $Z_1$是数据方差最大的方向, 我们通过线性组合$X$和$Y$, 将数据降为$1$维. 如图, 红色的点就是将原始绿点映射到第一主成分$Z_1$后的数据, 我们只需要用$Z_1$的坐标描述红点就行了.

<figure markdown='1'>
![](https://img.ricolxwz.io/369b36dea39de9c2e6a317ede6bdc7e7.png){ loading=lazy width='500' }
</figure>

由于所有的主成分都是正交的, 所以彼此之间是不相关的, 因此这些主成分之间的方差满足$Var(X+Y)=Var(X)+Var(Y)$, 即样本的总方差等于所有主成分的方差之和.

那么, 我们怎么选择需要多少的主成分呢? 是一个? 两个? 还是$m-1$个? 主要的方法有两种:

1. 设置一个最小方差百分比, 比如$95\%$, 然后选择前$k$个主成分$Z_1, Z_2, ..., Z_k$, 使得它们能够解释总方差的$95\%$. 如你有一个包含$100$个特征的数据集, 通过PCA, 你计算出每个主成分能够解释的方差百分比, 结果如下: 第一个主成分解释了$40\%$的方差, 第二个主成分解释了$30\%$的方差, 第三个主成分解释了$15\%$的方差, 第四个主成分解释了$10\%$的方差, 其余的$96$个主成分解释了$5\%$的方差, 如果你想保留总方差的$95\%$, 你可以选择前四个主成分, 来替代原来的$100$个主成分
2. 肘部法, Elbow Method. 绘制主成分的数量和累积方差图, 通常会在曲线上出现一个"肘点", 表示方差的增长速度开始减缓的地方, 这通常是一个合理的选择, 因为在此之后增加更多的维度带来的方差增益有限, 如下图, 总方差的$95\%$对应的是$153$维, 而肘点对应的是$100$维

    <figure markdown='1'>
    ![](https://img.ricolxwz.io/d27b500548b5b83cdbcb86d3f6ebcda6.png){ loading=lazy width='500' }
    </figure>

[^1]: 郑之杰. (2011, April 20). 主成分分析(Principal Component Analysis, PCA). 郑之杰的个人网站. https://0809zheng.github.io/2020/04/11/PCA.html