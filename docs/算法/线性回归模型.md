---
title: 线性回归模型
comments: true
---

???+ info "信息"

    - 前置知识
    - 默认省略例子边框

线性回归模型属于监督学习, 同时是一个回归问题.

## 单变量线性回归模型

单变量线性回归模型(1), 指的是只有一个输入/特征的回归模型. 
{.annotate}

1. univariate linear regression

### 模型表示

#### 问题

例子: 预测住房价格.

数据集: 已知一个数据集, 包括某个城市的住房价格. 每个样本包括住房尺寸和售价.

要求: 根据不同住房尺寸所出售的价格, 画出数据集.

问题: 对于一个给定的住房尺寸, 预测它的售价.

这个数据集可以用坐标表示:

![](https://img.ricolxwz.de/2024/06/20240625102405.png){:style="width:400px"}

也可以用表格表示:

| 房屋大小 ($x$) | 价格 ($y$) |
| :---:         |     :---:      |
| 2104 | 460 |
| 1416 | 232 |
| 1534 | 315 |
| 852  | 178 |
| ...  | ... |
| 3210 | 870 |

#### 术语

用如下符号来描述这个问题:

- $m$: 代表数据集中样本数量
- $x$: 代表输入/特征
- $y$: 代表输出/目标, 数据集中的实际真实值
- $\hat{y}$: $y$的估计或预测
- ($x, y$): 代表数据集中的一个样本
- ($x^{(i)}, y^{(i)}$): 代表第$i$个样本
- $h$: 代表学习算法的模型或函数也称为假设(hypothesis)

#### 流程

``` mermaid
graph LR
  A[数据集] --> B[学习算法];
  B --> D;
  C[特征] --> D[h];
  D --> E[目标的预测]
```

1. 把数据集(特征和目标)输入到学习算法
2. 学习算法计算出函数$h$. 函数的输入是特征$x$, 输出是目标的预测$\hat{y}$

#### 模型

对于房价预测问题, 模型/函数$h$可能的表述如下:

$h_{\theta}(x)=\theta_0+\theta_1 x$

将$\theta_0$和$\theta_1$称为模型/函数的参数. 在机器学习中, 模型的参数是可以在训练期间调整以改进模型的变量. 对于线性回归, 要做的就是选择参数的值以便更好的拟合数据, 选择的参数决定了$h$相对与数据集的准确程度.

定义建模误差(1)为模型所预测的值$\hat{y}$和实际值$y$之间的差距(蓝线表示):
{.annotate}

1. modeling error

![](https://img.ricolxwz.de/2024/06/6168b654649a0537c67df6f2454dc9ba.png){:style="width:400px"}

### 代价函数

为了衡量模型$h$的性能, 常见的方法是定义代价函数(1). 最常用的是均方误差函数(2):
{.annotate}

1. cost function
2. mean squared error

$J(\theta_1, \theta_1)=\frac{1}{2m}\sum^m_{i=1}(h_{\theta}(x^{(i)})-y^{(i)})^2=\frac{1}{2m}\sum^m_{i=1}(\hat{y}^{(i)}-y^{(i)})^2$

选取参数$\theta_1$和$\theta_2$以最小化代价函数$J$, 从而优化模型$h$.

???+ tip "Tip"

    除以$2$是为了让后面的计算看起来更加整洁, 无论是否除以$2$, 代价函数都有效.
    
可以绘制这个函数, 三个坐标分别为$\theta_0$, $\theta_1$, $J(\theta_0, \theta_1)$, 可以看到在三维空间中存在一个使得代价函数$J(\theta_0, \theta_1)$最小的点:

![](https://img.ricolxwz.de/2024/06/202406260834626.png){:style="width:400px"}

将其呈现为等高线图:

![](https://img.ricolxwz.de/2024/06/202406260839779.png){:style="width:680px"}

根据上图, 人工的方法很容易找到代价函数最小时对应的$\theta_0, \theta_1$, 但是我们真正需要的是一种有效的算法, 能够自动找出这些使代价函数达到最小值的参数, 也就是下面要讲到的[梯度下降](#梯度下降).

### 梯度下降 {#梯度下降}

梯度下降是一种求函数最小值的算法. 

梯度下降的思想是开始时随机选择一个参数组合$\theta_0, \theta_1, ..., \theta_n$, 计算代价函数. 然后寻找下一个能让代价函数值下降最多的参数组合. 持续这么做会找到一个局部最小值, 因为我们没有尝试完所有的参数组合, 所以无法确定得到的局部最小值是否是全局最小值, 选择不同的初始参数组合, 可能会得到不同的局部最小值.

以输入为两个参数的代价函数为例, 不同的起始点导致不同的局部最小值:

![](https://img.ricolxwz.de/2024/06/202406260850659.png){:style="width:400px"}

???+ tip "Tip"

	为了理解梯度下降, 可以想象站在山的一点上, 并且希望用最短的时间下山. 在梯度下降算法中, 要做的就是旋转360度, 看看周围, 要在某个方向上用小碎步尽快下山, 这些小碎步需要朝向什么方向? 如果我们站在山坡上的这一点, 看一下周围, 你会发现最佳的下山方向, 按照自己的判断迈出一步. 重复上述的步骤, 从新的位置, 环顾四周, 并决定从什么方向将会最快下山, 然后又迈进了一小步, 并依次类推, 直到你接近局部最低点的位置.
	
批量梯度下降(1)算法可以抽象为公式:
{.annotate}

1. batch gradient descent

$\theta_j:=\theta_j-\alpha\frac{\partial}{\partial \theta_j}J(\theta_0, \theta_1)$, $j=0, 1, ..., n$, $n$为参数的数量

其中$\alpha$是学习率(1), 它决定了沿着能让代价函数下降程度最大的方向向下迈出的步子有多大; 在批量梯度下降中, 每一次同时让所有的参数减去学习速率乘以代价函数的偏导数. 

???+ warning "注意"

	更新的时候需要同时更新所有的参数, 而不是逐个更新. 这种更新的方式确保了在计算下一个参数的梯度的时候, 其他参数的值保持不变. 在同步更新中, 所有的偏导数都是基于同一组参数值$\theta_0, \theta_1, ..., \theta_n$计算的.
	
	![](https://img.ricolxwz.de/2024/06/202406260922502.png){:style="width:400px"}
	
[^1]: Machine-learning-deep-learning-notes/machine-learning/linear-regression.md at master · loveunk/machine-learning-deep-learning-notes. (n.d.). Retrieved June 26, 2024, from https://github.com/loveunk/machine-learning-deep-learning-notes/blob/master/machine-learning/linear-regression.md