---
title: 集成学习
comments: true
---

传统的机器学习算法(如决策树, 人工神经网络, 支持向量机, 朴素贝叶斯等)的目标都是寻找一个最优分类器尽可能将训练数据分开. 集成学习, Ensemble Learning算法的基本思想就是将多个分类器组合, 从而实现一个预测效果更好的集成分类器. 集成算法可以描述为: "三个臭皮匠, 赛过诸葛亮".

集成算法大致上可以分为: Bagging, Boosting和Stacking等类型.

## Bagging

Bagging(Bootstrap Aggregating)由Breimam于1996年提出, 基本思想如下:

1. 每次采用有放回的抽样从训练集中取出$n$个样本组成新的训练集
2. 利用新的训练集, 训练得到$M$个子模型$\{h_1, h_2, ..., h_M\}$
3. 对于分类问题, 采用投票的方法, 得票最多的分类为最终的类别; 对于回归问题, 采用简单的取平均方法得到预测值

### 随机森林

随机森林(Random Forests)是一种利用决策树作为基学习器的Bagging集成学习算法. 随机森林的构建过程如下:

1. 数据采样

    作为一种Bagging集成算法, 随机森林同样采取有放回的采样, 对于总体训练集$T$, 抽样一个子集$T_{sub}$作为训练样本集. 除此之外, 假设总体训练集的特征个数为$d$, 每次仅选择$k(k<d)$个作为训练样本集的特征. 因此, 随机森林除了能够做到抗样本扰动外, 还添加了特征扰动, 对于特征的选择个数, 推荐值为$k=\log_2 d$.

2. 树的构建

    每次根据采样得到的数据和特征构建一颗决策树. 在构建决策树的过程中, 会让决策树生长完全不进行预剪枝. 构建出的若干棵决策树组成了最终的随机森林. 预测的方法同Bagging.

## Boosting

Boosting是一种提升算法, 可以将弱的学习算法提升为强的学习算法. 基本思想如下:

1. 初始训练: 首先, 从初始训练数据集训练一个基学习器(如决策树, 线性模型等). 这个基学习器可能并不完美, 但是它会对大多数样本做出合理的预测
2. 调整样本权重: 对于那些被机器学习错误分类的样本, 提高他们的权重. 这意味着在下一轮训练的时候, 这些错误分类的样本将获得更大的关注, 使得后续的基学习器能够更好地处理这些困难样本
3. 迭代训练: 在每一轮中, 利用调整后的样本权重训练新的基学习器. 这个过程会重复多次, 每次都会生成一个新的学习器, 并且每个新的学习器都试图修正前一个学习器的错误
4. 组合结果: 最终, 将所有的基学习器的结果进行组合. 对于分类问题, 采用有权重的投票方式来决定最终的分类结果; 对于回归问题, 采用甲醛平均的方式得到最终的预测值. 权重的分配依据每个基学习器的准确性, 表现好的学习器会获得更大的权重

[^1]: 范叶亮. (n.d.). 集成学习算法 (Ensemble Learning) - 范叶亮 | Leo Van. 范叶亮的个人网站 | Leo Van’s Personal Website. Retrieved August 26, 2024, from https://leovan.me/cn/2018/12/ensemble-learning/