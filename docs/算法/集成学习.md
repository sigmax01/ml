---
title: 集成学习
comments: true
---

???+ info "信息"

    - 这里的"基分类器"/"分类器"指的就是参与集成学习中使用的单个分类器. 事实上, "分类器"这个术语并不准确, 因为有一些问题不是分类问题, 而是回归问题, 我们可以使用更加通用的术语"基学习器"或者"基模型"来代替基分类器. 这里, 为了和网上广大的教程(包括悉大的课件)保持统一, 就称其为"基分类器".
    - 默认省略例子边框

传统的机器学习算法(如决策树, 人工神经网络, 支持向量机, 朴素贝叶斯等)的目标都是寻找一个最优分类器尽可能将训练数据分开. 集成学习, Ensemble Learning算法的基本思想就是将多个分类器组合, 从而实现一个预测效果更好的集成分类器. 集成算法可以描述为: "三个臭皮匠, 赛过诸葛亮".

集成算法大致上可以分为: Bagging, Boosting和Stacking等类型.

## 为何要集成学习

使用集成学习具有很多优势:

- 统计上的优势: 一个学习算法可以理解为在一个假设空间中找到一个最好的假设(假设空间是指所有可能的模型或者假设的集合). 但是, 当训练样本的数据量小到不够用来准确的学习到目标假设的时候, 学习算法可以找到很多满足训练样本的分类器. 所以, 学习算法选择任何一个分类器都会面临一定错误预测的风险, 因此将多个假设集成起来可以降低错误分类器的风险
- 计算上的优势: 很多学习算法在进行最优化搜索的时候很可能陷入局部最优的错误中, 因此对于学习算法而言很难得到一个全局最优的假设. 事实上人工神经网络和决策树已经被证明是一个NP问题. 集成算法可以从多个起始点进行局部搜索, 从而分散陷入局部最优的风险
- 表示上的优势: 在现实中, 假设空间中的任意一个单一模型(假设)通常难以完美的表示(或者近似的表示)数据的真实分类函数. 换句话说, 一个单一的模型不足以捕捉数据的复杂性, 无法做出准确的预测. 集成学习算法, 如Boosting, 通过组合多个不同的基分类器来形成一个更强的模型. 这些基分类器不是简单的平均组合, 而是通过加权的方式, 赋予每个基分类器不同的权重. 加权组合的结果可以看作是在原本假设空间基础上扩展了一个新的假设空间, 这个扩展后的假设空间更加灵活, 更优能力去逼近真正的分类函数

如我们通过组合25个二元基分类器得到了一个集成学习模型. 每一个二元基分类器的错误率$\epsilon=0.35$, 即准度为$0.65$.

1. 若所有的基分类器都是相同的, 即会犯同样的错误. 那么得到的集成学习模型在测试集上的错误率为$0.35$
2. 若所有的基分类器是独立的, 即他们犯的错误是没有关联的, 那么得到的集成学习模型是否错误取决于是否有超过半数的基分类器预测错误(假设采用投票的方式选出结果), 即有超过$13$个基分类器预测错误. 我们需要计算的是$13$个基分类器预测错误的概率+$14$次基分类器预测错误的概率+...+$25$次基分类器预测错误的概率, 即$\sum_{i=13}^{25}\binom{25}{i}\epsilon^i(1-\epsilon)^{25-i}=0.06$(符合二项分布). 可以看到出错的概率显著减小, $0.06<0.35$.

在上述例子中, 基分类器错误率和得到的集成学习模型错误率的关系如图所示:

<div style="text-align: center;">
    <img src="https://img.ricolxwz.io/2024/08/39c630377194dc1bd1310e8e5623aa6d.png" alt="description" style="width: 600px;">
</div>

从图中我们可以看到, 当基分类器错误率大于$0.5$的时候, 集成学习模型的错误率甚至会高于基分类器的错误率. 这种情况下基分类器的预测比在两个类别之间随机猜的结果还差.

所以, 我们可以得出结论, 如果要使集成学习模型表现比单个基分类器更好:

- 基分类器的错误率要低, 对于二元分类器来说, 至少至少你要比随机乱猜要好
- 基分类器之间必须相互独立, 在实践中, 我们无法做到每个基分类器的所有训练样本都是不同的, 即我们无法做到基分类器之间的完全独立. 事实上, 经验表明, 当基分类器之间有轻微关联的时候, 往往得到的集成学习模型效果会更好

## Bagging

Bagging(Bootstrap Aggregating)由Breimam于1996年提出, 基本思想如下:

1. 每次采用有放回的抽样从训练集中取出$n$个样本组成新的训练集
2. 利用新的训练集, 训练得到$M$个子模型$\{h_1, h_2, ..., h_M\}$
3. 对于分类问题, 采用投票的方法, 得票最多的分类为最终的类别; 对于回归问题, 采用简单的取平均方法得到预测值

### 随机森林

随机森林(Random Forests)是一种利用决策树作为基分类器的Bagging集成学习算法. 随机森林的构建过程如下:

1. 数据采样

    作为一种Bagging集成算法, 随机森林同样采取有放回的采样, 对于总体训练集$T$, 抽样一个子集$T_{sub}$作为训练样本集. 除此之外, 假设总体训练集的特征个数为$d$, 每次仅选择$k(k<d)$个作为训练样本集的特征. 因此, 随机森林除了能够做到抗样本扰动外, 还添加了特征扰动, 对于特征的选择个数, 推荐值为$k=\log_2 d$.

2. 树的构建

    每次根据采样得到的数据和特征构建一颗决策树. 在构建决策树的过程中, 会让决策树生长完全不进行预剪枝. 构建出的若干棵决策树组成了最终的随机森林. 预测的方法同Bagging.

## Boosting

Boosting是一种提升算法, 可以将弱的学习算法提升为强的学习算法. 基本思想如下:

1. 初始训练: 首先, 从初始训练数据集训练一个基分类器(如决策树, 线性模型等). 这个基分类器可能并不完美, 但是它会对大多数样本做出合理的预测
2. 调整样本权重: 对于那些被机器学习错误预测的样本, 提高他们的权重. 这意味着在下一轮训练的时候, 这些错误分类的样本将获得更大的关注, 使得后续的基分类器能够更好地处理这些困难样本
3. 迭代训练: 在每一轮中, 利用调整后的样本权重训练新的基分类器. 这个过程会重复多次, 每次都会生成一个新的分类器, 并且每个新的分类器都试图修正前一个分类器的错误
4. 组合结果: 最终, 将所有的基分类器的结果进行组合. 对于分类问题, 采用有权重的投票方式来决定最终的分类结果; 对于回归问题, 采用甲醛平均的方式得到最终的预测值. 权重的分配依据每个基分类器的准确性, 表现好的分类器会获得更大的权重

[^1]: 范叶亮. (n.d.). 集成学习算法 (Ensemble Learning) - 范叶亮 | Leo Van. 范叶亮的个人网站 | Leo Van’s Personal Website. Retrieved August 26, 2024, from https://leovan.me/cn/2018/12/ensemble-learning/