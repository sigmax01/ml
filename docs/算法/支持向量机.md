---
title: 支持向量机
comments: true
---

???+ info "信息"

    本文已省略例子边框

## 边际最大超平面

超平面, Hyperplane是数学和机器学习中的一个概念. 它是一个$n$维空间中将空间划分为两个部分的几何对象. 具体来说, 在$n$维空间中, 超平面是一个$n-1$维子空间. 例如, 在二维空间中, 超平面是一条一维的直线, 直线将平面分成两个部分; 在三维空间中, 超平面是一个二维的平面, 平面将空间分为两个部分, 依此类推...

在左图中, 找到一个线性的决策边界(直线, 超平面)使得数据能够分开, 这道题的答案有很多, 如右图.

<figure markdown='1'>
![](https://img.ricolxwz.io/85fd16c878d35bf1364e8d4aa3f3bac3.png){ loading=lazy width="285" align=left }
![](https://img.ricolxwz.io/b919e277383a5cd0a2a4a59e089067a6.png){ loading=lazy width="300" align=right }
</figure>

支持向量, support vector是离决策边界最近的样本(数据点). 边际, margin是指从决策边界到最靠近它的样本点(支持向量)的距离. 在SVM中, 我们通常会有两个边际, 一个位于决策边界的一侧, 另一个位于其另一侧. 决策边界位于两个边际的中间, 它是这两个边际的中心线[^1]. 如左图. 当然, 也可能同时出现多个支持向量, 因为它们都是最靠近决策边界的样本点, 如右图.

<figure markdown='1'>
![](https://img.ricolxwz.io/dea073ea85626a6d9ce2a67708cf5f4e.png){ loading=lazy width="293" align=left }
![](https://img.ricolxwz.io/1a457e5998165b974bd77c9ad5778938.png){ loading=lazy width="293" align=right }
</figure>

支持向量之所以被称为向量是因为给的样本就是一个向量, 试想, 样本有很多的特征, 这些特征构成了向量... 支持向量是所有样本点(向量)的子集.

下面来看, 哪一个超平面(决策边界)更优呢? B1还是B2? 哪个超平面能够更准确地分类新数据?

<figure markdown='1'>
![](https://img.ricolxwz.io/6209988905535c0d38422ae82d63dbed.png){ loading=lazy width="300" }
</figure>

答案是B1, 因为它的边际更大. 拥有最大边际的超平面我们称之为边际最大超平面. 按照这个超平面(决策边界)分类的数据准度会更高. 如果边际过小, 意味着决策边界和支持向量非常接近, 在这种情况下, 即使是很小的变化都可能导致分类结果发生显著变化, 这意味着模型对数据扰动很敏感, 容易出现过拟合现象; 如果边际较大, 意味着模型对于数据的微小变化更具有鲁棒性, 有较好的泛化性能. 这种选择大边际的策略在统计理论中也得到了支持, 称为"结构风险最小化原理".

## 线性支持向量机

现在我们有一个二分类问题, 一共有$N$个训练样本(输入向量). 我们定义$x$为输入向量, $y$为分类值, 即$x_i=(x_{i1}, x_{i2}, ..., x_{im})^T, y_i=\{-1, 1\}$. 参考[线性分类](/算法/线性回归/#线性分类). 我们有一个符号函数$sign$, 如果$w\cdot x+b$的结果$>0$, 说明样本点在决策边界的上方; 如果$w\cdot x+b$的结果$<0$, 说明样本点在决策边界的下方. $y=w\cdot x+b$就是决策边界. 

假设输入向量$x$是二维的. 我们可以将决策边界$H$的超平面方程式定义为$w_1x_1+w_2x_2+b=0$(标量形式). 将超平面方程式上下移动$c$个单位, 分别是$w_1x_1+w_2x_2+b=c$和$w_1x_1+w_2x_2+b=-c$, 来到对应的间隔上下边界$H_1, H_2$, 由于上下边界一定会经过一些样本数据点, 而这些点距离决策边界最近, 他们决定了间隔距离, 我们称之为支持向量. 我们可以把等式两边分别除以$c$, 得到$\frac{w_1}{c}x_1+\frac{w_2}{c}x_2+\frac{b}{c}=1$, $\frac{w_1}{c}x_1+\frac{w_2}{c}x_2+\frac{b}{c}=-1$和$\frac{w_1}{c}x_1+\frac{w_2}{c}x_2+\frac{b}{c}=0$, 使用$w'_1=\frac{w_1}{c}, w'_2=\frac{w_2}{c}$和$b'=\frac{b}{c}$替换, 可以得到$w'_1x_1+w'_2x_2+b'=1$, $w'_1x_1+w'_2x_2+b'=-1$和$w'_1x_1+w'_2x_2+b'=0$, 由于$w'_1, w'_2$和$b'$只是我们需要求解的代号, 所以将其换为$w_1, w_2$和$b$也不影响计算, 所以最终只需要求解$w_1, w_2$和$b$就可以了, 得到下面三个超平面方程: $w_1x_1+w_2x_2+b=1$, $w_1x_1+w_2x_2+b=-1$和$w_1x_1+w_2x_2+b=0$, 这三个方程式分别称为正超平面方程式(对应超平面$H_1$), 负超平面方程式(对应超平面$H_2$)和决策超平面方程式(对应超平面$H$). [^2]

<div style="position: relative; padding: 30% 45%;">
<iframe style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;" src="//player.bilibili.com/player.html?isOutside=true&aid=936042727&bvid=BV16T4y1y7qj&cid=494397114&p=1&high_quality=1&autoplay=false&muted=false&t=185&as_wide=1" frameborder="yes" scrolling="no" allowfullscreen="true"></iframe>
</div>

回到超平面方程式的向量形式, 对于任意一个样本点, 它到超平面$w\cdot x+b=0$的垂直距离可以通过公式$\frac{|w\cdot x+b|}{||w||}$计算. 因此, 支持向量之间的垂直距离可以计算为$d=\frac{|(w\cdot x_1+b)-(w\cdot x_2+b)|}{||w||}$, 由于$w\cdot x_1+b=1$, $w\cdot x_2+b=-1$, 我们可以将其代入上述公式, 得到$d=\frac{2}{||w||}$.

<figure markdown='1'>
![](https://img.ricolxwz.io/71e7706ae085c0fcaf7714a519c50a24.png){ loading=lazy width="400" }
</figure>

我们的目标就是要最大化这个$d$, 也就是最小化$||w||$, 等同于最小化函数$\frac{1}{2}||w||^2.$ 所以线性支持向量机将$\frac{1}{2}||w||^2$作为代价函数, 前提条件是所有的样本必须被正确分类, 即对于任意的$y_i$, 有$y_i(w\cdot x_i+b)\geq 1$.

[^1]: Saini, A. (2021, October 12). Guide on Support Vector Machine (SVM) Algorithm. Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/10/support-vector-machinessvm-a-complete-guide-for-beginners/
[^2]: FunInCode. (n.d.). 【数之道】支持向量机SVM是什么，八分钟直觉理解其本质_哔哩哔哩_bilibili. Retrieved September 2, 2024, from https://www.bilibili.com/video/BV16T4y1y7qj/