---
title: 支持向量机
comments: true
---

???+ info "信息"

    - 本文已省略例子边框
    - 使用粗体表示向量

## 边际最大超平面

超平面, Hyperplane是数学和机器学习中的一个概念. 它是一个$n$维空间中将空间划分为两个部分的几何对象. 具体来说, 在$n$维空间中, 超平面是一个$n-1$维子空间. 例如, 在二维空间中, 超平面是一条一维的直线, 直线将平面分成两个部分; 在三维空间中, 超平面是一个二维的平面, 平面将空间分为两个部分, 依此类推...

在左图中, 找到一个线性的决策边界(直线, 超平面)使得数据能够分开, 这道题的答案有很多, 如右图.

<figure markdown='1'>
![](https://img.ricolxwz.io/85fd16c878d35bf1364e8d4aa3f3bac3.png){ loading=lazy width="285" align=left }
![](https://img.ricolxwz.io/b919e277383a5cd0a2a4a59e089067a6.png){ loading=lazy width="300" align=right }
</figure>

支持向量, support vector是离决策边界最近的样本(数据点). 边际, margin是指正负超平面(什么是正负超平面下面有讲到)之间的距离距离, 如左图. 当然, 也可能同时出现多个支持向量, 因为它们都是最靠近决策边界的样本点, 如右图.

<figure markdown='1'>
![](https://img.ricolxwz.io/dea073ea85626a6d9ce2a67708cf5f4e.png){ loading=lazy width="293" align=left }
![](https://img.ricolxwz.io/1a457e5998165b974bd77c9ad5778938.png){ loading=lazy width="293" align=right }
</figure>

支持向量之所以被称为向量是因为给的样本就是一个向量, 试想, 样本有很多的特征, 这些特征构成了向量... 支持向量是所有样本点(向量)的子集.

下面来看, 哪一个超平面(决策边界)更优呢? B1还是B2? 哪个超平面能够更准确地分类新数据?

<figure markdown='1'>
![](https://img.ricolxwz.io/6209988905535c0d38422ae82d63dbed.png){ loading=lazy width="300" }
</figure>

答案是B1, 因为它的边际更大. 拥有最大边际的超平面我们称之为边际最大超平面. 按照这个超平面(决策边界)分类的数据准度会更高. 如果边际过小, 意味着决策边界和支持向量非常接近, 在这种情况下, 即使是很小的变化都可能导致分类结果发生显著变化, 这意味着模型对数据扰动很敏感, 容易出现过拟合现象; 如果边际较大, 意味着模型对于数据的微小变化更具有鲁棒性, 有较好的泛化性能. 这种选择大边际的策略在统计理论中也得到了支持, 称为"结构风险最小化原理".

## 线性支持向量机

现在我们有一个二分类问题, 一共有$N$个训练样本(输入向量). 我们定义$\boldsymbol{x}$为输入向量, $y$为分类值, 即$\boldsymbol{x_i}=(x_{i1}, x_{i2}, ..., x_{im})^T, y_i=\{-1, 1\}$. 参考[线性分类](/算法/线性回归/#线性分类). 我们有一个符号函数$sign$, 如果$\boldsymbol{w}\cdot \boldsymbol{x}+b$的结果$>0$, 说明样本点在决策边界的上方; 如果$\boldsymbol{w}\cdot \boldsymbol{x}+b$的结果$<0$, 说明样本点在决策边界的下方. $y=\boldsymbol{w}\cdot \boldsymbol{x} + b$就是决策边界. 

假设输入向量$\boldsymbol{x}$是二维的. 我们可以将决策边界$H$的超平面方程式定义为$w_1x_1+w_2x_2+b=0$(标量形式). 将超平面方程式上下移动$c$个单位, 分别是$w_1x_1+w_2x_2+b=c$和$w_1x_1+w_2x_2+b=-c$, 来到对应的间隔上下边界$H_1, H_2$, 由于上下边界一定会经过一些样本数据点, 而这些点距离决策边界最近, 他们决定了间隔距离, 我们称之为支持向量. 我们可以把等式两边分别除以$c$, 得到$\frac{w_1}{c}x_1+\frac{w_2}{c}x_2+\frac{b}{c}=1$, $\frac{w_1}{c}x_1+\frac{w_2}{c}x_2+\frac{b}{c}=-1$和$\frac{w_1}{c}x_1+\frac{w_2}{c}x_2+\frac{b}{c}=0$, 使用$w'_1=\frac{w_1}{c}, w'_2=\frac{w_2}{c}$和$b'=\frac{b}{c}$替换, 可以得到$w'_1x_1+w'_2x_2+b'=1$, $w'_1x_1+w'_2x_2+b'=-1$和$w'_1x_1+w'_2x_2+b'=0$, 由于$w'_1, w'_2$和$b'$只是我们需要求解的代号, 所以将其换为$w_1, w_2$和$b$也不影响计算, 所以最终只需要求解$w_1, w_2$和$b$就可以了, 得到下面三个超平面方程: $w_1x_1+w_2x_2+b=1$, $w_1x_1+w_2x_2+b=-1$和$w_1x_1+w_2x_2+b=0$, 这三个方程式分别称为正超平面方程式(对应正超平面$H_1$), 负超平面方程式(对应负超平面$H_2$)和决策超平面方程式(对应决策超平面$H$). [^1]

<div style="position: relative; padding: 30% 45%;">
<iframe style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;" src="//player.bilibili.com/player.html?isOutside=true&aid=936042727&bvid=BV16T4y1y7qj&cid=494397114&p=1&high_quality=1&autoplay=false&muted=false&t=185&as_wide=1" frameborder="yes" scrolling="no" allowfullscreen="true"></iframe>
</div>

回到超平面方程式的向量形式, 对于任意一个样本点, 它到超平面$\boldsymbol{w}\cdot \boldsymbol{x} + b=0$的垂直距离可以通过公式$\frac{|\boldsymbol{w}\cdot \boldsymbol{x} + b|}{||\boldsymbol{w}||}$计算. 因此, 支持向量之间的垂直距离可以计算为$d=\frac{|(\boldsymbol{w}\cdot \boldsymbol{x_1}+b)-(\boldsymbol{w}\cdot \boldsymbol{x_2}+b)|}{||\boldsymbol{w}||}$, 由于$\boldsymbol{w}\cdot \boldsymbol{x_1}+b=1$, $\boldsymbol{w}\cdot \boldsymbol{x_2}+b=-1$, 我们可以将其代入上述公式, 得到$d=\frac{2}{||\boldsymbol{w}||}$.

<figure markdown='1'>
![](https://img.ricolxwz.io/71e7706ae085c0fcaf7714a519c50a24.png){ loading=lazy width="400" }
</figure>

我们的目标就是要最大化这个$d$, 也就是最小化$||\boldsymbol{w}||$, 等同于最小化函数$\frac{1}{2}||\boldsymbol{w}||^2.$ 所以线性支持向量机将$\frac{1}{2}||\boldsymbol{w}||^2$作为代价函数, 前提条件是所有的样本必须被正确分类, 即对于任意的$y_i$, 有$y_i(\boldsymbol{w}\cdot \boldsymbol{x_i}+b)\geq 1$. 这是一个凸二次优化问题.

我们可以使用拉格朗日乘数法将约束条件结合到目标函数中, 构造如下: $L(\boldsymbol{w}, b, \boldsymbol{\lambda})=\frac{1}{2}||\boldsymbol{w}||^2-\sum_{i=1}^N \lambda_i[y_i(\boldsymbol{w}\cdot \boldsymbol{x_i}+b)-1]$, 其中$\lambda_i$为拉格朗日乘子, $\boldsymbol{\lambda}=(\lambda_1; \lambda_2; ...; \lambda_N), \lambda_i\geq 0$. 我们要求该函数的最小值, 令$L(\boldsymbol{w}, b, \boldsymbol{\lambda})$对$\boldsymbol{w}$和$b$求导为$0$可得: $\boldsymbol{w}=\sum_{i=1}^{N}\lambda_iy_i\boldsymbol{x_i}$, $0=\sum_{i=1}^N\lambda_iy_i$.

[^1]: FunInCode. (n.d.). 【数之道】支持向量机SVM是什么，八分钟直觉理解其本质_哔哩哔哩_bilibili. Retrieved September 2, 2024, from https://www.bilibili.com/video/BV16T4y1y7qj/
[^2]: 周志华. (n.d.). 机器学习.