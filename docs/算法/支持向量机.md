---
title: 支持向量机
comments: true
---

???+ info "信息"

    本文已省略例子边框

## 边际最大超平面

超平面, Hyperplane是数学和机器学习中的一个概念. 它是一个$n$维空间中将空间划分为两个部分的几何对象. 具体来说, 在$n$维空间中, 超平面是一个$n-1$维子空间. 例如, 在二维空间中, 超平面是一条一维的直线, 直线将平面分成两个部分; 在三维空间中, 超平面是一个二维的平面, 平面将空间分为两个部分, 依此类推...

在左图中, 找到一个线性的决策边界(直线, 超平面)使得数据能够分开, 这道题的答案有很多, 如右图.

<figure markdown='1'>
![](https://img.ricolxwz.io/85fd16c878d35bf1364e8d4aa3f3bac3.png){ loading=lazy width="285" align=left }
![](https://img.ricolxwz.io/b919e277383a5cd0a2a4a59e089067a6.png){ loading=lazy width="300" align=right }
</figure>

支持向量, support vector是离决策边界最近的样本(数据点). 边际, margin是指从决策边界到最靠近它的样本点(支持向量)的距离. 在SVM中, 我们通常会有两个边际, 一个位于决策边界的一侧, 另一个位于其另一侧. 决策边界位于两个边际的中间, 它是这两个边际的中心线[^1]. 如左图. 当然, 也可能同时出现多个支持向量, 因为它们都是最靠近决策边界的样本点, 如右图.

<figure markdown='1'>
![](https://img.ricolxwz.io/dea073ea85626a6d9ce2a67708cf5f4e.png){ loading=lazy width="293" align=left }
![](https://img.ricolxwz.io/1a457e5998165b974bd77c9ad5778938.png){ loading=lazy width="293" align=right }
</figure>

支持向量之所以被称为向量是因为给的样本就是一个向量, 试想, 样本有很多的特征, 这些特征构成了向量... 支持向量是所有样本点(向量)的子集.

下面来看, 哪一个超平面(决策边界)更优呢? B1还是B2? 哪个超平面能够更准确地分类新数据?

<figure markdown='1'>
![](https://img.ricolxwz.io/6209988905535c0d38422ae82d63dbed.png){ loading=lazy width="300" }
</figure>

答案是B1, 因为它的边际更大. 拥有最大边际的超平面我们称之为边际最大超平面. 按照这个超平面(决策边界)分类的数据准度会更高. 如果边际过小, 意味着决策边界和支持向量非常接近, 在这种情况下, 即使是很小的变化都可能导致分类结果发生显著变化, 这意味着模型对数据扰动很敏感, 容易出现过拟合现象; 如果边际较大, 意味着模型对于数据的微小变化更具有鲁棒性, 有较好的泛化性能. 这种选择大边际的策略在统计理论中也得到了支持, 称为"结构风险最小化原理".

[^1]: Saini, A. (2021, October 12). Guide on Support Vector Machine (SVM) Algorithm. Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/10/support-vector-machinessvm-a-complete-guide-for-beginners/