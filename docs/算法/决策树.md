---
title: 决策树
comments: true
---

## 定义

决策树, decision tree, 是一个树形结构(可以是二叉树或者非二叉树). 其每个非叶节点表示一个特征属性上的测试, 每个分支代表这个特征属性在某个值域上的输出, 而每个叶节点存放一个类别.

使用决策树进行决策的过程就是从根节点开始, 测试待分类项目中相应的特征属性, 并按照其值选择输出分支, 直到到达叶子节点, 将叶子节点存放的类别作为决策结果.

## 信息增益

详见[不确定性和熵](https://gk.ricolxwz.de/信息论/不确定性和熵).

信息熵代表的是随机变量的不确定性程度, 条件熵代表某一条件下, 不确定性减少的程度. 在决策树算法中, 我们的关键就是每次选择一个特征, 特征有多个, 那么到底按照什么标准来选择哪一个特征. 这个问题就可以用信息增益来度量. 如果选择一个特征之后, 信息增益最大, 那么我们就选择这个特征.

???+ example "例子"

    | 帅？ | 性格好？ | 身高？ | 上进？ | 嫁与否 |
    |-----|--------|-------|-------|-------|
    | 帅   | 不好    | 矮     | 不上进  | 不嫁   |
    | 不帅  | 好     | 矮     | 上进    | 不嫁   |
    | 帅   | 好     | 矮     | 上进    | 嫁    |
    | 不帅  | 爆好    | 高     | 上进    | 嫁    |
    | 帅   | 不好    | 矮     | 上进    | 不嫁   |
    | 帅   | 不好    | 矮     | 上进    | 不嫁   |
    | 帅   | 好     | 高     | 不上进  | 嫁    |
    | 不帅  | 好     | 中     | 上进    | 嫁    |
    | 帅   | 爆好    | 中     | 上进    | 嫁    |
    | 不帅  | 不好    | 高     | 上进    | 嫁    |
    | 帅   | 好     | 矮     | 不上进  | 不嫁   |
    | 帅   | 好     | 矮     | 不上进  | 不嫁   |

    可以求得随机变量$X$(嫁与不嫁)的信息熵为: $-\frac{1}{2}\log \frac{1}{2}-\frac{1}{2}\log \frac{1}{2}\simeq 0.301$. 假如现在我已经知道了一个男生的身高信息, 求直到这个信息之后的信息增益. 

    身高的可能取值有三个, 即随机变量$Y$的取值可能有三个: 矮, 中, 高. 

    - 矮的个数为7个, 对应的嫁的数量是1个, 不嫁的数量是6个
    - 中的个数是2个, 对应的嫁的数量是2个, 不嫁的数量是0个
    - 高的个数是3个, 对应的嫁的数量是3个, 不嫁的数量是0个 

    条件熵的计算公式为: $H(Y|X)=\sum_{x\in X}p(x)H(Y|X=x)$. 

    - $H(Y|X=矮)=-\frac{1}{7}\log\frac{1}{7}-\frac{6}{7}\log\frac{6}{7}\simeq 0.178$, $p(X=矮)=\frac{7}{12}$
    - $H(Y|X=中)=-1\log1-0=0$, $p(X=中)=\frac{2}{12}$
    - $H(Y|X=高)=-1\log1-0=0$, $p(X=低)=\frac{3}{12}$

    由此可以计算出条件熵为$\frac{7}{12}*0.178+\frac{2}{12}*0+\frac{3}{12}*0=0.103$.

    所以信息增益就是$0.301-0.103=0.198$. 也就是说, 本来如果我对一个男生什么都不知道的话, 作为他的女朋友决定是否嫁给他的不确定性有$0.301$这么大. 当我们直到男朋友的身高信息之后, 不确定性减少了$0.198$, 也就是说, 身高这个特征对于我们广大女生同学来说, 决定加不加给自己的男朋友是很重要的.