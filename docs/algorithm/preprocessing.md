---
title: 预处理
comments: true
---

## 数据

数据由一系列记录/实例/对象组成. 记录由属性/特征描述. 

### 名义/数值属性

- 名义/分类属性: 这些属性的值包含在一个预定义, 有限种可能的集合中
- 数值/连续属性: 这些属性的值是数字

### 数据分类

- 矩阵数据/表格数据
- 交易数据
- 顺序数据, 如基因序列
- 图
- 时空数据
- ...

## 数据清洗

数据不是完美的, 通常含有噪音. 这些噪音可以是值的失真(distortion), 虚假的值(spurious examples), 不一致或者重复的值. 此外, 还要考虑缺失的值.

### 噪音

噪音是由于人为或者数据处理工具的限制造成的.

总的来说, 不一致/重复的值比较好处理, 失真/虚假的值比较难处理. 对于后者, 通常采取的方法是:

- 在DM(数据挖掘)前使用信号和图像处理以及异常检测技术
- 使用更加适应噪音的机器学习算法, 在噪音存在的情况下给出可以接受的结果

#### 失真的值

???+ example "例子"

    如在差劲的通话环境下, 人的声音的信号是这样的: 

    ![](https://img.ricolxwz.io/00f2ff264dad186c92c52efb4c11ad57.png){:style="width:600px"}

#### 虚假的值

有一些值可能远离正常值, 有一些值可能和其他非噪音值混杂在了一起.

???+ example "例子"

    ![](https://img.ricolxwz.io/f1a3a65d8d33b23f41a23b7a5cb790fd.png){:style="width:600px"}

#### 不一致/重复的值

???+ example "例子"

    如负数的体重/身高. 不存在的邮编代码, 同一个人有2条记录... 

### 值缺失

对于值缺失, 通常采取的方法是:

- 忽略所有含有缺失值的记录: 如果这样的记录占比不是很大的话
- 通过存在的值预估缺失的值, 可以将缺失的属性A的值替换成: 
    - 最经常出现的属性A的值
    - 同一类下的最经常出现的属性A的值
    - 最邻近的邻居的属性A的均值

## 数据预处理

数据预处理包括:

- 数据聚合
- 降维
- 特征提取
- 特征子集选择
- 特征加权
- 属性类型转换
- 归一化/标准化

### 数据聚合

数据聚合指的是将两个或者多个属性聚合成一个属性的过程. 目的是: 

- 缩减数据, 占用更少的内存和计算事件; 为计算更加复杂的机器学习算法预留空间; 
- 尺度伸缩, 提供高层次的视图, 如城市聚合成州或者国家
- 稳定数据, 聚合数据比非聚合数据的变异性更小, 例如, 将每日消费的事务聚合成每周食物. 这里的变异性是指在不同观测和测量中的变化程度或差异. 它反映了数据的分散程度, 即数据点偏离平均值的程度. 高变异性表示数据点之间存在较大的差异, 而低变异性则表示数据点较集中, 差异较小

缺点是可能会丧失有趣的细节.

### 特征提取

特征提取指的是从原始数据中提取特征的过程, 这个过程非常重要. 它需要领域专家的知识. 例如, 将图像分类为户外或者室内, 原始数据是每个像素的颜色值, 提取的特征可以是颜色直方图, 主色调, 边缘直方图等. 可能需要将数据映射到一个新的空间, 然后提取特征, 新的空间可能揭示重要特征.

???+ example "例子"

    如下图, 左边的图显示的是横坐标为时间的信号, 该信号由两个正弦波和一些噪音组成. 右边的图是经过傅里叶变换之后的频域信号, 频域信号展示了信号在不同频率上的幅度分布. 在这个图中, 有两个明显的峰值, 这些峰值对应于原始信号中的两个主要正弦波的频率. 

    ![](https://img.ricolxwz.io/f77c1d154335251b77981a08481e95bd.png){:style="width:600px"}

### 特征子集选择

选择特征子集是移除不相关的多余的特征并且选择一小部分相关的充分的特征的过程. 对于实现好的分类至关重要, 更好的选择特征子集可以提高准确性. 使用更少的特征意味着更快的构建, 可以减少计算消耗, 更加紧凑且更容易解释的分类规则.

#### 选择方法

在机器学习中, 特征子集选择的研究算是比较成熟的, 有许多优秀的方法. 

- 暴力法: 尝试所有可能的特征的组合将其输入到机器学习算法中, 选择结果最好的那一组
- 嵌入法: 有一些机器学习算法能够自动选择特征, 如决策树
- 筛选法: 在运行机器学习算法之前选择特征, 机器学习算法和特征选取是相互独立的. 这类方法通常基于统计学方法, 如信息增益, 互信息, 赔率比等; 或者基于相关性, 如Relief算法
- 黑箱法: 为给定的机器学习算法选择最佳子集, 使用机器学习算法作为黑箱来评估不同子集并选择最优子集

### 特征加权

特征加权用于根据特征的重要性分配不同的权重. 这些权重反映了每个特征在模型中的相对重要性, 更重要的特征会被分配更高的权重, 不重要的特征会被分配更低的权重, 分配的方法要么是根据专业领域的知识手动分配的; 要么是根据一些分类算法(如boosting)或者选择启用特征加权选项(如k邻近算法)自动调整权重. 

### 属性类型转化

某些机器学习算法仅适用于数值, 名义或二进制属性, 需要进行一定的类型转化.

- 将数值属性转换为名义属性(离散化)
- 将数值和名义属性转换为二进制属性(二进制化)

#### 二进制化 {#bit-transform}

二进制化用于将数值属性转换为二进制, 没有最佳的方法, 最佳的方法就是最适合特定机器学习算法的方法, 但是无法评估所有的可能性. 最简单的方式如下: 

- 分类属性 -> 整数 -> 二进制数
- 数值属性 -> 分类 -> 整数 -> 二进制数

???+ example "例子"

    ![](https://img.ricolxwz.io/0c5968fc679a51290752b85f1f0fad37.png){:style="width:400px"}

#### 离散化

离散化用于将数值属性转换为名义属性. 离散化有两种类型:

- 无监督: 无法使用类别信息
- 有监督: 可以使用类别信息

需要考虑的是: 有多少的类别/区间, 在哪里分. 

##### 无监督离散化

![](https://img.ricolxwz.io/b1722f03cbe284678c874ce9efd607fb.png){:style="width:600px"}

考虑的问题有:

- 有多少类别/区间: 用户自定义, 如, 我定义要分4个区间
- 在哪里分: 一共有三种方法
    - 等宽: 区间的宽度相等
    - 等频率: 所有区间内点的数量都相等
    - K均值聚类: 区间分割点由K均值聚类算法决定

### 归一化/标准化 {#normalization}

- 归一化指的是将数据按照比例缩放到特定范围的过程. 适用于数据分布不确定或者存在显著范围差异时
- 标准化指的是将数据转换为均值为0, 标准差为1的分布. 适用于假设数据符合正态分布或者需要保留数据分布特征时

???+ example "例子"

    如年龄(用年表示)和年收入(用刀表示)有不同的尺度: 

    - A组数据: [20, 40000]
    - B组数据: [40, 60000]

    A组数据和B组数据的差异为|20-40|+|40000-60000|=20020. 可以看到, 收入占到了绝对主导地位, 无法体现年龄对差异的影响. 解决的方法就是先进行归一化/标准化, 然后再进行计算.

所有的属性都要进行归一化/标准化.

- 归一化: $x'=\frac{x-min(x)}{max(x)-min(x)}$
- 标准化: $x'=\frac{x-\mu(x)}{\sigma(x)}$

## 相似性测量 {#相似性测量}

许多机器学习算法需要测量两个记录之间的相似性. 假设有两条记录$A$和$B$, 有属性$a_1, a_2, ..., a_n$和属性$b_1, b_2, ..., b_n$, 以下是测量方法. 

### 欧几里得距离 {#euclidean-distance}

欧几里得距离是最经常用的测量方式. 

欧几里得距离$D(A, B)=\sqrt{(a_1-b_1)^2+(a_2-b_2)^2+...+(a_n-b_n)^2}$

### 曼哈顿距离 {#曼哈顿距离}

曼哈顿距离$D(A, B)=|a_1-b_1|+|a_2-b_2|+...|a_n-b_n|$.

### 闵可夫斯基距离

闵可夫斯基距离$D(A, B)=(|a_1-b_1|^q+|a_2-b_2|^q+...+|a_n-b_n|^q)^{1/q}$.

### 加权距离

为每一个属性根据它们的重要性分配一个权重(需要专业领域的知识).

加权距离$D(A, B)=\sqrt{w_1|a_1-b_1|^2+w_2|a_2-b_2|^2+...+w_n|a_n-b_n|^2}$.

### 汉明距离

汉明距离用于测量二进制向量之间的距离.

汉明距离$D(A, B)=|a_1-b_1|+|a_2-b_2|+...+|a_n-b_n|$.

### 相似系数 {#similarity-score}

相似系数用于测量二进制向量之间的相似性. 

- f00: 匹配的0-0位数. 这表示在两个比较的二进制序列中, 两个序列在相同位置上都是0的次数
- f01: 匹配的0-1位数. 这表示在两个比较的二进制序列中, 第一个序列为0而第二个序列为1的次数
- f10: 匹配的1-0位数. 这表示在两个比较的二进制序列中, 第一个序列为1而第二个序列为0的次数
- f11: 匹配的1-1位数. 这表示在两个比较的二进制序列中, 两个序列在相同位置上都是1的次数

???+ example "例子"

    $A=[1 0 0 0 0 0 0 0 0 0], B=[0 0 0 0 0 0 1 0 0 1]$, 则$f01=2, f10=1, f00=7, f11=0$.

### 简单匹配系数

简单匹配系数Simple Matching Coefficient(SMC)的定义为$SMC=\frac{f11+f00}{f01+f10+f11+f00}$.

???+ tip "Tip"

    SMC在设计上存在一定的缺陷. 若f00或f11特别大, 远远大于另一个, 则会导致其中一个值占主导地位. 

### 雅卡尔指数

雅卡尔指数, Jaccard Coefficient. 能够解决SMC的缺陷. 它的定义为$J=\frac{f11}{f01+f10+f11}$.

### 余弦相似度 {#cosine-similarity}

余弦相似度, Consine Similarity. 是一种用于衡量两个向量在多维空间中的相似程度的方法. 余弦相似度主要用于稀疏数据(无论是二进制还是非二进制数据), 尤其常用于文本文档的分类. 

余弦相似度的定义为$cos(A, B)=\frac{A\cdot B}{||A||||B||}$.

???+ example "例子"

    两份文件的向量为:

    - $d_1 = 3 2 0 5 0 0 0 2 0 0$
    - $d_2 = 1 0 0 0 0 0 0 1 0 2$

    经过计算得到: 

    - $d_1\cdot d_2 = 5$
    - $||d_1||=6.481$
    - $||d_2||=2.245$

    得到: $cos(d_1, d_2)=0.3150$

### 皮尔逊相关系数 {#pearson-correlation-coefficient}

皮尔逊相关系数, Pearson Correlation Coefficient, 用来衡量两个数值之间的线性关系.

皮尔逊相关系数的定义为$corr(x, y)=\frac{covar(x, y)}{std(x)std(y)}$. 公式中涉及的各个部分: 

- 均值: $mean(x)=\frac{\sum_{k=1}^n x_k}{n}$
- 标准差: $std(x)=\sqrt{\frac{\sum_{k=1}^n(x_k-mean(x))^2}{n-1}}$
- 协方差: $covar(x, y)=\frac{1}{n-1}\sum_{k=1}^n(x_k-mean(x))(y_k-mean(y))$

皮尔逊相关系数的范围为$[-1, 1]$, $-1$表示完全负相关, $1$表示完全正相关, $0$表示没有相关性.

![](https://img.ricolxwz.io/3e4007d639b983d41c0050bfdcfef2ce.png){:style="width:600px"}

## 降维

请见[降维](/algorithm/dimensional-reduction).

## 分箱

请见[分箱](https://gk.ricolxwz.de/information-theory/estimator/#分箱).