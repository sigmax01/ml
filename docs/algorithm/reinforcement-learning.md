---
title: 强化学习
comments: true
---

## 背景

监督学习旨在通过训练集中的已知标签$y$来训练模型, 使模型的输出尽可能模仿这些标签, 通常用于分类或回归任务. 例如, 将不同类型的宝可梦进行分类, 在这种情况下, 输入有一个正确答案或者标签, 模型试图预测每个输入的标签.

<figure markdown='1'>
![](https://img.ricolxwz.io/4ac5abb0cf015b7b4090727b14619b9f.png){ loading=lazy width='350' }
</figure>

但是, 在一些复杂任务中, 如让机器人学会骑自行车中, 定义"正确答案"非常困难, 很难提供明确的监督信号去指导算法模仿, 因此监督学习不适合这类问题. 

<figure markdown='1'>
![](https://img.ricolxwz.io/59327b41a383dd54769205f503770e69.png){ loading=lazy width='250' }
</figure>

## 定义

强化学习可以用来解决上述问题, 可以通过婴儿学习的类比来解释: 婴儿在成长过程中没有明确的老是指导, 而是通过与环境的互动积累经验. 例如, 婴儿通过玩耍, 挥动手臂或四处观察等活动, 接受到来自环境的正面(如称赞)和负面(如批评)反馈, 这些反馈塑造了他们的行为和性格, 这种试错和反馈的过程类似于强化学习中的"反馈机制".

<figure markdown='1'>
![](https://img.ricolxwz.io/3c49aa991e47c617c6c802da039debb3.png){ loading=lazy width='500' }
</figure>

强化学习的目标是学习如何做出一系列良好的决策. 智能体, Agent根据当前的状态$s_t$采取行动$a_t$, 并根据从环境中获得的奖励来调整其策略, 通过试错来最大化累计奖励. 这种奖励可以用奖励函数$r_t$来描述, 它是一个标量, 反馈智能体在$t$时刻的表现好坏. 

<figure markdown='1'>
![](https://img.ricolxwz.io/572319de8ee175bda36bc5024ccf4a7a.png){ loading=lazy width='250' }
</figure>

???+ tip "Tip"

	"奖励假设"告诉我们"所有目标都可以通过最大化预期的累计奖励来描述", 这意味着, 只要能够设计一个合适的奖励函数, 我们就可以通过强化学习实现各种复杂的目标, 涵盖从游戏到机器人控制的不用应用场景.

在每一个时间步, 智能体接受状态$s_t$和奖励$r_t$, 执行行动$a_t$. 环境接受行动$a_t$, 更新状态为$s_{t+1}$, 并发出新奖励$r_{t+1}$. 历史$H_t=s_1, r_1, a_1, ..., a_{t-1}, s_t, r_t$记录了智能体从开始到当前时间步$t$为止所有的状态, 行动和奖励, 接下来的行动$a_t$取决于这个历史.

## 马尔可夫决策过程

马尔可夫决策过程, Markov Decision Process, MDP, 是用于定义强化学习问题的数学框架, MDP通常通过五元组$(\mathcal{S}, \mathcal{A}, \mathcal{R}, \mathbb{P}, \gamma)$来表示:

- $\mathcal{S}$: 表示所有可能的状态集合, 即智能体在环境中可能遇到的不同状态
- $\mathcal{A}$: 表示所有可能的行动集合, 即智能体可以采取的各种行动
- $\mathcal{R}$: 表示在特定的状态和行动下, 奖励值的分布
- $\mathbb{P}$: 表示给定当前的状态和所选行动后, 转移到下一个状态的概率分布
- $\gamma$: 折扣因子, 用于确定未来奖励的价值, 较低的$\gamma$会让智能体更关注当前的奖励, 较高的$\gamma$会让智能体考虑长期的回报

在马尔可夫决策过程中, 假设只需要关注当前状态$s_t$就可以决定下一步, 而不依赖于更早的历史.

智能体由三个部分组成:

1. 策略: Policy, 定义了每个状态下智能体采取的行动. 通常用$\pi(a|s)$表示. 策略可以是确定性的(每个状态对应唯一的行动), 或者是随机性的(每个状态对应行动的概率分布)
2. 价值函数: Value Function, 衡量智能体在给定状态或(和)采取行动后的预期回报. 它能帮助智能体判断哪些状态或(和)行动更有利, 常见的价值函数有状态值函数$V^{\pi}(s)$和状态-行动值函数$Q^{\pi}(s, a)$, $\pi$是某个特定的策略, 侧重于评估
3. 模型: Model, 用于模拟环境的动态行为. 即预测在某个状态下采取特定行动后, 环境如何变化. 通常提供以下信息: a. 从当前状态$s$采取行动$a$后, 可能会转移到哪个下一个状态$s'$; b. 在状态$s$执行动作$a$后, 可能获得的即时奖励$r$. 侧重于模拟

	???+ tip "Tip"

		有些RL算法会使用模型来模拟和预见环境的变化, 称为基于模型的算法, 而某些RL算法不需要模型, 成为无模型算法. 如果是基于模型的算法, 直接计算价值函数需要智能体在真实环境中进行多次试错, 以获取足够的数据来估计每个状态或(和)采取行动后的预期汇报, 这对于一些复杂或者代价高昂的环境来说可能不切实际. 模型可以用来模拟环境, 这样, 智能体可以通过在模型中进行"虚拟"实验来估计价值函数, 而不是每次都与真实环境互动, 使得智能体能够"离线学习", 这种方式既节省了资源, 也加快了学习过程.