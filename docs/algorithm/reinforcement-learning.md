---
title: 强化学习
comments: true
---

## 背景

监督学习旨在通过训练集中的已知标签$y$来训练模型, 使模型的输出尽可能模仿这些标签, 通常用于分类或回归任务. 例如, 将不同类型的宝可梦进行分类, 在这种情况下, 输入有一个正确答案或者标签, 模型试图预测每个输入的标签.

<figure markdown='1'>
![](https://img.ricolxwz.io/4ac5abb0cf015b7b4090727b14619b9f.png){ loading=lazy width='350' }
</figure>

但是, 在一些复杂任务中, 如让机器人学会骑自行车中, 定义"正确答案"非常困难, 很难提供明确的监督信号去指导算法模仿, 因此监督学习不适合这类问题. 

<figure markdown='1'>
![](https://img.ricolxwz.io/59327b41a383dd54769205f503770e69.png){ loading=lazy width='250' }
</figure>

## 定义

强化学习可以用来解决上述问题, 可以通过婴儿学习的类比来解释: 婴儿在成长过程中没有明确的老是指导, 而是通过与环境的互动积累经验. 例如, 婴儿通过玩耍, 挥动手臂或四处观察等活动, 接受到来自环境的正面(如称赞)和负面(如批评)反馈, 这些反馈塑造了他们的行为和性格, 这种试错和反馈的过程类似于强化学习中的"反馈机制".

<figure markdown='1'>
![](https://img.ricolxwz.io/3c49aa991e47c617c6c802da039debb3.png){ loading=lazy width='500' }
</figure>

强化学习的目标是学习如何做出一系列良好的决策. 智能体, Agent根据当前的状态$s_t$采取行动$a_t$, 并根据从环境中获得的奖励来调整其策略, 通过试错来最大化累计奖励. 这种奖励可以用奖励函数$r_t$来描述, 它是一个标量, 反馈智能体在$t$时刻的表现好坏. 

<figure markdown='1'>
![](https://img.ricolxwz.io/572319de8ee175bda36bc5024ccf4a7a.png){ loading=lazy width='250' }
</figure>

???+ tip "Tip"

	"奖励假设"告诉我们"所有目标都可以通过最大化预期的累计奖励来描述", 这意味着, 只要能够设计一个合适的奖励函数, 我们就可以通过强化学习实现各种复杂的目标, 涵盖从游戏到机器人控制的不用应用场景.

在每一个时间步, 智能体接受状态$s_t$和奖励$r_t$, 执行行动$a_t$. 环境接受行动$a_t$, 更新状态为$s_{t+1}$, 并发出新奖励$r_{t+1}$. 历史$H_t=s_1, r_1, a_1, ..., a_{t-1}, s_t, r_t$记录了智能体从开始到当前时间步$t$为止所有的状态, 行动和奖励, 接下来的行动$a_t$取决于这个历史.

## 马尔可夫决策过程

马尔可夫决策过程, Markov Decision Process, MDP, 是用于定义强化学习问题的数学框架, MDP通常通过五元组$(\mathcal{S}, \mathcal{A}, \mathcal{R}, \mathbb{P}, \gamma)$来表示:

- $\mathcal{S}$: 表示所有可能的状态集合, 即智能体在环境中可能遇到的不同状态
- $\mathcal{A}$: 表示所有可能的行动集合, 即智能体可以采取的各种行动
- $\mathcal{R}$: 表示在特定的状态和行动下, 奖励值的分布
- $\mathbb{P}$: 表示给定当前的状态和所选行动后, 转移到下一个状态的概率分布
- $\gamma$: 折扣因子, 用于确定未来奖励的价值, 较低的$\gamma$会让智能体更关注当前的奖励, 较高的$\gamma$会让智能体考虑长期的回报

在马尔可夫决策过程中, 假设只需要关注当前状态$s_t$就可以决定下一步, 而不依赖于更早的历史.

智能体由三个部分组成:

1. [策略](#policy): Policy, 定义了每个状态下智能体采取的行动. 通常用$\pi(a|s)$表示. 策略可以是确定性的(每个状态对应唯一的行动), 或者是随机性的(每个状态对应行动的概率分布)
2. [价值函数](#value-function): Value Function, 衡量智能体在某个策略下给定状态或在给定状态下采取行动后的预期回报. 它能帮助智能体判断哪些状态或状态和行动的组合更有利, 常见的价值函数有状态值函数$V^{\pi}(s)$和动作值函数$Q^{\pi}(s, a)$, $\pi$是某个特定的策略, 侧重于评估
3. 模型: Model, 用于模拟环境的动态行为. 即预测在某个状态下采取特定行动后, 环境如何变化. 通常提供以下信息: a. 从当前状态$s$采取行动$a$后, 可能会转移到哪个下一个状态$s'$; b. 在状态$s$执行动作$a$后, 可能获得的即时奖励$r$. 侧重于模拟

	???+ tip "Tip"

		有些RL算法会使用模型来模拟和预见环境的变化, 称为基于模型的算法, 而某些RL算法不需要模型, 称为无模型算法. 如果是基于模型的算法, 直接计算价值函数需要智能体在真实环境中进行多次试错, 以获取足够的数据来估计每个状态或给定状态采取行动后的预期汇报, 这对于一些复杂或者代价高昂的环境来说可能不切实际. 模型可以用来模拟环境, 这样, 智能体可以通过在模型中进行"虚拟"实验来估计价值函数, 而不是每次都与真实环境互动, 使得智能体能够"离线学习", 这种方式既节省了资源, 也加快了学习过程.

???+ example "例子"

	假设有一个扫地机器人, 它完成扫地后要回到它的基地, 起始位置可以是任何房间.

	<figure markdown='1'>
	![](https://img.ricolxwz.io/d18b1e9303deaf5464c7261cd95d395a.png){ loading=lazy width='200' }
	</figure>

	如上图, 总共有$12$个房间, 机器人完成扫地后可以在任意房间, 意味着总共有$12$个状态, 它需要做的是在最少的步骤下回到灰色方块所指示的房间. 它的行为可以是向上, 向下, 向左, 向右. 每次移动都设置为一个负奖励, 表示每走一步就扣一分, 鼓励机器人在最少的步数内到达基地. 

### 策略和最佳策略 {#policy}

???+ example "例子"

	策略$\pi$是机器人在每个状态下选择的动作规则, 例如, 在每个房间, 机器人可以选择向上, 向下, 向左, 向右. 最优策略$\pi^*$是使得奖励总和最大化的策略. 由于初始状态和状态转移概率的不确定性, 如机器人初始可能在任意一个房间, 往各个房间(即下一个状态)走的概率也可能不一样.

	<figure markdown='1'>
	![](https://img.ricolxwz.io/7236bec10dca7d6d88ece46d3301adb6.png){ loading=lazy width='400' }
	</figure>

	在这种情况下, 最优策略需要在所有可能的路径中最大化期望奖励. 使用公式可以表示为$\pi^*=argmax_{\pi}\mathbb{E}[\sum_{t\geq 0}\gamma^t r_t|\pi]$. $\sum_{t\geq 0}$表示的是从时间步$t=0$开始一直到未来的所有时间步的奖励$r_t$之和, 每个奖励都乘以折扣因子$\gamma$, 用于表示对未来奖励的重视程度. 条件期望$\mathbb{E[...|\pi]}$表示在给定策略$\pi$下计算的期望值, 即在该策略引导下, 累积的折扣奖励的期望. 初始状态$s_0\sim p(s_0)$, 表示机器人初始状态符合一定的概率分布. 行为$a_t\sim \pi(\cdot |s_t)$, 表示在每个时间步$t$, 机器人的行为符合概率分布$\pi(\cdot|s_t)$, 策略$\pi$是一个关于状态的概率分布, 描述了在不同状态下选择不同动作的概率. 状态转移$s_{t+1}\sim p(\cdot | s_t, a_t)$, 表示机器人的下一个状态符合概率分布$p(\cdot|s_t, a_t)$, 这个转移概率分布描述了在当前状态和采取的动作下, 环境可能转移到的下一个状态的分布.

### 价值函数 {#value-function}

价值函数用于衡量智能体在**某个策略**下 ^^给定状态^^ 或 ^^在给定状态下采取行动后^^ 的预期回报. 根据划线部分的不同, 可以分为两种价值函数: 状态值函数和动作值函数.

- 状态值函数: 表示在某个策略$\pi$下, 特定状态$s$的好坏程度. 它定义为从状态$s$出发, 遵循策略$\pi$所能获得的期望累计奖励. 公式为$V^{\pi}(s)=\mathbb{E}[\sum_{t\geq 0}\gamma^t r_t|s_0=s, \pi]$
- 动作值函数: 表示在某个策略$\pi$下, 当在状态$s$时执行动作$a$的好坏程度. 它定义为首先在状态$s$执行动作$a$, 然后遵循策略$\pi$所能获得的期望累计奖励. 公式为$Q^{\pi}(s, a)=\mathbb{E}[\sum_{t\geq 0}\gamma^t r_t|s_0=s, a_0=a, \pi]$. 

#### Bellman方程 {#bellman-function}

定义$Q^*(s, a)$为最优Q值函数, 公式为$Q^*(s, a)=max_{\pi}\mathbb{E}[\sum_{t\geq 0}\gamma^t r_t|s_0=s, a_0=a, \pi]$, 注意和动作值函数做区分, 这里表示的是**在所有可能的策略**中, 而动作值函数表示的是**在特定的策略下**. 最优Q值函数满足以下Bellman方程: $Q^*(s, a) = \mathbb{E}_{s' \sim \mathcal{E}} \left[ r + \gamma \max_{a'} Q^*(s', a') \,|\, s, a \right]$.

这个公式的直观理解就是如果下一个状态-动作$s', a'$对应的最优Q值$Q^*(s', a')$已知, 那么在状态$s$执行动作$a$所能获得的最优回报$Q^*(s, a)$, 等于即时奖励$r$加上下一个状态-动作对$s', a'$的最优Q值函数的期望值. $s'\sim \mathcal{E}$指的是下一个状态$s'$符合状态转移概率分布$p(\cdot|s, a)$. 

最优策略$\pi^*$指的就是在每个状态下, 都选择能够使得$Q^*(s, a)$最大化的动作/行为. 

## Q学习算法

Q学习算法的目的就是获得所有状态-动作对的最优Q值, 步骤为:

1. 初始化: 对于所有的状态-动作对$s, a$, 初始化$Q(s, a)$, 通常为$0$, 构成的表称为Q表
2. 持续交互: 
	1. 开始一个新的回合: 随机设定一个初始状态
		1. 根据当前状态$s$和策略(如$\epsilon$-贪心策略)选择一个动作$a$
		2. 执行动作$a$, 获得即时奖励$r$和下一个状态$s'$
		3. 我们并不知道每个状态-动作对的最优Q值$Q^*(s, a)$, 所以需要使用增量更新的方式逐步逼近最优Q值, $Q(s, a)$: $Q(s, a)\leftarrow Q(s, a)+\alpha[r+\gamma max_{a'} Q(s', a')-Q(s, a)]$. 这个公式的推导: 在执行动作$a$后, 根据Bellman方程, 我们可以算出一个目标Q值, 表示接下来采取最优动作时可以获得地期望回报, 目标Q值为$R(s, a, s')+\gamma max_{a'}Q(s', a')$. 那么它和当前Q值之间的差距就是增量$\Delta Q(s, a)=$目标Q值$-Q(s, a)=(R(s, a, s') + \gamma max_{a'}Q(s', a'))-Q(s, a)$. 然后, 设置一个学习率$\alpha$用来控制差距调整的大小$\alpha[r+\gamma max_{a'} Q(s', a')-Q(s, a)]$, Q.E.D
		4. 将$s'$作为新的当前状态, 如果$s'$是终止状态, 回合结束; 如果不是终止状态, 则继续
	2. 回合结束? 进行下一个回合
3. 终止: 若达到固定回合数, 或者Q表中所有的Q值变化都很小了, 小于某个阈值近似收敛, 则终止算法, Q表中的值作为学习到的最优Q值.

???+ example "例子"

	继续上面的例子. 假设向上走是$0$, 向下走$1$, 向左走$2$, 向右走$3$.

	1. 初始化: 首先, 初始化一个Q表, 这张表是$12\times 4$, 因为有$12$个状态, $4$种动作. 初始值全部都是$0$
	2. 持续交互
		1. 开始一个新的回合, 随机设定初始房间为$8$
			1. 选择动作: 根据$\epsilon$-贪心策略, 机器人在每个房间选择一个动作. 例如在在房间$8$选择向上走
			2. 执行动作和接受奖励: 执行动作, 获取即时奖励, 并转移到下一个状态$s'$. 例如向上走到了房间$6$, 由于不是目标房间, 即时奖励为$-1$
			3. 更新Q值: 用公式更新Q表中$Q(8, 0)$的值, 我们可以在当前的Q表中找到$max_{a'}Q(6, *)$(初始为$0$, 后续会慢慢更新), 例如$a'$是向下走, $max_{a'}Q(s', a')=Q(6, 1)$, 如果新的Q值变大了, 说明这个房间$8$向上这个方向比较好
			4. 将房间$6$设置为新的状态, 重复上述过程
		2. 回合结束? 如果它走到了房间$1$或房间$12$, 回合结束, 进行下一个回合
	3. 终止: 若达到固定回合数, 或者Q表中所有的Q值变化都很小了, 小于某个阈值近似收敛, 则终止算法, Q表中的值作为学习到的最优Q值

### 深度Q学习算法

在传统的Q学习中, Q值是保存在Q表中的, 这个方法在小规模的, 离散的状态空间中效果良好, 但是在复杂环境中主要会面临以下问题:

- 状态空间过大, Q表会变得极其庞大, 导致存储和计算的资源需求变得不现实
- 无法处理连续状态, 对于很多现实问题, 例如自动驾驶或者机器人控制, 状态(例如位置, 速度, 角度)都是连续的, 无法用有限的Q表表示
- 难以进行泛化, 传统Q学习在Q表中仅能记录每个具体状态-动作对的Q值, 无法在类似状态之间进行泛化. 这意味着智能体在一个状态下学到的信息无法被类似的状态直接利用

为了解决上述问题, 深度Q学习(DQN)引入了神经网络来近似最优Q值函数, 其结果$Q_w(s, a)\simeq Q^*(s, a)$, $w$为权重. 即给定$s, a$, 神经网络输出的Q值接近真实的最优Q值.

这种网络被称为"Q网络", 如图.

<figure markdown='1'>
![](https://img.ricolxwz.io/674d26b8e92821c8839d8bbbe9a298ba.png){ loading=lazy width='300' }
</figure>

左图是❌错误的, 这个神经网络接受一个状态和一个动作, 然后输出该特定状态-动作对的Q值$Q_w(s, a)$. 这个方法的问题在于, 每次只能输出一个特定动作的Q值, 导致效率低下. 特别是在动作空间很大的情况下, 这会导致大量重复计算.

右图是✅正确的, 即只将状态$s$输入网络, 然后让网络输出该状态下所有动作的Q值$Q_w(s, a_1), Q_w(s, a_2), ..., Q_w(s, a_m)$. 这种设计更加高效, 因为对于每个状态$s$, 网络只需要一次前向传播, 就能计算出所有可能动作的Q值. 在DQN算法中, 通常会将状态$s$输入到神经网络, 网络的最后一层输出一个大小为动作数$m$的向量, 其中每个元素对应一个一个状态-动作对的Q值.

???+ warning "注意"

	DQN能够处理连续的状态值, 但是不能处理连续的动作值. 因为DQN的输出层是每个离散动作输出一个Q值. 如果需要处理连续动作空间, 通常会使用其他算法, 如DDPG, SAC, TD3.

#### 损失函数

为了训练Q网络, 我们需要定义一个损失函数$L$来最小化Q值的预测误差. 它是预测Q值和目标值之间的均方误差. 根据Bellman方程, 目标Q值为$r+\gamma max_{a'}Q_w(s', a')$, 所以$L = \left( r + \gamma \max_{a'} Q_w(s', a') - Q_w(s, a) \right)^2$, 通过最小化这个损失函数, 神经网络会逐步调整权重, 使得$Q_w(s, a)$尽可能逼近最优Q值$Q^*(s, a)$.

#### 经验回放

在连续的时间步中, 状态和动作是相关的, 因此直接使用这些样本进行训练会导致模型学习到时间相关性, 从而影响模型的稳定性.

经验回放是指智能体在环境中收集的经验, 一个四元组(当前状态, 采取的动作, 获得的奖励和到达的下一状态)存储在一个记忆池中, 然后在训练神经网络的时候从这个数据集中随机抽取经验进行预测, 然后根据损失函数更新Q网络的参数. 这种随机抽样能够打破时间上的相关性, 使得训练数据更加多样化, 独立, 从而减少模型的偏差.

---

算法可以被表示为:

1. 初始化经验回放记忆池$\mathcal{D}$, 容量为$N$
2. 随机初始化Q网络的参数$\theta$
3. 持续交互:
	1. 开始一个新的回合: 随机设定初始状态序列$s1=\{x_1\}$, 对其进行预处理$\phi_1 = \phi(s_1)$, 其中$x_1$是初始状态
		1. 根据当前状态$s$和$\epsilon$-贪心策略选择一个动作$a_t$
			1. 以概率$\epsilon$选择随机动作: 这是为了探索(exploration), 让智能体有一定的概率选择随机动作$a_t$, 从而探索更多的状态空间
			2. 以概率$1-\epsilon$选择估计最优动作: 这是为了利用(exploitation)当前网络的知识, 选择当前Q网络估计的最优动作$a_t = argmax_a Q(\phi(s_t), a; \theta)$. 其中, $\phi(s_t)$是当前时间步经过预处理后的状态序列, $a$动作, $\theta$是Q网络当前的参数
		2. 执行选择的动作$a_t$, 并获得即时奖励$r_t$和下一状态$x_{t+1}$
		3. 更新状态序列, 然后对其进行预处理, 得到$\phi_{t+1}$
		4. 将该经验$(\phi_t, a_t, r_t, \phi_{t+1})$存储在记忆池$\mathcal{D}$中 
		5. 从记忆池$\mathcal{D}$中随机采样一个小批量的经验$(\phi_j, a_j, r_j, \phi_{j+1})$, 进行预测, 打破样本相关性
			1. 如果$\phi_{j+1}$是终止状态, 则直接利用$r_j$作为目标Q值, 即$y_j=r_j$
			2. 如果$\phi_{j+1}$不是终止状态, 则根据Bellman公式有$y_j=r_j+\gamma max_{a'}Q(\phi_{j+1}, a'; \theta)$
		6. 计算误差函数$L = (y_j - Q(\phi_j, a_j;\theta ))^2$进行梯度下降最小化更新网络参数$\theta$
		7. 当记忆池容量已满, 则结束当前回合; 否则, 继续
	2. 回合结束? 进行下一回合
4. 终止: 若达到固定回合数, 或者所有的Q值变化都很小了, 小于某个阈值近似收敛, 则终止算法
			
#### 目标网络

在Q网络中, 网络的参数$\mathcal{w}$是在不断更新的, 而目标Q值的计算过程是基于网络的当前参数的, 这回导致目标Q值不太稳定, 导致Q值很难收敛, 