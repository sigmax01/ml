---
title: 强化学习
comments: true
---

## 背景

监督学习旨在通过训练集中的已知标签$y$来训练模型, 使模型的输出尽可能模仿这些标签, 通常用于分类或回归任务. 例如, 将不同类型的宝可梦进行分类, 在这种情况下, 输入有一个正确答案或者标签, 模型试图预测每个输入的标签.

<figure markdown='1'>
![](https://img.ricolxwz.io/4ac5abb0cf015b7b4090727b14619b9f.png){ loading=lazy width='350' }
</figure>

但是, 在一些复杂任务中, 如让机器人学会骑自行车中, 定义"正确答案"非常困难, 很难提供明确的监督信号去指导算法模仿, 因此监督学习不适合这类问题. 

<figure markdown='1'>
![](https://img.ricolxwz.io/59327b41a383dd54769205f503770e69.png){ loading=lazy width='250' }
</figure>

## 定义

强化学习可以用来解决上述问题, 可以通过婴儿学习的类比来解释: 婴儿在成长过程中没有明确的老是指导, 而是通过与环境的互动积累经验. 例如, 婴儿通过玩耍, 挥动手臂或四处观察等活动, 接受到来自环境的正面(如称赞)和负面(如批评)反馈, 这些反馈塑造了他们的行为和性格, 这种试错和反馈的过程类似于强化学习中的"反馈机制".

<figure markdown='1'>
![](https://img.ricolxwz.io/3c49aa991e47c617c6c802da039debb3.png){ loading=lazy width='500' }
</figure>

强化学习的目标是学习如何做出一系列良好的决策. 智能体, Agent根据当前的状态$s_t$采取行动$a_t$, 并根据从环境中获得的奖励来调整其策略, 通过试错来最大化累计奖励. 这种奖励可以用奖励函数$r_t$来描述, 它是一个标量, 反馈智能体在$t$时刻的表现好坏. 

<figure markdown='1'>
![](https://img.ricolxwz.io/572319de8ee175bda36bc5024ccf4a7a.png){ loading=lazy width='250' }
</figure>

???+ tip "Tip"

	"奖励假设"告诉我们"所有目标都可以通过最大化预期的累计奖励来描述", 这意味着, 只要能够设计一个合适的奖励函数, 我们就可以通过强化学习实现各种复杂的目标, 涵盖从游戏到机器人控制的不用应用场景.

在每一个时间步, 智能体接受状态$s_t$和奖励$r_t$, 执行行动$a_t$. 环境接受行动$a_t$, 更新状态为$s_{t+1}$, 并发出新奖励$r_{t+1}$. 历史$H_t=s_1, r_1, a_1, ..., a_{t-1}, s_t, r_t$记录了智能体从开始到当前时间步$t$为止所有的状态, 行动和奖励, 接下来的行动$a_t$取决于这个历史.