---
title: GPT
comments: false
---

# GPT[^1]

## 摘要

NLP包括很多任务, 如[文本蕴含](https://en.wikipedia.org/wiki/Textual_entailment, "判断一个文本片段是否能够逻辑上推导出另一个文本片段, <br>可用于信息检索, 问答系统, 自动摘要等任务"), [问答系统](https://en.wikipedia.org/wiki/Question_answering "针对用户提出的问题, 系统能够理解问题并给出准确的答案, <br>可用于搜索引擎, 智能客服, 知识库问答等任务"), [语义相似度评估](https://en.wikipedia.org/wiki/Semantic_similarity "评估两个文本在语义上的相似度, <br> 可用于信息检索, 文本聚类, 抄袭检测等任务"), [文档分类](https://en.wikipedia.org/wiki/Document_classification "将文档归类到预定义的类别中 <br>可用于垃圾邮件过滤, 新闻分类, 情感分析等任务"). 虽然大型未标注的语料库非常充足, 但是用于特定任务的已标注的文本确非常少, 导致[判别式模型](/dicts/discriminative-and-generative-model)很难在这些NLP任务中取得很好的性能. 作者展示了一种生成式预训练模型, Generative Pre-trained Model, 它通过在大量未标注的语料上训练, 学习通用的语言表示, 然后, 针对特定任务通过少量标注数据进行微调, 可以很好的完成任务. 这种预训练模型在性能上甚至优于那些专门为特定任务而设计的判别式模型, 在12项任务中的9项都打到了SOTA的表现. 在Stories Cloze Test上获得了8.9%的绝对提升, 在RACE上为5.7%, 在MultiNLI上为1.5%.

## 背景 {#intro}

### 动机

从raw文本中直接学习的能力对减轻NLP对监督学习依赖至关重要. 许多深度学习的方法需要大量的手工标注的数据, 导致它们在很多领域应用的潜力受到限制. 在这种情况下, 对于未标记文本中的语言信息进行建模, 相比于收集更多的高质量的标注, 不失为一种可行的方案. 而且, 即使在有足够多的带标注的数据的情况下, 以无监督方式学习良好的表示也能够显著地提高性能. 迄今为止, 最令人信服的证据是, [词嵌入预训练](/algorithm/neural-network/word-embedding/#transfer-learning)的广泛应用.

???+ note "优化目标和NLP任务的区别"

    优化目标(Optimization Objective)是训练模型的一种**手段和策略**, 侧重于如何从数据中提取有效的信息. NLP任务是现实中需要解决的具体问题, 是**最终需求**, 评估模型在这些问题上的表现. 优化目标的典型例子有语言建模(Language Modeling), 预测下一个单词或者填补句中的空白, 如BERT中的MLM; 对比学习(Contrastive Learning), 最小化正样本之间的相似性, 最小化负样本之间的相似性等等... NLP任务的典型例子是文本分类, 机器翻译, 问答系统, 文本生成等等...

    | **维度**   | **优化目标**                     | **NLP任务**                     |
    |------------|----------------------------------|----------------------------------|
    | **定义**   | 指导模型训练的数学目标           | 解决实际语言处理问题的具体功能   |
    | **阶段**   | 用于模型训练                    | 用于模型评估或实际应用           |
    | **范围**   | 通常通用，适用于多个任务         | 具体、与应用场景相关             |
    | **目标对象**| 优化模型参数，学习表示          | 评估模型是否能够完成特定任务     |
    | **举例**   | 语言建模、对比学习目标           | 文本分类、机器翻译、问答系统     |

从未标注数据中提取超越词级别的信息(即词嵌入获得的信息)是具有挑战性的, 主要有两个原因:

1. **不清楚哪种优化目标最能有效地学习适用于迁移的文本表示, 因为不同的下游任务(如情感分析, 文本蕴含, 机器翻译等)对文本表示的需求不同.** 近期的研究探索了多种优化目标, 比如语言建模(预测下一个词或者MLM), 机器翻译, 篇章连贯性等等, 每种方法都在不同的任务上优于其他方法, 如语言建模可能对生成任务表现良好, 篇章连贯性可能对需要理解文本逻辑关系的任务(如文本蕴含)更有优势, 机器翻译可能在多语言任务中更加有效.
2. **到目前为止还没有在最有效的迁移这些学习到的表示到目标任务的方法上达成一致.** 现有方法通常包括:
    1. 针对特定任务调整模型架构. 如为分类任务添加分类头, 为序列标注任务添加序列解码模块, 这些调整需要针对不同任务进行设计, 缺乏通用性.
    2. 使用复杂的训练策略. 如微调, 调整预训练模型参数, 使其适应目标任务; 冻结部分参数, 仅调整模型的一部分, 减少计算开销; 逐层解锁, 从浅层到深层逐步调整参数, 这些学习策略的设计和选择通常需要大量实验和经验, 增加了开发的复杂性.
    3. 添加辅助学习目标: 除了主任务的目标函数外, 引入额外的优化目标来辅助模型训练, 如在情感分析任务中, 同时训练模型识别文本中的关键情感用语; 使用对比学习目标增强嵌入的语义一致性.

这些不确定性使得开发有效的[半监督学习](https://zh.wikipedia.org/zh-hans/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0)方法变得困难.

### 方案

在这篇文章中, 作者探索了一种用于语言理解任务的半监督学习方法, 该方法包括**无监督预训练和监督微调**. 目标是学习到一种通用的表征, 该表征能够在少量调整的情况下迁移到各种任务. 他们假设可以访问大量的未标记文本语料库和几个手动标注的训练示例(目标任务)的数据集, 并不要求这些目标任务和未标注语料库所代表的领域相同. 他们的训练分为两个阶段: 第一个阶段, 在未标记的数据上使用语言建模作为优化目标来学习神经网络模型的初始参数. 第二个阶段, 使用相应的监督目标将这些参数适应于各种目标任务.

对于模型架构, 他们选择了Transformer, 他已经被证明可以用于多种任务, 如机器翻译, 文件生成和句法分析. 这个模型相比于RNN能够提供用于处理文本中长期依赖关系的更结构化的记忆, 从而能够在各种目标任务中实现强大的迁移性能. 在迁移的国臣各种, 他们使用了遍历式方法(Traversal-style Approaches)将结构化文本输入(如表格, 树状数据)转化为单一的, 连续的token序列, 使其可以直接输入到预训练的模型中, 避免了额外的结构化处理模块, 这种方式可以以很少的对预训练模型的调整实现对目标任务的高效适配.

### 评估

除了上述的的评估方法以及结果, 它们还衡量了zero-shot的表现. 并发现, 预训练模型获得了可以用于下游任务的有用语言知识.

???+ note "什么是zero-shot"

    zero-shot(零样本学习)是机器学习和NLP中的一种技术或者学习范式. 指的是模型在没有见过任何目标任务或者目标类别的训练样本情况下, 能够直接对新任务或者新类别进行预测.

## 相关工作

### NLP领域的半监督学习

他们的工作主要属于NLP的半监督学习范畴. 该范式在序列标注(如NER)或者文本分类等任务中得到了广泛引用. 较早的半监督学习方法利用未标注的数据来计算词汇级别或者短语级别的统计信息, 如词频或者共现信息, 然后, 使用这些统计信息喂给监督学习模型. 近年来, 研究人员提出了利用词嵌入的方法, 这些嵌入是在未标注的语料上训练的, 它将短语隐射到一个密集的向量空间, 将词汇的语义转化为向量表达. 然而, 这些方法主要关注传递词汇级别的信息, 而作者的工作是理解更加复杂/高层次的语义信息. 最近的工作调查了从未标注数据中学习利用超越词汇层次的语义信息, 基于句子级别, 段落级别的嵌入的模型, 可以使用未标记语料库进行训练, 从而将文本编码为适合各种目标任务的向量表示.

### 无监督预训练

**无监督预训练+有监督微调(two-stages)是半监督学习的一种特例, 其目的是去找到一个好的初始化点而不是修改监督学习的优化目标.** 早期的工作主要集中在图像分类和回归任务中, 探讨无监督预训练是否能提高模型在这些任务上的表现. 后续的研究对无监督预训练进行了更加详细的分析和验证, 发现, 无监督预训练不仅仅是为了找到一个好的初始点, 它还充当了一种正则化机制, 帮助模型避免过拟合. 在最近的工作中, 无监督预训练方法已经被广泛地应用于不同的深度学习任务中, 尤其是在图像分类, 语音识别, 机器翻译等任务中都得到了较好的效果.

???+ note "对于第一句话的个人理解"

    🐝无监督学习(预训练阶段)的优化目标和监督学习(微调阶段)的优化目标通常是不同的, 无监督学习阶段的优化目标是语言模型(预测下一个词或者MLM), 而监督学习阶段的优化目标通常和无监督学习阶段的优化目标不同, 这个时候的目标是直接优化与具体任务相关的目标函数(如分类的交叉熵损失, 回归的均方误差等). 无监督学习的优化目标和监督学习的优化目标**通常不同**.

与我们最相似的研究使用语言模型作为优化目标对神经网络进行预训练, 然后对目标任务进行监督学习微调. Dai等人[^2]和Howard, Ruder[^3]的工作都采用了上述的方法来改进文本分类任务. 但是, 他们使用的是LSTM模型(因为那个时候Transformer还没有出来). 与LSTM不同, 作者选择了Transformer, 它能捕获全局注意力而且能够使用GPU进行密集并行计算. 而且, 作者不仅仅在文本分类任务上取得了不错的结果, 还在其他的下游任务中展现出了良好的性能, 如自然语言推理, 同义句检测和故事补全, 具有很好的泛化能力.

还有一些其他的方法使用了从预训练的语言模型或者机器翻译模型中提取的隐藏层表示作为辅助特征, 并在目标任务上训练一个监督学习模型. 简单来说, 这些方法将预训练模型生成的特征用作输入, 进行监督学习. 但是, 这种方法需要在每个目标任务上引入大量新的参数, 导致模型的复杂性大大增加. 与之相比, 作者的方法在进行任务迁移的时候只需要对模型架构进行极小的修改, 不需要为每个任务训练大量新的参数, 减少计算和存储开销.

### 辅助训练目标

辅助训练目标(Auxiliary Training Objectives)指在监督学习模型的主要优化目标之外, 加一些额外的无监督优化目标, 它是一种实现半监督学习的另一种形式(这种就没有two-stages, 相当于是结合了有监督学习和无监督学习的优化目标, 同时优化它俩的损失函数, 是one-stage). 如Rei等人[^5]在目标任务训练过程中, 添加了一个辅助的语言建模目标, 这意味着在微调阶段, 模型不仅优化目标任务的损失函数, 还同时优化语言建模的损失函数. Collobert和Weston[^4]在训练模型的时候, 不仅仅专注于主要任务(语义角色标注), 还同时优化了词性标注, 分块, NER, 语言建模的损失函数. 作者的研究也引入了辅助优化目标, 作者强调, 虽然它们也使用了辅助目标, 但是, two-stages的方案(即无监督预训练+有监督微调)已经足够强大, 使得这种形式的半监督学习的必要性相对较低.

## 框架

### 无监督预训练

[^1]: Radford, A. (2018). Improving language understanding by generative pre-training. https://www.mikecaptain.com/resources/pdf/GPT-1.pdf
[^2]: Dai, A. M., & Le, Q. V. (2015). Semi-supervised sequence learning (No. arXiv:1511.01432). arXiv. https://doi.org/10.48550/arXiv.1511.01432
[^3]: Howard, J., & Ruder, S. (2018). Universal language model fine-tuning for text classification (No. arXiv:1801.06146). arXiv. https://doi.org/10.48550/arXiv.1801.06146
[^4]: Collobert, R., & Weston, J. (2008). A unified architecture for natural language processing: Deep neural networks with multitask learning. Proceedings of the 25th International Conference on Machine Learning, 160–167. https://doi.org/10.1145/1390156.1390177
[^5]: Rei, M. (2017). Semi-supervised multitask learning for sequence labeling (No. arXiv:1704.07156). arXiv. https://doi.org/10.48550/arXiv.1704.07156
