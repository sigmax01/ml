---
title: GPT
comments: false
---

# GPT[^1]

## 摘要

NLP包括很多任务, 如[文本蕴含](https://en.wikipedia.org/wiki/Textual_entailment, "判断一个文本片段是否能够逻辑上推导出另一个文本片段, <br>可用于信息检索, 问答系统, 自动摘要等任务"), [问答系统](https://en.wikipedia.org/wiki/Question_answering "针对用户提出的问题, 系统能够理解问题并给出准确的答案, <br>可用于搜索引擎, 智能客服, 知识库问答等任务"), [语义相似度评估](https://en.wikipedia.org/wiki/Semantic_similarity "评估两个文本在语义上的相似度, <br> 可用于信息检索, 文本聚类, 抄袭检测等任务"), [文档分类](https://en.wikipedia.org/wiki/Document_classification "将文档归类到预定义的类别中 <br>可用于垃圾邮件过滤, 新闻分类, 情感分析等任务"). 虽然大型未标注的语料库非常充足, 但是用于特定任务的已标注的文本确非常少, 导致[判别式模型](/dicts/discriminative-and-generative-model)很难在这些NLP任务中取得很好的性能. 作者展示了一种生成式预训练模型, Generative Pre-trained Model, 它通过在大量未标注的语料上训练, 学习通用的语言表示, 然后, 针对特定任务通过少量标注数据进行微调, 可以很好的完成任务. 这种预训练模型在性能上甚至优于那些专门为特定任务而设计的判别式模型, 在12项任务中的9项都打到了SOTA的表现. 在Stories Cloze Test上获得了8.9%的绝对提升, 在RACE上为5.7%, 在MultiNLI上为1.5%.

## 背景 {#intro}

### 动机

从raw文本中直接学习的能力对减轻NLP对监督学习依赖至关重要. 许多深度学习的方法需要大量的手工标注的数据, 导致它们在很多领域应用的潜力受到限制. 在这种情况下, 对于未标记文本中的语言信息进行建模, 相比于收集更多的高质量的标注, 不失为一种可行的方案. 而且, 即使在有足够多的带标注的数据的情况下, 以无监督方式学习良好的表示也能够显著地提高性能. 迄今为止, 最令人信服的证据是, [词嵌入预训练](/algorithm/neural-network/word-embedding/#transfer-learning)的广泛应用.

???+ note "优化目标和NLP任务的区别"

    优化目标(Optimization Objective)是训练模型的一种**手段和策略**, 侧重于如何从数据中提取有效的信息. NLP任务是现实中需要解决的具体问题, 是**最终需求**, 评估模型在这些问题上的表现. 优化目标的典型例子有语言建模(Language Modeling), 预测下一个单词或者填补句中的空白, 如BERT中的MLM; 对比学习(Contrastive Learning), 最小化正样本之间的相似性, 最小化负样本之间的相似性等等... NLP任务的典型例子是文本分类, 机器翻译, 问答系统, 文本生成等等...

    | **维度**   | **优化目标**                     | **NLP任务**                     |
    |------------|----------------------------------|----------------------------------|
    | **定义**   | 指导模型训练的数学目标           | 解决实际语言处理问题的具体功能   |
    | **阶段**   | 用于模型训练                    | 用于模型评估或实际应用           |
    | **范围**   | 通常通用，适用于多个任务         | 具体、与应用场景相关             |
    | **目标对象**| 优化模型参数，学习表示          | 评估模型是否能够完成特定任务     |
    | **举例**   | 语言建模、对比学习目标           | 文本分类、机器翻译、问答系统     |

从未标注数据中提取超越词级别的信息(即词嵌入获得的信息)是具有挑战性的, 主要有两个原因:

1. **不清楚哪种优化目标最能有效地学习适用于迁移的文本表示, 因为不同的下游任务(如情感分析, 文本蕴含, 机器翻译等)对文本表示的需求不同.** 近期的研究探索了多种优化目标, 比如语言建模(预测下一个词或者MLM), 机器翻译, 篇章连贯性等等, 每种方法都在不同的任务上优于其他方法, 如语言建模可能对生成任务表现良好, 篇章连贯性可能对需要理解文本逻辑关系的任务(如文本蕴含)更有优势, 机器翻译可能在多语言任务中更加有效.
2. **到目前为止还没有在最有效的迁移这些学习到的表示到目标任务的方法上达成一致.** 现有方法通常包括:
    1. 针对特定任务调整模型架构. 如为分类任务添加分类头, 为序列标注任务添加序列解码模块, 这些调整需要针对不同任务进行设计, 缺乏通用性.
    2. 使用复杂的训练策略. 如微调, 调整预训练模型参数, 使其适应目标任务; 冻结部分参数, 仅调整模型的一部分, 减少计算开销; 逐层解锁, 从浅层到深层逐步调整参数, 这些学习策略的设计和选择通常需要大量实验和经验, 增加了开发的复杂性.
    3. 添加辅助学习目标: 除了主任务的目标函数外, 引入额外的优化目标来辅助模型训练, 如在情感分析任务中, 同时训练模型识别文本中的关键情感用语; 使用对比学习目标增强嵌入的语义一致性.

这些不确定性使得开发有效的[半监督学习](https://zh.wikipedia.org/zh-hans/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0)方法变得困难.

### 方案

在这篇文章中, 作者探索了一种用于语言理解任务的半监督学习方法, 该方法包括**无监督预训练和监督微调**. 目标是学习到一种通用的表征, 该表征能够在少量调整的情况下迁移到各种任务. 他们假设可以访问大量的未标记文本语料库和几个手动标注的训练示例(目标任务)的数据集, 并不要求这些目标任务和未标注语料库所代表的领域相同. 他们的训练分为两个阶段: 第一个阶段, 在未标记的数据上使用语言建模作为优化目标来学习神经网络模型的初始参数. 第二个阶段, 使用相应的监督目标将这些参数适应于各种目标任务.

对于模型架构, 他们选择了Transformer, 他已经被证明可以用于多种任务, 如机器翻译, 文件生成和句法分析. 这个模型相比于RNN能够提供用于处理文本中长期依赖关系的更结构化的记忆, 从而能够在各种目标任务中实现强大的迁移性能. 在迁移的国臣各种, 他们使用了遍历式方法(Traversal-style Approaches)将结构化文本输入(如表格, 树状数据)转化为单一的, 连续的token序列, 使其可以直接输入到预训练的模型中, 避免了额外的结构化处理模块, 这种方式可以以很少的对预训练模型的调整实现对目标任务的高效适配.

### 评估

除了上述的的评估方法以及结果, 它们还衡量了zero-shot的表现. 并发现, 预训练模型获得了可以用于下游任务的有用语言知识.

???+ note "什么是zero-shot"

    zero-shot(零样本学习)是机器学习和NLP中的一种技术或者学习范式. 指的是模型在没有见过任何目标任务或者目标类别的训练样本情况下, 能够直接对新任务或者新类别进行预测.

[^1]: Radford, A. (2018). Improving language understanding by generative pre-training. https://www.mikecaptain.com/resources/pdf/GPT-1.pdf
