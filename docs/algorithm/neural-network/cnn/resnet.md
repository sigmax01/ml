---
title: ResNet
comments: false
---

在深度学习中, 网络的"深度", 即层数通常和模型的能力成正比. 然而, 随着网络深度的增加, 一些问题也随之出现, 最突出的就是[梯度消失和爆炸问题](/algorithm/neural-network/#vanishing-gradient), 这使得深层网络难以训练.

**深度网络自然地整合了低层, 中层和高层特征**. 深度网络由多个层组成, 每一层的输出会成为下一层的输入. 在这个过程中, 网络的前几层(低层特征)通常提取一些简单的特征, 比如图像的边缘, 角点等基础信息. 中间层(中层特征)会基于低层特征组合出更复杂的模式, 比如形状或者局部纹理. 后几层(高层特征)会逐渐关注更加抽象的语义信息, 比如对象类别或者场景含义.

在深度网络中, 特征提取(如卷积层提取特征)和分类器(如全连接层最终输出列别)是一个整体, 通过端到端训练(即从输入到输出的误差传播), 网络会自动调整特征和分类器的参数. 这种方式比传统的分开设计特征提取和分类器的方法更加高效, 也更加准确.

深度残差神经网络, Deep Residual Network, 简称ResNet, 它由微软研究院何凯明等人在2015年首次提出, 在深度学习领域产生了深远的影响. 它通过一种创新的"残差学习"机制, 解决了传统深度神经网络中的梯度消失问题, 从而实现了对非常深度网络的有效训练.

ResNet的核心思想是引入残差块, Residual Block, 通过捷径连接, Shortcut Connection让信息直接跳过一层或者多层网络. 
