---
title: AlexNet
comments: false
---

## 架构[^1]

### ReLU激活函数

AlexNet采用的是非饱和非线性函数, 而不是才用传统的饱和非线性函数, 如$\tanh$或者是sigmoid. 饱和的意思就是说当$x$的值变得很大或者很小的时候, 饱和函数的值会解禁期极限, 在这些区域, 导数趋近于$0$, 导致梯度下降更新缓慢, 称为"梯度消失问题". 梯度消失会导致网络学习效率大幅降低, 尤其是在深度网络中, 靠近输入层的参数几乎无法被有效更新. 

$max(0, x)$是AlexNet采用的非饱和非线性函数. 输出为正值的时候, 导数恒为$1$, 输出为负值的时候, 导数恒为$0$. 

<figure markdown='1'>
![](https://img.ricolxwz.io/ef635b798a4cfa5fe4207d4d54967ff5.png){ loading=lazy width='300' }
</figure>

从图中可以看出, 使用一个ReLU的四层CNN达到$25\%$错误率的速度比使用$\tanh$激活函数的CNN快$6$倍. 

> Jarrett et al. [11] claim that the nonlinearity f (x) = |tanh(x)| works particularly well with their type of contrast normalization followed by local average pooling on the Caltech-101 dataset.

虽然早期的研究也用了$|\tanh(x)|$作为激活函数, 但是他们的主要目标是避免过拟合, 而不是真正的加速模型的训练.

### 分布式GPU训练

AlexNet的作者使用了两块GTX 580训练, 至于这里为什么是$2$, 是因为它的内存太小了, 导致无法把整个网络放在上面. 

得益于GPU能够直接访问和写入其他GPU的内存, 不用经过宿主的通用内存, GPU的分布式训练得以发展. 

他们将卷积核(神经元)分半分配到两个GPU上, 每个GPU只处理一部分神经元. 并且, 他们还使用了一种优化通信的策略: 在普通的卷积神经网络中, 某一层的所有卷积核(或神经元)会从上一层的所有特征图中获取输入, 比如, 层3的核会从层2的所有特征图中获取信息, 这种全连接方式有助于模型更好地获取全局信息, 但是在多GPU并行训练的时候会带来巨大的通信开销. 该文章引入了一个新的方法, 仅在部分层进行GPU间的通信, 比如, 层3的核心仍然从层2的所有特征图中获取输入(即, 跨GPU通信仍然会发生), 但是从层3到层4的时候, 每个GPU上的核心只能从同一个GPU的特征图获取输入.

不同的连接模式会影响通信量: 全连接(每层的所有神经元都连接)通信量大但是性能受限; 部分连接(限制某些层或者某些神经元的连接)减少通信但是可能会影响模型的性能. 可以使用交叉验证来测试不同的连接模式, 直到找到通信量和计算量之间的最佳平衡点, 目标是调整通信量, 使其成为总计算量中可以接受的比例.

这个结构其实和一种"柱状"的CNN较为类似, 每个柱都是一部分独立的CNN分支, 这些柱可以看作是并行的网络结构, 每个柱都会生成自己的输出, 通过后续层将这些输出融合在一起. 文中提到的是非独立的柱状CNN, 这个柱不是完全独立的, 柱之间会共享某些信息, 通过两块GPU分别负责不同的柱, 能够实现并行计算.

由于网络绝大部分的参数都集中在了全连接层的第一层上, 而全连接层的第一层接受来自卷积层的最后一层的输出作为输入, 为了保持双GPU网络和单GPU网络的参数总量相近, 设计者决定不减少最后一层卷积层和全连接层的参数数量.

### 局部归一化[^2]

局部归一化, Local Response Normalization (LRN)首次在AlexNet中提出, 通过实验证实可以提高模型的泛化能力, 但是提升的很少, 而且计算复杂, 以至于后面都不再使用, 甚至有人觉得它是一个伪命题, 因此饱受争议. 如下是LRN的公式.

$$
b_{i}^{x, y} = \frac{a_{i}^{x, y}}{\left( k + \alpha \sum_{j=\max(0,i-n/2)}^{\min(N-1,i+n/2)} (a_{j}^{x, y})^2 \right)^\beta}
$$

这个$(x, y)$应该是特征图上的坐标, 原始图像的部分区域(感受野)会通过卷积核i卷积映射到特征图中的单个坐标点$(x, y)$. $a_i^{x, y}$表示的是特征图中位置$(x, y)$的激活值.

$n$表示的是领域大小, 表示局部归一化的范围, 通常是一个奇数, 表示对前后总共$n$个通道进行归一化. $\sum$是一个求和操作, 对局部范围内所有通道, 从第$j = \max(0, i - n/2)  到  j = \min(N-1, i + n/2) ）$的激活值平方求和. $j$表示邻域的索引, $N$表示输入特征图的通道总数. $k$是一个偏置值, 防止分母为零, $\alpha$控制归一化强度的比例系数. 

LRN受到了神经生物学的一个启发. 侧抑制(Lateral Inhibition)是一种来源于神经生物学的机制, 用来描述被激活的神经我对周围相邻神经元的抑制作者用. 这种机制在生物神经系统中非常普遍, 特别是在视觉, 听觉等感知系统中, 用于增强对比度和突出关键信息. 例如, 在视网膜中, 当某些感光细胞对光有强烈响应的时候, 他们会抑制周围感光细胞的响应, 突出边缘或者对比度. 

> where the sum runs over n “adjacent” kernel maps at the same spatial position, and N is the total number of kernels in the layer.

这里的"adjacent" kernel maps指的是同一层中不同的卷积核学习到的特征图. 每个卷积核学习到一种特定的特征. 归一化的目的是通过相邻特征图之间的交互来抑制过大的响应值, 也就是说, 模拟生物神经网络中的"侧抑制", 一个卷积核的强响应值会抑制邻近卷积核的响应值, 突出显著特征. 他强调的是特征图之间的竞争, 同时也可以促进多样性特征的学习, 避免单一卷积核主导学习. 

### 重叠池化

传统的CNN中的池化单元是不能重叠的. 准确的说, 池化层可以被认为是由间隔$s$像素的池化单元网格组成, 每个单元总结其位置中心周围大小为$z*z$的区域, 如果我们设置$s=z$, 那么我们会得到一个传统CNN中的池化层; 如果我们设置$s<z$, 那么我们会得到一个含有重叠单元的池化层. 

---

如下图, 是一张AlexNet整体架构图:

<figure markdown='1'>
![](https://img.ricolxwz.io/1588ab9320a43a44d17240bf1a5aa017.png){ loading=lazy width='800' }
</figure>

其中, 第二, 四, 五卷积层的核仅仅连接到前一层中位于同一GPU上的那些特征图. 第三卷积层的核连接到前一层的所有神经元, 从图中可以看到是不同GPU对应的部分之间是有虚线连接的(说明存在GPU之间的通信). 全连接层中的神经元连接到前一层的所有神经元(为了保持和单GPU训练时的参数量一致).

第一, 二个卷积层之后有一个LRN层, 重叠池化层位于两个LRN层和第五卷积层之后, ReLU非饱和非线性函数应用于每个卷积层和全连接层的输出. 

注意, 第一个卷积层使用了$256$个$5*5*48$的卷积核, 应该会产生$256$个特征图, 这里, 这些特征图中$128$个被发送到GPU0训练, $128$个被发送到GPU1训练, 所以在图中表示出来是上面$128$, 下面$128$, 但是实际上第二个卷积层的卷积核的通道数还是$256$. 同理, 对于下面几层也是.

## 减少过拟合[^1]

### 数据增强

最简单的降低过拟合的方法是对数据集进行一个人工的标签保留转换(label-preserving transformations). 作者采用了两种方法, 它使用的是CPU在原始图像上做出修改, 这种CPU和GPU的分工并行导致计算资源的利用率较高.

第一种方法是通过从$256*256$的图像中提取随机的$224*224$的补丁(及其水平反转)并用这些提取的补丁来训练我们的网络. 在预测的时候, 模型从图像的各个corner和中间取$5$个$224*224$的补丁(包括他们的水平反转), 然后这$10$个补丁的softmax值取平均. 

第二种是用PCA. 首先来复习一下什么是特征值和特征向量. 

$\bm{A}$是$n$阶矩阵, 如果数$\lambda$和$n$维非$0$列向量$\bm{x}$满足$\bm{A}\bm{x}=\lambda\bm{x}$, 那么数$\lambda$称为$\bm{A}$的特征值. $\bm{x}$称为$\bm{A}$对应于特征值$\lambda$的特征向量. 式$\bm{A}\bm{x}=\lambda\bm{x}$可以写成$(\bm{A}-\lambda\bm{E})\bm{x}=0$, $|\lambda\bm{E}-\bm{A}|$叫做$\bm{A}$的特征多项式. 当特征多项式等于$0$的时候, 称为$\bm{A}$的特征方程, 特征方程是一个齐次线性方程组, 求解特征值的过程其实就是求解特征方程的解. 

这种数据增强的方法改变了训练图像中RGB通道的像素值. 具体步骤如下: 首先, 对整个ImageNet训练集中所有图像的RGB像素值进行主成分分析(PCA), 对于每一张训练图像, 将一些主成分的线性组合添加到每个像素值上, 添加的量由以下组成:

- 对应主成分的特征值(代表主成分的重要性)
- 一个从均值为$0$, 标准差为$0.1$的高斯分布中随机采样的值

注意, PCA通过线性变换将原始的RGB通道重新组合成一组主成分, 这些主成分所代表的方向可以看作是新的坐标轴.

对于每个RGB像素$I_{xy} = [I^R_{xy}, I^G_{xy}, I^B_{xy}]^T + [\mathbf{p}_1, \mathbf{p}_2, \mathbf{p}_3] [\alpha_1 \lambda_1, \alpha_2 \lambda_2, \alpha_3 \lambda_3]^T$, 其中, $\mathbf{p}_i$是主成分向量, $\lambda_i$是第$i$个主成分的特征值, 它衡量了$\mathbf{p}_i$对数据整体方差的贡献大小, 特征值越大, 说明这个主成分越重要.

这个$\alpha_i$, 对于每张训练图像来说, 只在该图像被用于训练的时候随机生成一次, 在这张图像的整个训练过程中, $\alpha_i$的值对所有像素点保持不变, 如果这张图像在后续训练中再次使用, 则会重新生成它的$\alpha_i$值. 如, 在第一个epoch中, $\alpha_1$, $\alpha_2$, $\alpha_3$会被随机生成为$-0.05, 0.1, -0.02$, 在这次训练中, 图像中的所有像素点都会以这组$\alpha_i$值进行扰动, 到了第二个epoch的时候, 图像再次被使用, $\alpha_i$会重新生成. 

### Dropout

将各种不同模型的预测组合起来的方法是一种很好的减少测试误差的方法, 但是对于动辄需要几天来训练的大型神经网络来说, 这几乎不可能. 

该文章采用了Dropout来解决训练时间长和过拟合的问题, 其核心思想就是有$50\%$的可能性将隐藏层神经元的输出设置为$0$. 这会导致那些被drop out的神经元不会参与前向/反向传播. 

[^1]: Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. Advances in Neural Information Processing Systems, 25. https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html
[^2]: LoveMIss-Y. (2019, 三月 26). 深度学习饱受争议的局部响应归一化(LRN)详解. Csdn. https://blog.csdn.net/qq_27825451/article/details/88745034