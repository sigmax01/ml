## å°é¢˜è€ƒç‚¹

ä¸ªäººè®¤ä¸ºçš„å°é¢˜è€ƒç‚¹(æŒç»­æ›´æ–°ä¸­...):

é‡è¦ç¨‹åº¦: 

- è¶…çº§é‡è¦: â˜¢ï¸
- é‡è¦: âš ï¸
- ä¸€èˆ¬: â™»ï¸
- ä¸é‡è¦: ğŸ—‘ï¸
- è¶…çº§ä¸é‡è¦: ğŸ´â€â˜ ï¸

- é¢„å¤„ç†
    - âš ï¸ä¸ºä»€ä¹ˆè¦è¿›è¡Œé¢„å¤„ç†: not perfect, noise(distortion(human voice), spurious(outlier or mixed with non-noisy data), inconsistent(negative weight, non-existing zip code), duplicate), missing
    - âš ï¸å™ªéŸ³å¦‚ä½•è¿›è¡Œå¤„ç†: signal/image processing&outlier detection; use robust ml algorithm; easy to deal with inconsistent&duplicate
    - âš ï¸ç¼ºå¤±æ•°æ®å¦‚ä½•å¤„ç†: ignore all examples with missing values; estimate the missing values by remaining values(nominal: replace most common in A, replace most common in A with same class; numerial: average value of nearest neighbors)
	- â™»ï¸ä»€ä¹ˆæ˜¯æ•°æ®èšåˆ: combining two or more attributes into one
    - â™»ï¸ä¸ºä»€ä¹ˆè¦è¿›è¡Œæ•°æ®èšåˆ: data reduction(same memory&computation time); change scale; stabilize data(less variable)
	- â™»ï¸ä»€ä¹ˆæ˜¯é€‰æ‹©ç‰¹å¾å­é›†: the process of removing irrelevant and redundant features
	- âš ï¸ä¸ºä»€ä¹ˆè¦é€‰å–ç‰¹å¾å­é›†: improves accuracy; faster building; easier to interpret
    - âš ï¸å¦‚ä½•é€‰å–ç‰¹å¾å­é›†: brute force(try all possible pairs and see the results); embedded(e.g. decision tree, use entropy or gini); filter(based on statistical measures, e.g. mutual information, information gain; or based on correlation, e.g. relief); wrapper(use ML algorithm as the black box)
    - âš ï¸å¦‚ä½•ä¸ºç‰¹å¾æ·»åŠ æƒé‡: based on domain knowledge; some algorithm, e.g. boosting can automatically add weight to features
    - â™»ï¸å¦‚ä½•å¯¹è¿ç»­æ•°æ®è¿›è¡Œç¦»æ•£åŒ–(discretization): equal width; equal frequency; clustering
    - âš ï¸å½’ä¸€åŒ–çš„ä½œç”¨: avoid the dominance attributes with large values
    - â™»ï¸æ ‡å‡†åŒ–: assume data follows Gaussian distribution, convert it to standard Gaussian distribution(average -1, standard deviation 1) 
	- â™»ï¸ä½™å¼¦ç›¸ä¼¼åº¦å’Œçš®å°”é€Šç›¸å…³ç³»æ•°ç»“æœçš„å«ä¹‰: consine similarity = 0, 0; corr = -1, +1, 0
- KNN
	- â™»ï¸å¤æ‚åº¦åˆ†æ: m training examples with n attibutes, o(mn)
	- â™»ï¸åŠ æƒæœ€é‚»è¿‘ç®—æ³•: closer? bigger weight; further? smaller weight
	- â™»ï¸ç‰¹ç‚¹: require normalization; not effective for high dimensional data; sensitive to k; very accurate; slow for big datasets; 
- æœ´ç´ è´å¶æ–¯
	- âš ï¸å…ˆéªŒæ¦‚ç‡å’ŒåéªŒæ¦‚ç‡æ˜¯å•¥: posteriori probability, probability of an event after seeing the evidence; prior probability: probability of an event before seeing evidence
	- â˜¢ï¸æœ´ç´ è´å¶æ–¯ç†è®ºä¸ºä»€ä¹ˆæ˜¯æœ´ç´ çš„: independence: attributes are conditionally independent of each other, given the class; equally importance: all attributes are equally importance
	- âš ï¸æ‹‰æ™®æ‹‰æ–¯å¤„ç†é›¶é¢‘é—®é¢˜: an attribute value does not occur with every class value, e.g. $p(E_1|yes)=0$. Use Laplace correction or smoothing
	- â™»ï¸å¤„ç†ç¼ºå¤±å€¼é—®é¢˜: do not include that posteriori probability when calculating

## å¤§é¢˜è€ƒç‚¹

ä¸ªäººè®¤ä¸ºçš„å¤§é¢˜è€ƒç‚¹(æŒç»­æ›´æ–°ä¸­...):

é‡è¦ç¨‹åº¦: 

- è¶…çº§é‡è¦: â˜¢ï¸
- é‡è¦: âš ï¸
- ä¸€èˆ¬: â™»ï¸
- ä¸é‡è¦: ğŸ—‘ï¸
- è¶…çº§ä¸é‡è¦: ğŸ´â€â˜ ï¸

- é¢„å¤„ç†
    - â™»ï¸[äºŒè¿›åˆ¶åŒ–](/algorithm/preprocessing/#bit-transform)
    - ğŸ—‘ï¸[å½’ä¸€åŒ–](/algorithm/preprocessing/#normalization)
    - â™»ï¸[è·ç¦»è®¡ç®—](/algorithm/preprocessing/#euclidean-distance), ç‰¹åˆ«æ³¨æ„Hamming distance, counts the number of different bits
    - âš ï¸[ç›¸ä¼¼ç³»æ•°è®¡ç®—](/algorithm/preprocessing/#similarity-score), å¾—åˆ°äº†ç›¸ä¼¼ç³»æ•°ä¹‹å, å¯ä»¥è®¡ç®—ç®€å•åŒ¹é…ç³»æ•°, é›…å¡å°”æŒ‡æ•°
    - âš ï¸[ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—](/algorithm/preprocessing/#cosine-similarity)
    - âš ï¸[çš®å°”é€Šç›¸å…³ç³»æ•°è®¡ç®—](/algorithm/preprocessing/#pearson-correlation-coefficient)
- æœ€é‚»è¿‘
	- â˜¢ï¸[ä½¿ç”¨k-é‚»è¿‘ç®—æ³•è¿›è¡Œé¢„æµ‹](/algorithm/knn/#knn), ä¾‹å¦‚, ä½¿ç”¨2-é‚»è¿‘ç®—æ³•, Euclidean Distance
- æœ´ç´ è´å¶æ–¯
    - â˜¢ï¸[ä½¿ç”¨æœ´ç´ è´å¶æ–¯ç®—æ³•è¿›è¡Œé¢„æµ‹](/algorithm/naive-bayes/#nb-algorithm)
    - â™»ï¸[æ•°å€¼å±æ€§æœ´ç´ è´å¶æ–¯è¿›è¡Œé¢„æµ‹](/algorithm/naive-bayes/#numeric-nb)
- è¯„ä¼°
    - â™»ï¸[æ··æ·†çŸ©é˜µè®¡ç®—](/algorithm/evaluation/#confusion-matrix)
- å†³ç­–æ ‘:
    - â˜¢ï¸[ä¿¡æ¯ç†µ, ä¿¡æ¯å¢ç›Šçš„è®¡ç®—](/algorithm/decision-tree/#information-gain)
    - â˜¢ï¸[å¦‚ä½•é€‰æ‹©æœ€ä¼˜å±æ€§](/algorithm/decision-tree/#how-to-choose-best-feature)
- é›†æˆå­¦ä¹ 
    - [Baggingå¦‚ä½•è¿›è¡ŒæŠ½æ ·](/algorithm/ensemble-learning/#bagging)
    - [Adaboostè¿›è¡Œé¢„æµ‹](/algorithm/ensemble-learning/#adaboost)
- æ”¯æŒå‘é‡æœº
    - [æ ¸æ–¹æ³•å¦‚ä½•ç®€åŒ–ç‚¹ç§¯è®¡ç®—](/algorithm/svm/#kernel-trick)
- é™ç»´
    - [å‹ç¼©ç‡è®¡ç®—](/algorithm/dimensional-reduction/#compression-rate)
- ç¥ç»ç½‘ç»œ
    - [æ„ŸçŸ¥æœºå­¦ä¹ è¿‡ç¨‹](/algorithm/neural-network/#learning-algorithm)
    - [å‰é¦ˆç¥ç»ç½‘ç»œå­¦ä¹ è¿‡ç¨‹](/algorithm/neural-network/fnn/#training-procedure)
    - [åå‘ä¼ æ’­ç®—æ³•](/algorithm/neural-network/fnn/#backpropagation-algorithm)
    - [åå‘ä¼ æ’­å…¬å¼æ¨å¯¼](/algorithm/neural-network/backpropagation)
    - [å·ç§¯è®¡ç®—](/algorithm/neural-network/cnn/#convolutional-layer)
- èšç±»
    - [K-meansèšç±»å¦‚ä½•åˆ†ç°‡](/algorithm/clustering/#k-means)
    - [GMMç®—æ³•å¦‚ä½•è¿›è¡Œåˆ†ç°‡](/algorithm/clustering/#gmm)
    - [èšåˆå¼ç®—æ³•å¦‚ä½•è¿›è¡Œåˆ†ç°‡](/algorithm/clustering/#agglomerative-algorithm)
    - [DBSCANç®—æ³•å¦‚ä½•åˆ†ç°‡](/algorithm/clustering/#dbscan)
    - [è®¡ç®—å‡èšåº¦/åˆ†ç¦»åº¦](/algorithm/clustering/#conhesion-separration)
- é©¬å°”å¯å¤«é“¾
    - [åˆ©ç”¨é©¬å°”ç§‘å¤«å‡è®¾è¿›è¡Œé¢„æµ‹](/algorithm/markov-chain/#markov-assumption)
    - [å‰å‘ç®—æ³•](/algorithm/markov-chain/#forward-algorithm)
    - [Viterbiç®—æ³•](/algorithm/markov-chain/#viterbi)
- å¼ºåŒ–å­¦ä¹ 
    - [Qå­¦ä¹ ç®—æ³•](/algorithm/reinforcement-learning/#q-algo)
    - [æ·±åº¦Qå­¦ä¹ ç®—æ³•](/algorithm/reinforcement-learning/#dql)

