---
title: 聚类
comments: true
---

## 定义

聚类, Clustering, 是将数据对象分割为组(也叫作簇)的过程. 同一个簇内的对象特征相似, 不同簇内的对象特征不相似. 最简单的相似性测量方法是测量距离. 对象之间距离小说明相似度高, 对象之间距离大说明相似度小. 它是一种无监督学习, 也就是说输入一个未标注的训练集和想要的簇的数量$k$, 输出$k$个簇. 一个好的聚类应该具备: 高内聚性(即簇内的高度相似性); 高分离度(即簇之间的低相似性).

## 应用

聚类可以用于:

- 作为独立的工具对数据进行分簇
- 作为其他算法的构建模块, 例如, 作为[降维](/算法/降维)的预处理工具
- ...

举几个例子把...

1. 根据购买历史, 浏览历史将顾客分为不同特征的群体, 利用这些信息开发针对性的营销活动 
2. 分析客户的行为, 找到可能丢失的客户群体, 例如转移到其他医疗保险, 电力或者电话公司
3. 找到具有相似结构和功能的基因, 使用微阵列从数千个基因🧬中同时分析
4. 基于文件的内容找到相似的其他文件, 如专利查询, 个性化新闻推荐
5. 了解特定群体(如年轻的亚洲人)的饮食习惯和饮食模式
6. 聚类像素点, 然后用几何中心的颜色替代达到图像压缩的效果
7. 基于不同的颜色将图像分割为不同的部分

## 测量

### 点的距离

数据点A和B之间的相似性是通过距离测量的. 距离测量的方式有很多种. 请参考[相似性测量](/算法/预处理/#相似性测量).

### 簇的距离

### 质心和中心

考虑一个含有$N$个点的簇$\{p_1, ..., p_N\}$. 质心, Centroid, 是簇的几何中心, 通常不是簇中的一个数据点. 而中心点, Medoid, 是簇中一个具有代表性的数据点, 其选取方式是: 找到簇中所有点到该点的距离和最小的那个点, 如[图](https://img.ricolxwz.io/e6cedeff0f7b2b51aa22fd01709d7a34.png)所示.

---

簇的距离可以通过多种方式衡量.

- 质心: 根据质心点之间的距离
- 中心: 根据中心点之间的距离
- 单链接: 根据簇A任意一个点和簇B任意一个点之间的最短距离
- 全链接: 根据簇A任意一个点和簇B任意一个点之间的最大距离
- 平均链接: 根据簇A所有点和簇B所有点之间的平均距离

## 分类

聚类算法主要分为四种.

- 划分式: Partional, 代表算法为K-menas, K-medoids. 通过划分数据集生成一个簇的集合, 每个簇都对应数据中的一个子集
- 模型式: Model-based, 代表算法为高斯混合模型(GMM). 假设数据式由不同的概率分布生成的, 使用该模型来估计这些分布并分配数据点
- 层次式: Hierarchical, 代表算法为聚合式(Agglomerative), 分裂式(Divisive). 构建嵌套的簇结构, 可以通过层次图展示, 层次聚类逐步合并或分裂数据, 创建不同层次的簇
- 密度式: Density-based, 代表算法为DBSCAN. 基于数据点的密度进行聚类, 能够识别出形状不规则的簇, 并能够检测出噪声点

## 划分式

### K-means

K均值聚类, K-means式一种非常流行且广泛使用的划分式聚类算法. 它通过将数据集划分为$k$个簇来进行聚类. K-means因为其简单性和计算效率被广泛应用于各种领域的数据聚类任务. K-means算法要求用户预先定义簇的数量$k$, 这是它的一个主要限制.

算法的步骤为:

1. 选择$k$个初始质心: 选择$k$个样本作为初始质心
2. 迭代步骤
    1. 将每个样本分配到最近质心: 将每个样本分配到距离最近的质心所属的簇中, 形成$k$个簇, 这个步骤通过最小化样本和质心的距离来完成, 通常使用欧几里得距离
    2. 重新计算质心: 每次迭代后, 重新计算每个簇的质心, 注意, 不一定是实际的数据点
3. 检查停止条件: 当质心不再发生变化, 则算法终止; 否则, 重复第2步, 用新的质心重新进行样本分配和质心计算

<figure markdown='1'>
![](https://img.ricolxwz.io/fd9096289dd73f230d6b032b260d8949.png){ loading=lazy width='600' }
</figure>

???+ note "细节"

    - 最初的质心通常是随机选取的, 而且选的是实际存在的点
    - 质心的选取会对结果产生严重影响, 详情请见[这里](#质心选取)
    - 绝大多数的收敛发生在前几个轮次
    - 通常情况下, 停止的条件是"直到较少的质心发生变化"而不是"没有质心发生变化"
    - 复杂度为$O(n*k*i*d)$, 其中$n$为数据点的数量, $k$是簇的数量, $i$是迭代的次数, $d$是属性的数量

???+ example "例子"

    下表是五个数据点之间的距离.

    |   | A | B | C | D | E |
    |---|---|---|---|---|---|
    | **A** | 0 | 2 | 7 | 10| 1 |
    | **B** | 2 | 0 | 3 | 4 | 6 |
    | **C** | 7 | 3 | 0 | 5 | 9 |
    | **D** | 10| 4 | 5 | 0 | 1 |
    | **E** | 1 | 6 | 9 | 1 | 0 |

    现在, 使用K-means算法将其聚类为$2$个簇. 初始的质心为A和B. 展示第一轮之后的簇.

    - C和A之间的距离是$7$, C和B之间的距离是$3$, 所以C被分配到簇$2$
    - D和A之间的距离是$10$, D和B之间的距离是$4$, 所以D被分配到簇$2$
    - E和A之间的距离是$1$, E和B之间的距离是$6$, 所以E被分配到簇$1$

#### 质心选取 {#质心选取}

算法对于初始的质心很敏感, 不同的初始质心可能会产生不同的簇.

???+ example "例子"

    === "好的初始质心"

        <figure markdown='1'>
        ![](https://img.ricolxwz.io/713548f4d06b34edda13ffe7aaaddd66.png){ loading=lazy width='500' }
        </figure>

    === "差的初始质心"

        <figure markdown='1'>
        ![](https://img.ricolxwz.io/c74b14d48e7e9e679ee96535450ff907.png){ loading=lazy width='500' }
        </figure>

选取初始质心的方法大致有两种:

1. 运行数次初始质心不同的K-means算法, 然后找到Sum of Squared Error(SSE)最小的初始质心. 什么是SSE? 对于每一个点来说, 它的误差是到最近质心的距离, SSE就是这些距离的总和. 也就是$SSE=\sum_{i=1}^k\sum_{x\in k_i}d(c_i, x)^2$, 其中$c_i$是质心, $k$是簇的数量, $x$是数据点.
2. 使用K-means++算法

##### K-means++ {#kmeans++}

K-means++是一种K-means算法的变种, 它使用不同的方法选取初始质心. 剩余的和标准的K-means算法是相同的. 它的初始质心选取方法为: 

质心是逐个选择的, 直到选出$k$个质心为止. 每一步中, 每个数据点都有一定的概率被选为质心, 该概率与该点到当前与其最近的质心的距离平方成正比, 这意味着离现有质心最远的点更有可能被选中, 这样能够确保质心分布良好, 互相分隔. 

该方法在实际使用中能够显著改进初始质心的选择, 从而提高K-means算法的整体效果. BTW, 在[COMP5045(Computational Geometry)](https://www.sydney.edu.au/units/COMP5045/2024-S1C-ND-CC)中, 其中第11周讲到的Approximation Algorithms使用的也是这个思想, 那里叫"K-Clustering"算法, 过程是一样的.

#### 问题

##### 空簇

K-means会产生空簇, 也就是没有点会被分配到一个簇中, 只包含一个初始质心. 解决方法有:

- 选取离当前质心最远的点 
- 使用[K-means++算法](#kmeans++)
- 从具有最高SSE的簇中选取一个新的质心来分裂该簇

如果有多个空簇出现, 可以重复上述步骤多次, 直至所有的簇都有点被分配.

##### 离群

离群点, OUtliers, 是指在数据集中明显偏离其他数据点的样本点. 它们与大多数数据点的分布有很大的差异, 可能在空间中距离其他数据点较远. 当数据集中存在离群点的时候, 计算得到的质心往往不够代表性, 并且会导致SSE增加. 尤其是在多次聚类运行过程中, 离群点的影响会比较明显.

一种常见的解决方法是在进行聚类之前移除掉离群点. 但是对于某些应用场景, 离群点非常重要, 移除它们可能导致信息丢失. 例如, 数据压缩, 金融分析. 另一种方法是在聚类完成之后再移除这些离群点: 通过跟踪每个点对SSE的贡献, 特别是对贡献异常高的数据点, 可以移除. 