---
title: 卷积神经网络
comments: true
---

卷积神经网络, Convolutional Neural Networks, CNN, 是一种特殊类型的多层神经网络, 利用反向传播算法进行训练. 设计目标是直接从图像的像素中识别视觉部分, 而不需要做预处理, 如OCR. CNN对图像的扭曲和几何变化如平移有很强的鲁棒性, 在图像和声音设别中表现出色, 尤其是在手写数字识别, 面部检测和图像分类等任务上可以取得优异成绩.

## 历史

1989年, Yan LeCun等人首次提出了卷积神经网络的概念, 这是CNN的最早发展阶段, 主要用于手写字符识别, OCR. 2012年, AlexNet(由Alex Krizhe等人提出的深度卷积神经网络)在ImageNet竞赛中获取了胜利, 标志着深度学习和CNN的重大突破. AlexNet的成功显著降低了错误率, 从$26\%$降至$15\%$, 使CNN称为计算机视觉领域的主流方法.

## 优势

CNN主要用于图像的识别(也可以用于其他形式的数据). 图像由矩阵描述, 对于8位灰白图, 每个像素的取值范围是$0$到$255$, 每张图对应于一个矩阵; 对于8位彩色图, 有三种通道, 红色, 绿色和蓝色, 每个通道的的取值范围是$0$到$255$, 每个通道对应于一个矩阵, 所以每张图对应于三个矩阵.

<figure markdown='1'>
![](https://img.ricolxwz.io/990dc043f998e4a6c1094e3727596d1e.png){ loading=lazy width='500' }
</figure>

传统的神经网络, 如多层前馈神经网络无法考虑图像的空间结构, 如下图显示传统神经网络会将图像的像素展平为一列, 直接作为网络的输入. 这种方式忽略了像素之间的空间关系, 导致网络难以捕捉到图像中的重要特征, 例如形状, 边缘等.

<figure markdown='1'>
![](https://img.ricolxwz.io/a06f3ddfec3526cfed0d4b4afd1328f0.png){ loading=lazy width='500' }
</figure>

CNN通过卷积层和池化层等新类型的层来客服这个问题. 卷积层能够处理图像的空间信息, 保持像素之间的相对位置; 池化层则帮助减少计算量, 同时保留关键信息, 进一步提高模型的泛化能力.

传统的神经网络还对图像中的物体的位置特别敏感, 如下图. 

<figure markdown='1'>
![](https://img.ricolxwz.io/ae36687da8c2bc8faf23ccb977195fcb.png){ loading=lazy width='500' }
</figure>

图中的字母P只是向左平移了$2$个像素, 但是输入却发生了很大的改变, 这会导致已经训练得到的模型在平移之后的图像上表现很差, CNN对这种平移或转换有很强的鲁棒性.

## 架构 

``` mermaid
graph LR
    A[输入层] ---> B[卷积层]
    subgraph 重复这一过程
        B --> C[RELU激活函数]
        C --> D[池化层]
    end
    D --> E[全连接层]
```

- 输入层: 接受像素值作为输入
- 卷积层: CONV, 用于从输入中提取局部特征, 通过卷积核对图像进行滑动计算, 保留纹理等特征
- RELU激活函数: 引入非线性因素, 帮助模型学习更加复杂的特征
- 池化层: POOL, 用于降低特征图的维度, 同时保留关键信息, 提高泛化能力
- 全连接层: FC, 也被称为密集层, 每个神经元都与前一层的所有神经元相连

如下图所示, $3$个重复的CONV-RELU-POOL和$2$个全连接层.

<figure markdown='1'>
![](https://img.ricolxwz.io/b7804c495021a955d6ad5a6d7414a840.png){ loading=lazy width='500' }
</figure>

CNN和全连接神经网络非常相似, 都由具有可学习权重的神经元组成. 每个神经元会接受一些输入, 然后执行点积运算, 之后再通过一个非线性的激活函数. 从原始的图像像素到分类得分, 整个网络可以被表示为一个可微分的函数, 使用反向传播算法, 逐步调整网络中的权重和截距以最小化误差. 

## 卷积层

卷积类似于在矩阵上应用一个滑动窗口, 对应的元素(滤波器/卷积核的权重和图像的像素值)进行相乘并求和. 以一个黑白图片作为例子, 其中像素值为$1$或者$0$. 这里使用了一个$3\times 3$的卷积核, 它从左到右, 从上到下在图像上滑动, 在每一步都执行卷积操作. 卷积的结果称为"特征图"或者"激活图". 在上图中, 特征图的第一个值为$4$, 计算过程为$1\times 1 + 1\times 0 + 1\times 1 + 0\times 0 + 1\times 1 + 1\times 0 + 0\times 1 + 0\times 0 + 1\times 1 = 4$.

<figure markdown='1'>
![](https://img.ricolxwz.io/6428cf505ac1e9e1cf462e1ec8fe9a68.gif){ loading=lazy width='300' }
</figure>

我们可以将卷积核看作是一个特征发现器. 如下图是一个发现一条曲线的卷积核.

<figure markdown='1'>
![](https://img.ricolxwz.io/89e8c73e0a019bc95d1a00d3bc6c7024.png){ loading=lazy width='400' }
</figure>

现在, 我们将这个卷积核用于一张我们想要分类图片(一张老鼠🐭的简笔画)的左上角上, 

<figure markdown='1'>
![](https://img.ricolxwz.io/b74c44bf131469e142856c3840828728.png){ loading=lazy width='500' }
</figure>

在这个区域进行卷积计算得到的值为$(50\times 30)+(50\times 30)+(50\times 30)+(20\times 30)+(50\times 30)=6600$, 这个值非常大! 说明这边很可能有一根类似的曲线.

我们尝试将卷积核移到图片的另一个地方, 老鼠的耳朵上.

<figure markdown='1'>
![](https://img.ricolxwz.io/6afdf8649d233e15c4c79991eaf53d25.png){ loading=lazy width='550' }
</figure>

在这个区域进行卷积计算计算得到的值为$0$. 这说明在图上没有任何和曲线卷积核相同的地方.

所以说, 特征图会显示在图像中有曲线的部分, 较大的值就意味着有一根曲线"激活"了卷积核; 较小的值就意味着图像中没有曲线. 每个卷积层都可能有多个卷积核, 每个卷积核都会产生一个特征图, 如果某个卷积层有$n$个卷积核, 卷积操作后, 这个卷积层将输出$n$个不同的特征图, 每个特征图代表卷积核检测到的特定模式或特征, 比如, 某些卷积核可能检测出边缘, 某些卷积核可能检测到圆形或者线条.

需要注意的是, 和上面例子不同的是, 在现实中, 卷积核是神经网络自动学习的, 它们的值不是预先设计好用于检验某种特定的特征, 而是通过训练数据自动进行学习的, 这里的关键就是反向传播算法的作用, 它使得卷积核可以通过训练不断更新, 从而适应不同的任务需求.

### 步幅

在之前我们的例子中, 卷积核每次只右移动一个单位. 当它到达最右边的时候, 会下移动一格然后移动到最左边. 我们称这种情况下水平步幅和垂直步幅都是$1$, 即$s_h=1, s_v=1$. 

<figure markdown='1'>
![](https://img.ricolxwz.io/0fc4e5268b9e6c471e5c29f62a353e90.png){ loading=lazy width='500' }
</figure>

越大的步幅会产生越小的特征图.

<figure markdown='1'>
![](https://img.ricolxwz.io/96700a8eb863fe63adef6d2617a6f336.png){ loading=lazy width='300' }
</figure>

### 填充

当我们将卷积核移动到行尾或者列尾的时候, 可能会"超出"原图的边界. 超出边界的那一部分我们可以用$0$来填充超出的部分. 填充的大小是一个超参数. 填充可以处理图像的边缘区域, 防止边缘区域信息丢失. 

<figure markdown='1'>
![](https://img.ricolxwz.io/e62b0910ddf7f9ed2b887dd5acc42d15.png){ loading=lazy width='500' }
</figure>

### 感受野[^1]

感受野, receptive field. 特征图上某个元素的计算受输入图像上的某个区域影响, 这个区域就是该元素的感受野. 在卷积神经网络中, 越深层的神经元看到的输入区域越大, 如下图所示, 卷积核的大小均为$3\times 3$, 步幅均为$1$. 第二层的每个神经元可以看到第一层的$3\times 3$大小的区域, 第三层的每个神经元可以看到第二层上$3\times 3$大小的区域, 又可以看到第一层上$5\times 5$大小的区域.

<figure markdown='1'>
![](https://img.ricolxwz.io/94958fe3ed2ac69fc5b4e3c91235d192.png){ loading=lazy width='250' }
</figure>

所以, 感受野是一个相对的概念, 某层的特征图上的元素看到前面不同层的区域范围是不同的, 通常在不特殊指定的情况下, 感受野指的是看到输入图像上的区域.

[^1]: 彻底搞懂感受野的含义与计算 - shine-lee - 博客园. (不详). 取读于 2024年9月18日, 从 https://www.cnblogs.com/shine-lee/p/12069176.html
