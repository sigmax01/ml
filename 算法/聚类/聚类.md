---
title: 聚类
comments: true
---

## 定义

聚类, Clustering, 是将数据对象分割为组(也叫作簇)的过程. 同一个簇内的对象特征相似, 不同簇内的对象特征不相似. 最简单的相似性测量方法是测量距离. 对象之间距离小说明相似度高, 对象之间距离大说明相似度小. 它是一种无监督学习, 也就是说输入一个未标注的训练集和想要的簇的数量$k$, 输出$k$个簇. 一个好的聚类应该具备: 高内聚性(即簇内的高度相似性); 高分离度(即簇之间的低相似性).

## 应用

聚类可以用于:

- 作为独立的工具对数据进行分簇
- 作为其他算法的构建模块, 例如, 作为[降维](/算法/降维)的预处理工具
- ...

举几个例子把...

1. 根据购买历史, 浏览历史将顾客分为不同特征的群体, 利用这些信息开发针对性的营销活动 
2. 分析客户的行为, 找到可能丢失的客户群体, 例如转移到其他医疗保险, 电力或者电话公司
3. 找到具有相似结构和功能的基因, 使用微阵列从数千个基因🧬中同时分析
4. 基于文件的内容找到相似的其他文件, 如专利查询, 个性化新闻推荐
5. 了解特定群体(如年轻的亚洲人)的饮食习惯和饮食模式
6. 聚类像素点, 然后用几何中心的颜色替代达到图像压缩的效果
7. 基于不同的颜色将图像分割为不同的部分

## 测量

### 点的距离

数据点A和B之间的相似性是通过距离测量的. 距离测量的方式有很多种. 请参考[相似性测量](/算法/预处理/#相似性测量).

### 簇的距离

### 质心和中心

考虑一个含有$N$个点的簇$\{p_1, ..., p_N\}$. 质心, Centroid, 是簇的几何中心, 通常不是簇中的一个数据点. 而中心点, Medoid, 是簇中一个具有代表性的数据点, 其选取方式是: 找到簇中所有点到该点的距离和最小的那个点, 如[图](https://img.ricolxwz.io/e6cedeff0f7b2b51aa22fd01709d7a34.png)所示.

---

簇的距离可以通过多种方式衡量.

- 质心: 根据质心点之间的距离
- 中心: 根据中心点之间的距离
- 单链接: 根据簇A任意一个点和簇B任意一个点之间的最短距离
- 全链接: 根据簇A任意一个点和簇B任意一个点之间的最大距离
- 平均链接: 根据簇A所有点和簇B所有点之间的平均距离

## 分类

聚类算法主要分为四种.

- 划分式: Partional, 代表算法为K-menas, K-medoids. 通过划分数据集生成一个簇的集合, 每个簇都对应数据中的一个子集
- 模型式: Model-based, 代表算法为高斯混合模型(GMM). 假设数据式由不同的概率分布生成的, 使用该模型来估计这些分布并分配数据点
- 层次式: Hierarchical, 代表算法为聚合式(Agglomerative), 分裂式(Divisive). 构建嵌套的簇结构, 可以通过层次图展示, 层次聚类逐步合并或分裂数据, 创建不同层次的簇
- 密度式: Density-based, 代表算法为DBSCAN. 基于数据点的密度进行聚类, 能够识别出形状不规则的簇, 并能够检测出噪声点

## 划分式

### K-means

K均值聚类, K-means式一种非常流行且广泛使用的划分式聚类算法. 它通过将数据集划分为$k$个簇来进行聚类. K-means因为其简单性和计算效率被广泛应用于各种领域的数据聚类任务. K-means算法要求用户预先定义簇的数量$k$, 这是它的一个主要限制.

算法的步骤为:

1. 选择$k$个初始质心: 选择$k$个样本作为初始质心
2. 迭代步骤
    1. 将每个样本分配到最近质心: 将每个样本分配到距离最近的质心所属的簇中, 形成$k$个簇, 这个步骤通过最小化样本和质心的距离来完成, 通常使用欧几里得距离
    2. 重新计算质心: 每次迭代后, 重新计算每个簇的质心, 注意, 不一定是实际的数据点
3. 检查停止条件: 当质心不再发生变化, 则算法终止; 否则, 重复第2步, 用新的质心重新进行样本分配和质心计算

<figure markdown='1'>
![](https://img.ricolxwz.io/fd9096289dd73f230d6b032b260d8949.png){ loading=lazy width='600' }
</figure>

???+ note "细节"

    - 最初的质心通常是随机选取的, 而且选的是实际存在的点
    - 质心的选取会对结果产生严重影响, 详情请见[这里](#质心选取)
    - 绝大多数的收敛发生在前几个轮次
    - 通常情况下, 停止的条件是"直到较少的质心发生变化"而不是"没有质心发生变化"
    - 复杂度为$O(n*k*i*d)$, 其中$n$为数据点的数量, $k$是簇的数量, $i$是迭代的次数, $d$是属性的数量

???+ example "例子"

    下表是五个数据点之间的距离.

    |   | A | B | C | D | E |
    |---|---|---|---|---|---|
    | **A** | 0 | 2 | 7 | 10| 1 |
    | **B** | 2 | 0 | 3 | 4 | 6 |
    | **C** | 7 | 3 | 0 | 5 | 9 |
    | **D** | 10| 4 | 5 | 0 | 1 |
    | **E** | 1 | 6 | 9 | 1 | 0 |

    现在, 使用K-means算法将其聚类为$2$个簇. 初始的质心为A和B. 展示第一轮之后的簇.

    - C和A之间的距离是$7$, C和B之间的距离是$3$, 所以C被分配到簇$2$
    - D和A之间的距离是$10$, D和B之间的距离是$4$, 所以D被分配到簇$2$
    - E和A之间的距离是$1$, E和B之间的距离是$6$, 所以E被分配到簇$1$

#### 质心选取 {#质心选取}

算法对于初始的质心很敏感, 不同的初始质心可能会产生不同的簇.


=== "好的初始质心"

    <figure markdown='1'>
    ![](https://img.ricolxwz.io/713548f4d06b34edda13ffe7aaaddd66.png){ loading=lazy width='500' }
    </figure>

=== "差的初始质心"

    <figure markdown='1'>
    ![](https://img.ricolxwz.io/c74b14d48e7e9e679ee96535450ff907.png){ loading=lazy width='500' }
    </figure>

选取初始质心的方法大致有两种:

1. 运行数次初始质心不同的K-means算法, 然后找到Sum of Squared Error(SSE)最小的初始质心. 什么是SSE? 对于每一个点来说, 它的误差是到最近质心的距离, SSE就是这些距离的总和. 也就是$SSE=\sum_{i=1}^k\sum_{x\in k_i}d(c_i, x)^2$, 其中$c_i$是质心, $k$是簇的数量, $x$是数据点.
2. 使用K-means++算法

##### K-means++ {#kmeans++}

K-means++是一种K-means算法的变种, 它使用不同的方法选取初始质心. 剩余的和标准的K-means算法是相同的. 它的初始质心选取方法为: 

质心是逐个选择的, 直到选出$k$个质心为止. 每一步中, 每个数据点都有一定的概率被选为质心, 该概率与该点到当前与其最近的质心的距离平方成正比, 这意味着离现有质心最远的点更有可能被选中, 这样能够确保质心分布良好, 互相分隔. 

该方法在实际使用中能够显著改进初始质心的选择, 从而提高K-means算法的整体效果. BTW, 在[COMP5045(Computational Geometry)](https://www.sydney.edu.au/units/COMP5045/2024-S1C-ND-CC)中, 其中第11周讲到的Approximation Algorithms使用的也是这个思想, 那里叫"K-Clustering"算法, 过程是一样的.

#### 问题

##### 空簇

K-means会产生空簇, 也就是没有点会被分配到一个簇中, 只包含一个初始质心. 解决方法有:

- 选取离当前质心最远的点 
- 使用[K-means++算法](#kmeans++)
- 从具有最高SSE的簇中选取一个新的质心来分裂该簇

如果有多个空簇出现, 可以重复上述步骤多次, 直至所有的簇都有点被分配.

##### 离群

离群点, OUtliers, 是指在数据集中明显偏离其他数据点的样本点. 它们与大多数数据点的分布有很大的差异, 可能在空间中距离其他数据点较远. 当数据集中存在离群点的时候, 计算得到的质心往往不够代表性, 并且会导致SSE增加. 尤其是在多次聚类运行过程中, 离群点的影响会比较明显.

一种常见的解决方法是在进行聚类之前移除掉离群点. 但是对于某些应用场景, 离群点非常重要, 移除它们可能导致信息丢失. 例如, 数据压缩, 金融分析. 另一种方法是在聚类完成之后再移除这些离群点: 通过跟踪每个点对SSE的贡献, 特别是对贡献异常高的数据点, 可以移除. 

#### 其他扩展算法

##### 二分K-means

二分K-means, Bisecting K-means, 是对标准K-means算法的一种扩展. 它会讲数据集分成两个簇, 然后选择其中的一个簇进一步拆分, 如此重复, 直到获得$k$个簇为止. 

1. 开始时所有的点都放在一个簇中
2. 重复以下过程
    1. 从当前的簇列表中, 选择一个簇用于拆分
    2. 对于指定的迭代次数, 使用K-means对选中的簇进行二分
    3. 将二分后的簇中SSE最低的两个簇添加到簇列表中
3. 终止条件: 当簇的数量达到$k$个时

有多种方式可以选择要拆分的簇:

- 选择最大的簇
- 选择SSE最大的簇
- 基于大小和SSE的综合指标


如图所示, 是一个使用二分K-means算法聚类的过程.

<figure markdown='1'>
![](https://img.ricolxwz.io/2acfb8b14da20e49c79e2c88bfc5fe28.png){ loading=lazy width='500' }
</figure>

#### 缺陷

K-means算法在以球形分类, 同等大小, 分裂明显的原始数据上的表现非常好. 但是在非球形分类的, 复杂的, 大小不一致, 密度不一致的原始数据上表现不佳.

=== "非球形分类的原始数据"

    <figure markdown='1'>
    ![](https://img.ricolxwz.io/678f1837a8e2d8c0a18d30fd615ce1a6.png){ loading=lazy width='500' }
    </figure>

=== "大小不一致的原始数据"

    K-means算法只会讲最大的原始簇分开.

    <figure markdown='1'>
    ![](https://img.ricolxwz.io/813c5603544934b740c39a96095e238c.png){ loading=lazy width='500' }
    </figure>

=== "密度不一致的原始数据"

    K-means算法只会分裂最大的原始簇.

    <figure markdown='1'>
    ![](https://img.ricolxwz.io/8cc3ff2b958c8b9fac726eeed41c80fa.png){ loading=lazy width='500' }
    </figure>

## 模型式

### GMM

高斯混合模型, Gaussian Mixture Model, 可以看作是对单一高斯分布的扩展, 它假设数据是由多个高斯分布混合而成的, 每个分布在模型中具有一定的权重. 

对于一个单一的高斯分布, 它的概率密度函数为$f(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$, 其中$\mu$是均值, $\sigma$是标准差. 我们假设数据是由$k$个高斯分布混合而成的, 每一个分布都有自己的均值和标准差, 一个高斯分布对应的是一个簇. 我们并不知道这些高斯分布的均值和标准差, 我们的目标是通过观察到的数据来估计这些参数. 在每一轮的估计结束后, 我们会重新计算每个数据点属于每个簇的概率, 然后重新估计参数, 如此循环直至收敛. 算法可以表示为:

1. 初始化参数: 随机初始化每个高斯分布的均值和标准差, 以及每个高斯分布的权重
2. 重复以下步骤:
    1. E步: 计算每个数据点属于每个簇的概率, 也就是每个高斯分布的概率
    2. M步: 重新估计每个高斯分布的均值, 标准差和权重
3. 检查停止条件: 当参数不再发生变化, 或者达到最大迭代次数时, 算法终止

其中, E步指的是Expectation Step, M步指的是Maximization Step. E步相当于K-means算法中的分配数据点到簇的过程, M步相当于重新计算质心的过程.

???+ example "例子"

    假设有$20000$个数据点, 从两个高斯分布构成, 分布1和分布2.
   
    <figure markdown='1'>
    ![](https://img.ricolxwz.io/44e79b3181b5e130d898eac1dea29026.png){ loading=lazy width='500' }
    </figure>

    为了简化起见, 假设我们已经知道了它们的标准差$\sigma_1$和$\sigma_2$, $\sigma_1=\sigma_2=2$.

    1. 第一步, 初始化每个高斯分布的均值和标准差, 由于我们已经知道标准差了, 所以只需要初始化均值. 随机初始化均值为$\mu_1=-2, \mu_2=3$. 那么初始状态下两个分布的参数为$\theta_1=(-2, 2), \theta_2=(3, 2)$. 整体参数$\theta=(\theta_1, \theta_2)$
    2. E步, 计算每一个数据点属于分布$j(j=1, 2)$的概率. $p(distribution\ j|x_i, \theta)=\frac{w_jp(x_i|\theta_j)}{w_1p(x_i|\theta_1)+w_2p(x_i|\theta_2)}$. 其中, $w_j$是每一个分布的权重(也就是分布$j$产生数据点的概率), 所有的权重加起来应该等于$1$, 即$w_1+w_2=1$. 假设$w_1=w_2=0.5$, 那么$p(distribution\ j|x_i, \theta)=\frac{0.5p(x_i|\theta_j)}{0.5p(x_i|\theta_1)+0.5p(x_i|\theta_2)}$, 对于数据点$x_i=0$, 根据概率密度分布函数有$p(x_i|\theta_1)=0.12, p(x_i|\theta_2)=0.06$, 那么$p(distribution\ 1|x=0, \theta)=\frac{0.12}{0.12+0.06}=0.66, p(distribution\ 2|x=0, \theta)=\frac{0.06}{0.12+0.06}=0.33$, $20000$个数据点都要算一遍
    3. M步, 重新估计每一个高斯分布的均值. $\mu_1=\sum_{i=1}^n x_i\frac{p(distribution\ 1|x_i, \theta)}{\sum_{i=1}^n p(distribution\ 1|x_i, \theta)}$, $\mu_2=\sum_{i=1}^n x_i\frac{p(distribution\ 2|x_i, \theta)}{\sum_{i=1}^n p(distribution\ 2|x_i, \theta)}$. 可以看到, 新的均值是基于数据点的加权平均值计算的. 权重由每个数据点属于特定分布的概率决定
    4. 迭代和收敛, 重复步骤2和步骤3, 直到$\mu_1$和$\mu_2$的不再产生变化或变化非常小, 数据点最终分配给概率更高的分布

#### GMM vs. K-means

GMM其实是K-means算法的推广. K-means算法假设每个簇是圆形的, 而GMM则放宽了这个假设, 允许簇的形状为椭圆形, 因此更加灵活. 

<figure markdown='1'>
![](https://img.ricolxwz.io/9927e4cc93c42ff8a27e38e94dee29f7.png){ loading=lazy width='500' }
</figure>