---
title: 前馈神经网络
comments: true
---

## 神经元

神经网络由神经元(单元, 节点)组成, 这些神经元通过有向链接互相连接, 每个连接都有一个相关的数值权重. 神经元会组成层状结构, 包括输入层, 输出层或多个隐藏层. 在训练的过程中, 权重会被调整, 以学习执行一个特定的任务. 

在生物神经网络中, 每个神经元与其他神经元相连, 当它兴奋的时候, 就会向相连的神经元发送化学物质, 从而改变这些神经元内的电位, 如果某神经元的电位超过一个阈值, 那么它就会被激活, 即兴奋起来, 向其他神经元发送化学物质. 机器学习中的神经元也类似, 神经元接受到其他$n$个神经元传递过来的输入信号, 这些输入信号通过带权重的连接进行传递, 神经元接收到总输入将与神经元的阈值进行比较, 然后通过"激活函数"处理以产生神经元的输出.

1943年, McMulloch和Pittes将上述情景抽象, 这就是一直沿用至今的"M-P"神经元模型:

<figure markdown='1'>
![](https://img.ricolxwz.io/9e2e01f533f1d459d48b80f753580c0c.png){ loading=lazy width='500' }
</figure> 

每一个神经元包含:

- 输入向量$\bm{x}$, 表示输入数据, 其中每个元素$x_1, x_2, ..., x_n$, 对应$n$个特征
- 权重向量$\bm{w}$: 每个输入都有一个对应的权重$w_1, w_2, ..., w_n$, 对应$n$个参数
- 偏置$b$: 是一个常数, 在将输入加权求和后添加, 用于平移激活曲线
- 求和$\sum$: 输入$x_1, x_2, ..., x_n$和权重$w_1, w_2, ..., w_n$相乘相加, 加上偏置值, 结果表达式为$\bm{w}\bm{x}+b$
- 传递函数$f$: 又叫激活函数, 是将求和结果进行映射并生成输出, 其中$a=f(\bm{w}\bm{x}+b)$ 
- 输出$a$: 对加权和及偏置应用激活函数后产生的最终结果

<figure markdown='1'>
![](https://img.ricolxwz.io/f2b529bb036ac241e7d09acd263e4c90.png){ loading=lazy width='500' }
</figure>

理想的激活函数应该是阶跃函数(如上左图所示), 它将输入值映射为输出值为$0$或$1$, $1$对应于神经元兴奋, $0$对应于神经元抑制. 但是阶跃函数具有不连续, 不光滑等不太好的特性, 因此实际常用sigmoid函数作为激活函数(如上右图所示).

## 轮, 批, 迭代

轮, 批, 迭代是神经网络中最基础的三个概念.

- 轮, epoch, 是指使用训练集的全部数据对模型进行一次完整训练, 称之为"一代训练"
- 批, batch, 使用训练集中的一小部分样本对模型权重进行一次反向传播的参数更新, 这一小部分样本被称为"一批数据"
- 迭代, iteration, 使用一个批batch数据对模型尽心关一次参数更新的过程, 被称之为"一次训练"
- 批大小, 在训练集中选择的一组用来更新权值的样本的数量, 通常设为$2$的$n$次幂
- 迭代次数, 迭代次数等于总的样本数量除以批大小

???+ example "例子"

    假设有一个数据集, 已经被分为5个批次, batch1, batch2, batch3, batch4, batch5. 接下来要使用这些批次来训练模型.

    1. 批次1, 使用批次1进行一次正向传播, 进行一次反向传播, 更新权重, 完成了一次迭代
    2. 批次2, 使用批次2进行一次正向传播, 进行一次反向传播, 更新权重, 完成了一次迭代
    3. 批次3, 使用批次3进行一次正向传播, 进行一次反向传播, 更新权重, 完成了一次迭代
    4. 批次4, 使用批次4进行一次正向传播, 进行一次反向传播, 更新权重, 完成了一次迭代
    5. 批次5, 使用批次5进行一次正向传播, 进行一次反向传播, 更新权重, 完成了一次迭代

    批次5结束后, 就完成了1个轮次, 如果要继续进行训练, 你可以再进行下一轮, 即再遍历所有5个轮次


## 感知机

感知机, Perceptron, 它是最简单的神经网络, 由两层神经元组成, 输入层接受外界输入的信号后传递给输出层, 输出层是M-P神经元, 也被称为"阈值逻辑单元", 激活函数为阶跃函数.

<figure markdown='1'>
![](https://img.ricolxwz.io/c4090b80e90cb85ba787e93a035cb384.png){ loading=lazy width='300' }
</figure>

它由Frank Rosenblatt在1957年提出. 同时它的局限性由Marvin Minsky和Seymour Papert在书<Perceptrons>中提出. Rosenblatt和他的同事意识到这个局限可以通过使用更复杂的NNs, 即multi-layer perceptrons解决, 但是他们没有将感知机使用于神经网络的训练.

### 学习算法

那么, 感知机是如何进行学习的呢?

之前我们说过, 神经元会在训练过程中调整权重$w_1, w_2$, 这个其实就是感知机学习的过程. 权重的更新公式为$\bm{w}^{new}=\bm{w}^{old}+e\bm{x}^T$, $e=t-a$, $t$为目标输出($0$或$1$), $a$为实际输出($0$或$1$), $\bm{x}$为输入向量; 同时还要调整截距, $b^{new}=b^{old}+e$

- 当$e=1$的时候, $\bm{w}^{new}=\bm{w}^{old}+\bm{x}^T$, $b^{new}=b^{old}+1$
- 当$e=-1$的时候, $\bm{w}^{new}=\bm{w}^{old}-\bm{x}^T$, $b^{new}=b^{old}-1$
- 当$e=0$的时候, $\bm{w}^{new}=\bm{w}^{old}$, $b^{new}=b^{old}$

具体算法如下:

1. 初始化权重和截距$\bm{w}$, $b$为小的随机数, 设置当前轮次为$1$
2. 对于每一个训练样本$\{\bm{x}, t\}$
    1. 计算$a$, 这一步又叫作网络激活
    2. 计算误差$e=t-a$
    3. 更新权重和截距: $\bm{w}^{new}=\bm{w}^{old}+e\bm{x}^T$, $b^{new}=b^{old}+e$
3. 在每论结束时(即循环一遍所有的训练样本)检查是否满足停止条件, 如果所有的样本都被正确分类, 或者训练次数达到最大轮次数, 则停止训练, 否则继续, 执行第2步

???+ tip "Tip"

    感知机学习算法中批大小通常等于全部的样本数量, 所以在感知机学习算法中, 一轮=一次迭代.

???+ example "例子"

    给出下列的训练样本:

    |序号|特征(输入)|标签(输出)|
    |-|-|-|
    |1|1 0 0|0|
    |2|1 0 1|1|
    |3|1 1 0|0|

    初始权重为$\bm{w}=[0.3\ 0.2\ 0.4]$, 初始截距为$b=0.1$. 最大轮次数为$5$.

    === "第一个样本 ---->"

        - $a=sign([0.3\ 0.2\ 0.4][1\ 0\ 0]+0.1)=sign(0.4)=1$, 错误, $e=0-1=-1$
        - $\bm{w}^{new}=[0.3\ 0.2\ 0.4] + (-1)[1\ 0\ 0]=[-0.7\ 0.2\ 0.4]$
        - $b^{new}=0.1+(-1)=-0.9$

    === "第二个样本 ---->"

        - $a=sign([-0.7\ 0.2\ 0.4][1\ 0\ 1]-0.9)=sign(-1.2)=0$, 错误, $e=1-0=1$
        - $\bm{w}^{new}=[-0.7\ 0.2\ 0.4] + (1)[1\ 0\ 1]=[0.3\ 0.2\ 1.4]$
        - $b^{new}=-0.9+1=0.1$

    === "第三个样本"

        - $a=sign([0.3\ 0.2\ 1.4][1\ 1\ 1]+0.1)=sign(0.6)=1$, 错误, $e=0-1=-1$
        - $\bm{w}^{new}=[0.3\ 0.2\ 1.4] + (-1)[1\ 1\ 1]=[-0.7\ -0.8\ 1.4]$
        - $b^{new}=0.1-1=-0.9$

    第一轮结束后的权重$\bm{w}=[-0.7\ -0.8\ -1.4]$, 截距$b=-0.9$, 检查:

    1. 所有的样本都被正确分类? 
        1. 第一个样本: $a=sign([-0.7\ -0.8\ 1.4][1\ 0\ 0]-0.9)=sign(-1.6)=0$, 正确✅
        2. 第二个样本: $a=sign([-0.7\ -0.8\ 1.4][1\ 0\ 1]-0.9)=sign(-0.2)=0$, 错误❎
        3. 第三个样本: 无需检查第三个样本, 因为第二个样本已经错误
    2. 达到最大轮次数? 🈚️

    进入第二轮...


### 限制条件

如果训练样本是线性可分的, 即样本可以用一条直线在超平面(1)中分开, 感知机学习算法保证能够在有限步骤内找到一组权重和截距, 这组权重和截距能正确地将所有的训练样本分类. 这时候, 感知机将会找到一个线性的决策边界, 但不一定是"最优"的边界, 而是找到一个可行的边界后就会停止. 在现实世界中, 大多数的问题都是线性不可分的, 这意味着训练样本无法用一个超平面正确分开, 这也是感知机的最大局限之一.
{.annotate}

1. 什么是超平面, 可以在[这里](/算法/支持向量机#边际最大超平面)找到.

### 逻辑门

=== "与门"

    感知机能够实现与门, 这是一个线性可分的问题, 如图所示.

    <figure markdown='1'>
    ![](https://img.ricolxwz.io/46ab58645bfbd9676237b0a659b777b2.png){ loading=lazy width='300' }
    </figure>

    感知机能够找到一个线性的决策边界, 如$w_1=1=w_2=1, b=2$, 即$y=sign(x_1+x_2-2)$.

=== "或门"

    感知机能够实现或门, 这是一个线性可分的问题, 如图所示.

    <figure markdown='1'>
    ![](https://img.ricolxwz.io/b08609a89b6033ac9fae3be1f27257a3.png){ loading=lazy width='300' }
    </figure>

    感知机能够找到一个线性的决策边界, 如$w_1=w_2=1, b=0.5$, 即$y=sign(x_1+x_2-0.5).$

=== "与非门"

    感知机能够实现与非门, 这是一个线性可分的问题, 如图所示.

    <figure markdown='1'>
    ![](https://img.ricolxwz.io/4abb87c91ddaecb717517d3b804c5dc1.png){ loading=lazy width='300' }
    </figure>

=== "异或门"

    感知机无法实现异或门, 这不是一个线性可分的问题, 如图所示.
    <figure markdown='1'>
    ![](https://img.ricolxwz.io/0a17bba3cfe79eb90d6be35c888c0e0d.png){ loading=lazy width='300' }
    </figure>

    然而, 异或门可以通过与门, 非门和与非门的组合实现, 如图所示.

    <figure markdown='1'>
    ![](https://img.ricolxwz.io/363ac96f910fd04b3f83b13284af5a29.png){ loading=lazy width='300' }
    </figure>

    所以, 只要使用一个两层的感知机就能解决异或问题.

    <figure markdown='1'>
    ![](https://img.ricolxwz.io/710cecd9811caa570be36bda6616991c.png){ loading=lazy width='300' }
    </figure>

---

从上面的实验中, 我们得出结论, 如果增加更多的层, 可以得到更加复杂的决策边界, 如图所示.

<figure markdown='1'>
![](https://img.ricolxwz.io/7018b43e284156ad9fbd3b1c3e4d240f.png){ loading=lazy width='800' }
</figure>

## 前馈神经网络

### 架构

前馈神经网络, Feedforward NN, 的架构如图所示.

<figure markdown='1'>
![](https://img.ricolxwz.io/182181d298a387078d650701b708d254.png){ loading=lazy width='500' }
</figure>

具体说明如下:

- 输入层: 位于网络最底层, 输入变量$x_1, x_2, ..., x_n$表示输入特征. 每个输入节点代表一个特征, 输入层只复杂将数据传递到下一层(隐藏层), 不执行任何计算
- 隐藏层: 网络中间的层. 每个隐藏层的神经元接受来自上一层的加权求和输入, 并加上一个偏置项$b_m$, 然后通过激活函数$f$进行线性变换, 公式为$o_m=f(z_m)=f(\sum_{i=1}^n w_{im}x_i+b_m)$
- 输出层: 位于网络最顶层, 输出结果$o_k$是隐藏层经过类似的加权和计算后的输出, 公式为$o_k=f(z_k)=f(\sum_{i=1}^m w_{wk}o_i+b_k)$

除此之外, FNN还有两个非常重要的特征:

- 每一个神经元只接受前一个层的输出
- 当前层的每一个神经元都与前一个层的所有神经元相连
- 每个神经元的输入在完成加权和计算后, 会通过一个激活函数, 这个激活函数不局限于阶跃函数, 可以是ReLu, Sigmoid, Tanh函数, 最常用的是Sigmoid函数. 特别注意, 这个激活函数要可微

### 反向传播算法

对于每一个训练样本$\{\bm{x}, t\}$, $\bm{x}=\{x_1, x_2, ..., x_n\}$, $t$为其标签. 将其传入网络, 直到输出层, 这个过程称为正向传播, 将其输出$o$与标签$t$进行比较, 计算误差, 根据误差, 从输出层到输入层逐级反向传播, 调整每个神经元的权重, 以减小误差, 这个过程就是反向传播. 权重的更新公式为$w_{pq}^{new}=w_{pq}^{old}+\Delta w_{pq}$. 这种过程会被不断重复, 即正向->反向->正向->反向, 直到输出层输出结果的误差在可以接受的范围内. "正向->反向"这样的一次更新就成为一次迭代, 或一个批次.

那么, 我们怎么计算这个权重变化$\Delta w_{pq}$的呢? 参考线性回归, 我们可以定义一个误差损失函数然后使用梯度下降算法解决. 如均方误差函数, MSE. 每一次迭代/每一个批次对应于下山的"一步", 在山上的每一个位置对应于一组权重配置, 梯度的方向是误差增加最快的方向, 沿着负梯度的方向移动则可以使误差减小. 目标是通过调整权重, 使模型"下坡", 最终达到地形的最低点, 这样误差最小, 模型性能最佳. 用来下山的步子被称为"学习率", 这是算法的一个超参数.

<figure markdown='1'>
![](https://img.ricolxwz.io/4392978da300d80f0485a0aa396966ff.png){ loading=lazy width='500' }
</figure>

要注意的是, 梯度下降算法并不保证能找到全局最小值, 它只会找到基于起点的最近的局部最小值.

<figure markdown='1'>
![](https://img.ricolxwz.io/21ad32b9aa5ba67e5b0f6abc71d55d08.png){ loading=lazy width='500' }
</figure>

假设 $w_{pq}(t)$表示的是从神经元$p$到神经元$q$在$t$这个时间的权重, 那么下一次在$t+1$这个时间的权重为$w_{pq}(t+1)=w_{pq}(t)+\Delta w_{pq}$, 其中$\Delta w_{pq}=\eta\cdot \delta_q\cdot o_p$, 即权重变化与神经元$p$在激活函数激活后的输出$o_p$, 神经元$q$的误差$\delta_q$成正比.

神经元$q$的误差$\delta_q$要分两种情况计算:

1. 若$q$是输出层的神经元, 则$\delta_q=(t_q-o_q)f'(z_q)$, 见[图](https://img.ricolxwz.io/5f941efe6d1e0e2a24f4cc02e2b5f50c.png)
2. 若$q$是隐藏层的神经元, 则$\delta_q=f'(z_q)\sum_i w_{qi}\delta_i$, 见[图](https://img.ricolxwz.io/240c3a21ce4f4009099695a2a1c28f42.png)

注意, $i$是$q$后面的神经元, 即顺序为$p\rightarrow q\rightarrow i$; 这里的$z_q$是在激活函数激活前的输出, $o_q=f(z_q)$. 可以被证明$f'(z_q)=f(z_q)(1-f(z_q))$(前提是使用sigmoid激活函数), 所以有$f'(z_q)=o_q(1-o_q)$. 上面的误差计算公式可以写为:

1. 若$q$为输出层神经元, 则$\delta_q=(t_q-o_q)o_q(1-o_q)$
2. 若$q$为隐藏层神经元, 则$\delta_q=o_q(1-o_q)\sum_i w_{qi}\delta_i$

注意, $i$是$q$后面的神经元, 即顺序为$p\rightarrow q\rightarrow i$.

???+ tip "Tip"

    反向传播公式推导请见[这里](/算法/神经网络/反向传播公式推导).

#### 训练过程

1. 初始化所有的权重和截距为较小的随机数
2. 重复循环, 直到停止条件被满足
    1. 正向传播: 计算神经网络的输出
    2. 反向传播
        1. 计算输出层神经元的误差$\delta$, 更新输出层的权重和截距
        2. 从输出层开始, 对于神经网络的每一层, 重复循环, 直到输入层
            1. 反向传播$\delta$
            2. 更新两层之间的权重
3. 检查是否满足条件, 若训练集的误差已经低于某一个值或已经达到了最大轮次, 则停止循环

???+ example "例子"

    假设学习率为$0.9$.

    === "1--->"

        <figure markdown='1'>
        ![](https://img.ricolxwz.io/efbaeae02cd8777f9b80255112fe680c.png){ loading=lazy width='550' }
        </figure>

    === "2--->"

        <figure markdown='1'>
        ![](https://img.ricolxwz.io/c5859d33d92cbffd51fa2b05db6f50e3.png){ loading=lazy width='650' }
        </figure>

    === "3--->"

        <figure markdown='1'>
        ![](https://img.ricolxwz.io/4fbfa4681dde83cc2983beabf18b2b5c.png){ loading=lazy width='650' }
        </figure>

    === "4--->"
        
        <figure markdown='1'>
        ![](https://img.ricolxwz.io/ad1fbdd3eb510d21579643d0c982ea49.png){ loading=lazy width='650' }
        </figure>

    === "5--->"

        <figure markdown='1'>
        ![](https://img.ricolxwz.io/478525aca15db7ed94df36a9839ac82d.png){ loading=lazy width='650' }
        </figure>

    === "6--->"

        <figure markdown='1'>
        ![](https://img.ricolxwz.io/e053763d1f673ce6b687928f85addb82.png){ loading=lazy width='650' }
        </figure>

    === "7"

        <figure markdown='1'>
        ![](https://img.ricolxwz.io/84b6a3083f59e94953f1dff056775fd7.png){ loading=lazy width='650' }
        </figure>

#### 其他梯度下降算法

标准的梯度下降算法计算出$\sum_i w_{qi}\delta_i$, 所有的误差之后再更新权重$\delta_q$, 而随机梯度下降算法随机选一个$i$层神经元来计算$\delta_q$. 而小批量梯度下降是选取一小部分$i$层神经元来计算$\delta_q$.

常用的优化算法总结:

- 标准梯度下降, standard gradient descent
- 随机梯度下降, SGD, stochastic gradient descent
- [动量法](#动量), momentum
- 自适应梯度算法, adagard
- Nesterov加速算法, NAG
- RMSProp
- AdaDelta
- Adam

### 通用逼近定理

根据Cyberko和Hornik等人在1989年的研究, 任何连续函数都可以通过一个单隐藏层的神经网络, 以任意小的误差进行逼近. 这意味着即使是简单的神经网络, 也能处理复杂的连续函数. 根据Cyberko 1988年的研究, 任何函数(包括不连续函数)都可以通过一个具有两个隐藏层的神经网络, 以任意小的误差进行逼近. 这意味着即使是非连续的复杂函数, 神经网络也能够很好的逼近.

这两个定理属于存在性定理, 也就是说, 它们仅仅说明了在理论上, 这样的神经网络能够逼近任意的函数, 但是并没有告诉我们应该如何选择网络的具体架构(如隐藏层侧数量, 每层神经元的数量等)以及如何设置超参数.

### 神经元的数量

#### 输入层

- 数值属性: 每个属性对应一个神经元
- 类别属性: 如果该属性有$k$个值, 那么就需要$k$个神经元, 并使用one-hot编码方式. 这种编码方式将具有$k$个值的属性表示为$k$个二进制的属性, 只有一个位置是$1$, 其余位置是$0$, 如假设一个天气属性有三个取值, 晴天, 阴天, 雨天, 那么, 晴天可以表示为$1\ 0\ 0$, 阴天可以表示为$0\ 1\ 0$, 雨天可以表示为$0\ 0\ 1$

#### 输出层

- $k$类问题: 对于一个有$k$个类别的问题, 输出层会有$k$个神经元, 也用one-hot编码
- 二分类问题: 可以使用两个神经元, 使用one-hot编码; 也可以使用一个神经元, 配合sigmoid激活函数, 若输出值接近$0$, 属于第一类, 接近$1$则属于第二类

#### 隐藏层 

隐藏层神经元对误差的影响如[图](https://img.ricolxwz.io/54225f471feeb87e7664d00730b2f0cf.png).

- 隐藏层神经元过多: 导致过拟合
- 隐藏层神经元过少: 导致欠拟合

思想就是开始的时候使用较少的神经元, 然后逐步增加神经元的数量, 直到模型的误差不再显著减少.

1. 一开始使用较少的隐藏神经元来初始化网络
2. 训练网络, 直到均方误差或其他损失指标不再显著减少
3. 此时, 向隐藏层中添加一些新神经元, 并使用随机初始化的权重重新训练网络, 均方误差会减少, 如[图](https://img.ricolxwz.io/b8176513be1475a51dbd250cd4e9bcdc.png)
4. 重复上述步骤, 直到满足终止条件, 例如添加新神经元不会导致显著的误差减少, 或者隐藏层达到设定的最大大小

### 学习率

神经网络的性能和学习率的相关性非常大. 

- 若学习率太小: 收敛地很慢
- 若学习率很大: 振荡, 超出最小值

我们无法在训练前就预知最优的学习率.

<figure markdown='1'>
![](https://img.ricolxwz.io/8a0280ce5fa12521df322cc115dfe04d.png){ loading=lazy width='500' }
</figure>

学习率可以是固定的, 也可以随时间变化. 后者一开始学习率交大, 随着时间推移, 慢慢减小. 一开始学习率较大能够造成更大的权重变化, 能够减少训练批次, 甚至还可能跳过某些局部最小吃; 而后, 学习率慢慢减小, 防止振荡的发生. 公式有两种:

- $\eta_n = \frac{\eta_{n-1}}{1+d*n}$
- $\eta_n = \eta_0 e^{-dn}$

其中, $\eta_n$表示第$n$批次的学习率, $d$是一个超参数, 表示衰减率.

### 动量

动量, momentum, 通过在权重更新公式中引入一个额外的动量项, 使得当前的权重更新依赖于之前的更新, 从而减少振荡并允许使用更大的学习率. 计算公式为$\Delta w_{pq}(t)=\eta\delta_q o_p + \mu(w_{pq}(t)-w_{pq}(t-1))$.

### 初始化

神经网络模型的性能非常依赖于权重和截距的初始化.常见的做法有:

- 标准的做法: 从$-1$到$1$之间选择小的随机数
- Xavier初始化: 权重从一个正态分布中产生, $\sigma=\sqrt{\frac{2}{N_{in}+N_{out}}}$, 其中$N_{in}$是输入神经元的数量, $N_{out}$是输出神经元的数量, 注意, 这里的输入输出神经元不是整个神经网络的输入输出神经元, 是相对于当前层神经元来说的上一层/下一层神经元, 当前层神经元就是权重/截距待更新的神经元

## 深度学习

深度学习和一般的神经网络的主要区别在于层数的深度以及能力的提升, 具体可以解释:

- 层数的区别: 一般的神经网络通常指那些只有一层或者几层隐藏层的神经网络, 也叫做浅层神经网络. 虽然他们可以处理简单的任务, 但是其表现收到限制, 尤其在处理复杂的非线性问题时效果不佳. 深度学习则使用多层隐藏层, 这些隐藏层构成了一个深层神经网络(DNN), 通过多个隐藏层, 网络可以捕捉到更加复杂的特征和模式, 解决如图像识别, 语音识别等复杂的任务
- 特征学习能力: 在浅层神经网络中, 特征的提取能力有限, 通常需要手工提取特征, 然后将特征输入神经网络进行分类或回归. 深度神经网络可以自动学习特征, 尤其在深度网络的层次结构中, 每一层可以逐渐学习到更高的特征, 例如, 卷积神经网络(CNN)可以自动从低级别的边缘, 纹理特征逐步学习到高级别的对象形状, 语义信息.
- 训练方式和算法: 浅层神经网络的训练相对简单, 常用的训练方法包括梯度下降和反向传播. 深度学习网络由于层数较多, 训练起来较复杂, 需要更加强大的硬件, 如GPU, 并且依赖于改进的训练方法, 如权重初始化(使用autoencoders), 正则化技术(如dropout), 优化算法等来避免梯度消失(vanishing gradient problem)和爆炸问题
- 数据需求: 浅层神经网络可以在中小规模的数据集上进行有效的训练, 深度神经网络通常需要大量数据来避免过拟合问题, 大型数据集是深度学习成功的重要因素之一

### 梯度消失问题

回顾在上面小节中讲到的误差计算公式:

1. 若$q$为输出层神经元, 则$\delta_q=(t_q-o_q)o_q(1-o_q)$
2. 若$q$为隐藏层神经元, 则$\delta_q=o_q(1-o_q)\sum_i w_{qi}\delta_i$

注意$o_q$是经过sigmoid函数激活后的输出, sigmoid函数的取值范围是$(0, 1)$, 若$o_1$非常贴近$0$或$1$, 可能导致$\delta_q$特别小, 接近于$0$, 而$\delta_q$是会被反向传播的, 到前一层计算误差的时候也会特别小, 前面所有的层权重变化都特别小.

这种现象在隐藏层较多的时候较明显. 会导致梯度消失, 收敛地特别慢. 就算输出层没有饱和, 重复的乘以一个小于$1$的值, 梯度会变得非常小, 当传播到接近输出层的时候, 梯度可能会消失. 由于较低层的梯度非常小, 这些层的学习速度就会很慢.

解决方法是使用其他的激活函数, 如ReLU和LReLu. 它们没有上限, 因此输出不会饱和, 对于$x>0$, ReLU的梯度为$1$.

<figure markdown='1'>
![](https://img.ricolxwz.io/529ee4cf1b146b4e80b2c9613be8fbe6.png){ loading=lazy width='500' }
</figure>

### Dropout

Dropout是一种防止过拟合的方法. 

核心思想是在每一次反向迭代的时候, 网络的每一层都会随机选择部分神经元, 并将其输出设为$0$, 即"丢弃"这些神经元. 这些"丢弃"的神经元不会参与当前轮次的权重更新, 相当于暂时禁用它们. 真正有用的特征更能抵抗神经元的随机移除, 因为它们在不同的神经元的组合下仍然表现良好.

由于Dropout在训练的时候随机丢弃一部分神经元, 这实际上是在每次迭代的时候训练一个较小的子网络. 

- 在训练的过程中, 我们会用反向传播算法更新子网中的权重和截距, 然后将这些更新的权重值加回到原始的网络中
- 在测试的过程中, 我们不会丢弃任何的神经元, 由于在训练过程中, Dropout会随机"丢弃"一部分神经元, 因此, 对于每一次训练迭代, 模型的激活和权重更新都是基于部分神经元计算, 例如, 如果Dropout率是$0.5$, 那么在训练过程中, 激活函数的输入会是原始网络中大约$50%$的信号, 这意味着, 在每一次前向传播的过程中, 神经元的输出实际值比全网络的预期值大约低$50%$. 在测试阶段, 我们需要对权重进行缩放, 乘以$0.5$

<figure markdown='1'>
![](https://img.ricolxwz.io/4fb64c6600f1837d32bee9477168fee8.png){ loading=lazy width='500' }
</figure>

### Softmax函数

神经网络输出的结果能够被继续处理成为概率. 神经网络的输出为$(o_1, ..., o_n)$, softmax函数为$p_i=\frac{e^{o_i}}{\sum_j e^{o_i}}$. 

???+ example "例子"

    如三个输出神经元的输出为$o_1=0.3, o_2=0.8, o_3=0.2$. 使用softmax函数之后$p_1=0.28, p_2=0.46, p_3=0.26$.

### 其他的损失函数

可以使用交叉熵损失而不是MSE. 它衡量的是模型预测的概率分布和真实类别分布之间的差异, 例如通过softmax函数得到的输出 交叉熵损失用于反映模型对于正确类别的预测有多大的不确定性.

对于单个样本的交叉熵损失公式为$CCE_i=-\sum_{j=1}^C y_{ij}\cdot \log(\hat{y_{ij}})$. 其中, $y_{ij}$是样本$i$的真实one-hot编码向量的第$j$个元素, 真实类别上为$1$, 其他为$0$. $\hat{y_{ij}}$为模型预测的概率分布中对应第$j$个类别的概率.

对于$N$个样本, 总的交叉熵损失是每个样本的交叉熵损失的总和: $CCE=\sum_{i=1}^N CCE_i$.

[^1]: 神经元模型和感知机 | Machine Learning. (2018, August 12). https://zhjunqin.gitbook.io/machine-learning/ji-qi-xue-xi/shen-jing-wang-luo/shen-jing-yuan-mo-xing